{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# OpenAI Chat | Testing"
      ],
      "metadata": {
        "id": "XBoYFDS9m1f8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing basic libraries"
      ],
      "metadata": {
        "id": "9mndZ2kpnJfX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sys"
      ],
      "metadata": {
        "id": "3btGBOzDLhYN"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing OpenAI libraries"
      ],
      "metadata": {
        "id": "HuIxrR_DLvHP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import os\n",
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "AxW3u98BoUK_"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading OpenAI API key"
      ],
      "metadata": {
        "id": "88Ahi3KsqAV4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "-sJMbeegNIeh"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Basic Chat App | Model as Assistant"
      ],
      "metadata": {
        "id": "yNC1U3SVqISz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_chatgpt_response(prompt):\n",
        "    try:\n",
        "        client = openai.OpenAI(api_key = OPENAI_API_KEY)\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ],\n",
        "            max_tokens=4096,\n",
        "            temperature=0.7\n",
        "        )\n",
        "        return response.choices[0].message.content\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"An error occurred: {str(e)}\""
      ],
      "metadata": {
        "id": "nt6J2jQIoojR"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing Chat App"
      ],
      "metadata": {
        "id": "zQ1_At8IqWxh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user_prompt = input(\"Enter your prompt: \")\n",
        "print(\"ChatGPT's response:\")\n",
        "print(get_chatgpt_response(user_prompt))"
      ],
      "metadata": {
        "id": "fYsBnWGUpvVT",
        "outputId": "4dca08ac-ca8b-43d6-cd3d-f9559ebf2876",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your prompt: Why can't we train LLM using LLM like we do in reinforcement learning. Explain in detail.\n",
            "ChatGPT's response:\n",
            "Training a large language model (LLM) using another LLM, similar to how reinforcement learning models can be trained using other reinforcement learning models, is not a straightforward process due to several key reasons:\n",
            "\n",
            "1. Lack of clear reward signal: In reinforcement learning, the agent learns through a reward signal provided by the environment. This signal helps the agent understand whether its actions are leading to positive outcomes or not. In the case of training an LLM using another LLM, there is no clear external reward signal that can guide the training process. Language models are typically trained using supervised learning with large text corpora, and the objective is to minimize a loss function (such as cross-entropy) rather than maximizing a reward signal.\n",
            "\n",
            "2. Lack of well-defined actions and states: In reinforcement learning, the agent interacts with an environment by taking actions and transitioning between different states. These actions and states are well-defined within the environment, allowing the agent to learn optimal policies. In the case of training an LLM using another LLM, it is not clear how to define actions and states within the context of language modeling. Language models generate text based on input sequences, but it is challenging to frame this as a reinforcement learning problem with clearly defined actions and states.\n",
            "\n",
            "3. Training instability: Training large language models is already a challenging task due to issues such as vanishing gradients, overfitting, and exploding gradients. Introducing a reinforcement learning framework on top of this could lead to further training instability and make the optimization process more complex. Reinforcement learning algorithms often involve exploration-exploitation trade-offs and can be sensitive to hyperparameters, which could exacerbate training difficulties when applied to large language models.\n",
            "\n",
            "4. Computational complexity: Training large language models is computationally intensive and requires significant resources in terms of memory and processing power. Adding a reinforcement learning component on top of this would further increase the computational complexity of the training process, making it even more challenging to scale up and train efficiently.\n",
            "\n",
            "Overall, while it is theoretically possible to explore training techniques that combine reinforcement learning and large language models, it is currently a less explored area due to the challenges mentioned above. Researchers continue to investigate ways to improve the training and performance of language models, but training an LLM using another LLM in a reinforcement learning setup remains a complex and challenging task.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}