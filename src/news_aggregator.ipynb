{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLcTQ93ZaOk-"
      },
      "source": [
        "# News Aggregator App"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Important Installations | Required"
      ],
      "metadata": {
        "id": "n41W70xmaVLD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade --quiet  langchain langchain-community langchain-pinecone langchain-huggingface neo4j langchain-core tiktoken yfiles_jupyter_graphs newsapi-python requests huggingface_hub pinecone-client tqdm pinecone sentence_transformers"
      ],
      "metadata": {
        "id": "DkBPPIrOaZ6O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing libraries | Required"
      ],
      "metadata": {
        "id": "LuNp4ghnbVBA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Standard library imports\n",
        "import hashlib\n",
        "import json\n",
        "import os\n",
        "import time\n",
        "from uuid import uuid4\n",
        "from datetime import datetime, timedelta\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "\n",
        "# Third-party imports\n",
        "import pinecone\n",
        "import requests\n",
        "import torch\n",
        "from google.colab import output, userdata\n",
        "from neo4j import GraphDatabase\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from tqdm import tqdm\n",
        "from yfiles_jupyter_graphs import GraphWidget\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "\n",
        "# LangChain imports\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.graphs import Neo4jGraph\n",
        "from langchain_community.vectorstores import Neo4jVector\n",
        "from langchain_core.messages import AIMessage, HumanMessage\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.prompts.prompt import PromptTemplate\n",
        "from langchain_core.runnables import (\n",
        "    ConfigurableField,\n",
        "    RunnableBranch,\n",
        "    RunnableLambda,\n",
        "    RunnableParallel,\n",
        "    RunnablePassthrough,\n",
        ")\n",
        "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_pinecone import PineconeVectorStore\n",
        "\n",
        "# Google Colab widget configuration\n",
        "try:\n",
        "    output.enable_custom_widget_manager()\n",
        "except:\n",
        "    pass"
      ],
      "metadata": {
        "id": "Xb4Bm8v5bsDS"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading environment variables | API keys"
      ],
      "metadata": {
        "id": "nvHqQNbLc3Xt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "HUGGINGFACE_TOKEN = userdata.get('HUGGINGFACE_TOKEN')\n",
        "NEWSAPI_KEY = userdata.get('NEWSAPI_KEY')\n",
        "PINECONE_API_KEY = userdata.get('PINECONE_API_KEY')\n",
        "NEO4J_PASSWORD = userdata.get('NEO4J_PASSWORD')\n",
        "NEO4J_URI=\"neo4j+s://87dc4a97.databases.neo4j.io\"\n",
        "NEO4J_USERNAME=\"neo4j\""
      ],
      "metadata": {
        "id": "mstvz2KddKfG"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Validate HuggingFace access token   \n",
        "*(run below command to validate your access token in terminal)*"
      ],
      "metadata": {
        "id": "Fw-qg_43elsZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login --token $HUGGINGFACE_TOKEN"
      ],
      "metadata": {
        "id": "OUdXI3smeiKJ",
        "outputId": "ae58e4bb-edd1-46c4-b92a-7f977cf07d8f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: read).\n",
            "The token `giru-upgrad-news-agg-read-only` has been saved to /root/.cache/huggingface/stored_tokens\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful.\n",
            "The current active token is: `giru-upgrad-news-agg-read-only`\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setup OS environment variables"
      ],
      "metadata": {
        "id": "ZkKgBtKnfPfq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['HUGGINGFACE_TOKEN'] = HUGGINGFACE_TOKEN\n",
        "os.environ['NEWSAPI_KEY'] = NEWSAPI_KEY\n",
        "os.environ['PINECONE_API_KEY'] = PINECONE_API_KEY\n",
        "os.environ['NEO4J_PASSWORD'] = NEO4J_PASSWORD\n",
        "os.environ['NEO4J_URI'] = NEO4J_URI\n",
        "os.environ['NEO4J_USERNAME'] = NEO4J_USERNAME"
      ],
      "metadata": {
        "id": "WgAyaoEZflOz"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wait for it!!"
      ],
      "metadata": {
        "id": "Fikd2dWxq3Sw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def count_words(text: str) -> int:\n",
        "    \"\"\"Count words in a text string.\"\"\"\n",
        "    if not text:\n",
        "        return 0\n",
        "    return len(text.split())\n",
        "\n",
        "def create_safe_filename(topic: str, title: str) -> str:\n",
        "    \"\"\"Create a safe filename from topic and title.\"\"\"\n",
        "    # Remove or replace invalid filename characters\n",
        "    invalid_chars = '<>:\"/\\\\|?*'\n",
        "    safe_title = ''.join(c if c not in invalid_chars else '_' for c in title)\n",
        "    safe_title = safe_title[:100]  # Limit length\n",
        "    return f\"{topic}_{safe_title}.txt\"\n",
        "\n",
        "def fetch_multiple_topics(api_key: str, topics: List[str], database_folder: str = \"database\") -> Dict[str, int]:\n",
        "    \"\"\"\n",
        "    Fetch articles for multiple topics and save each article to a separate file.\n",
        "\n",
        "    Args:\n",
        "        api_key: NewsAPI key\n",
        "        topics: List of topics to fetch articles for\n",
        "        database_folder: Folder to store article files\n",
        "        days_from: Number of days from today to fetch articles\n",
        "\n",
        "    Returns:\n",
        "        Dictionary with topics and their saved article counts\n",
        "    \"\"\"\n",
        "    # Ensure database folder exists\n",
        "    os.makedirs(database_folder, exist_ok=True)\n",
        "    article_counts = {topic: 0 for topic in topics}\n",
        "\n",
        "    for topic in topics:\n",
        "        try:\n",
        "            url = (\n",
        "                f\"https://newsapi.org/v2/everything\"\n",
        "                f\"?q={topic}\"\n",
        "                f\"&sortBy=popularity\"\n",
        "                f\"&pageSize=100\"\n",
        "                f\"&apiKey={api_key}\"\n",
        "            )\n",
        "\n",
        "            response = requests.get(url)\n",
        "            response.raise_for_status()\n",
        "\n",
        "            articles = response.json().get('articles', [])\n",
        "\n",
        "            for article in articles:\n",
        "                title = article.get('title', 'No title')\n",
        "                content = article.get('content', '')\n",
        "                description = article.get('description', '')\n",
        "\n",
        "                # Combine content and description for word count\n",
        "                full_text = f\"{content}\\n{description}\".strip()\n",
        "                word_count = count_words(full_text)\n",
        "\n",
        "                # Skip if content is too short\n",
        "                if word_count < 10:\n",
        "                    continue\n",
        "\n",
        "                # Create filename using topic and title\n",
        "                filename = create_safe_filename(topic, title)\n",
        "                filepath = os.path.join(database_folder, filename)\n",
        "\n",
        "                # Write article to file\n",
        "                with open(filepath, 'w', encoding='utf-8') as file:\n",
        "                    # Write metadata header\n",
        "                    file.write(\"=\" * 50 + \"\\n\")\n",
        "                    file.write(f\"Topic: {topic}\\n\")\n",
        "                    file.write(f\"Title: {title}\\n\")\n",
        "                    file.write(f\"Published: {article.get('publishedAt', 'No date')}\\n\")\n",
        "                    file.write(f\"Source: {article.get('source', {}).get('name', 'Unknown')}\\n\")\n",
        "                    file.write(f\"URL: {article.get('url', 'No URL')}\\n\")\n",
        "                    file.write(f\"Word Count: {word_count}\\n\")\n",
        "                    file.write(\"=\" * 50 + \"\\n\\n\")\n",
        "\n",
        "                    # Write content\n",
        "                    file.write(full_text)\n",
        "\n",
        "                article_counts[topic] += 1\n",
        "\n",
        "            # Sleep to respect API rate limits\n",
        "            time.sleep(1)\n",
        "\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Error fetching articles for {topic}: {str(e)}\")\n",
        "            continue\n",
        "\n",
        "    return article_counts"
      ],
      "metadata": {
        "id": "vrcW9iPZlngu"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ArticleRAG:\n",
        "    def __init__(self, database_folder: str = \"database\", index_name: str = \"articles-embeddings\"):\n",
        "        \"\"\"\n",
        "        Initialize RAG system using LangChain and HuggingFace embeddings with Pinecone integration.\n",
        "\n",
        "        Args:\n",
        "            database_folder: Folder containing article files\n",
        "            index_name: Name for the Pinecone index\n",
        "        \"\"\"\n",
        "        self.database_folder = database_folder\n",
        "\n",
        "        # Initialize HuggingFace Embeddings\n",
        "        print(\"Initializing HuggingFace Embeddings...\")\n",
        "        self.embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
        "\n",
        "        # Initialize Pinecone with new pattern\n",
        "        print(\"Initializing Pinecone...\")\n",
        "        self.pc = Pinecone(api_key=PINECONE_API_KEY)\n",
        "        self.index_name = index_name\n",
        "\n",
        "        # Check if index exists and create if needed\n",
        "        if self.index_name not in self.pc.list_indexes().names():\n",
        "            print(f\"Creating new Pinecone index: {self.index_name}\")\n",
        "            self.pc.create_index(\n",
        "                name=self.index_name,\n",
        "                dimension=self.embeddings.embed_query(\"\").shape[0],\n",
        "                metric='cosine',\n",
        "                spec=ServerlessSpec(\n",
        "                    cloud='aws',\n",
        "                    region='us-east-1'\n",
        "                )\n",
        "            )\n",
        "\n",
        "        # Initialize vector store with the new index\n",
        "        self.vector_store = PineconeVectorStore(\n",
        "            index=self.pc.Index(self.index_name),\n",
        "            embedding=self.embeddings\n",
        "        )\n",
        "\n",
        "        # Text splitter for chunking\n",
        "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=500,  # Adjust for larger chunks\n",
        "            chunk_overlap=100,\n",
        "            length_function=len\n",
        "        )\n",
        "\n",
        "    def process_articles(self):\n",
        "        \"\"\"\n",
        "        Read and process articles from the database folder, storing embeddings in Pinecone.\n",
        "        \"\"\"\n",
        "        print(\"Processing articles from database folder...\")\n",
        "        docs = []\n",
        "        for filename in os.listdir(self.database_folder):\n",
        "            if filename.endswith(\".txt\"):\n",
        "                filepath = os.path.join(self.database_folder, filename)\n",
        "                with open(filepath, \"r\", encoding=\"utf-8\") as file:\n",
        "                    # Read the file content\n",
        "                    content = file.read()\n",
        "\n",
        "                    # Split content into metadata and body\n",
        "                    parts = content.split(\"=\" * 50)\n",
        "                    if len(parts) < 3:\n",
        "                        print(f\"Skipping malformed file: {filename}\")\n",
        "                        continue\n",
        "\n",
        "                    metadata_text = parts[1].strip()\n",
        "                    article_content = parts[2].strip()\n",
        "\n",
        "                    # Chunk the article content\n",
        "                    chunks = self.text_splitter.split_text(article_content)\n",
        "\n",
        "                    # Create Document objects with metadata\n",
        "                    for i, chunk in enumerate(chunks):\n",
        "                        doc = Document(\n",
        "                            page_content=chunk,\n",
        "                            metadata={\n",
        "                                \"source_file\": filename,\n",
        "                                \"chunk_index\": i,\n",
        "                                \"total_chunks\": len(chunks),\n",
        "                                \"metadata_text\": metadata_text\n",
        "                            }\n",
        "                        )\n",
        "                        docs.append(doc)\n",
        "\n",
        "        if docs:\n",
        "            # Generate UUIDs for documents\n",
        "            uuids = [str(uuid4()) for _ in range(len(docs))]\n",
        "\n",
        "            # Add documents to Pinecone\n",
        "            print(\"Adding documents to Pinecone...\")\n",
        "            self.vector_store.add_documents(documents=docs, ids=uuids)\n",
        "        else:\n",
        "            print(\"No valid documents to process.\")\n",
        "\n",
        "    def query(self, query_text: str, k: int = 5):\n",
        "        \"\"\"\n",
        "        Query the Pinecone vector database.\n",
        "\n",
        "        Args:\n",
        "            query_text: Query string\n",
        "            k: Number of top results to return\n",
        "\n",
        "        Returns:\n",
        "            List of relevant results with metadata\n",
        "        \"\"\"\n",
        "        print(\"Querying Pinecone...\")\n",
        "        results = self.vector_store.similarity_search(query_text, k=k)\n",
        "        formatted_results = [\n",
        "            {\n",
        "                \"content\": res.page_content,\n",
        "                \"metadata\": res.metadata\n",
        "            }\n",
        "            for res in results\n",
        "        ]\n",
        "        return formatted_results"
      ],
      "metadata": {
        "id": "77mX_tXTj9gt"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Fetch articles using fetch_multiple_topics\n",
        "    API_KEY = NEWSAPI_KEY  # Replace with your API key\n",
        "    topics = [\"technology\", \"artificial intelligence\", \"economics\", \"politics\", \"climate change\"]\n",
        "\n",
        "    print(\"Fetching articles...\")\n",
        "    results = fetch_multiple_topics(api_key=API_KEY, topics=topics, database_folder=\"database\")\n",
        "\n",
        "    # Print summary\n",
        "    print(\"\\nArticles saved:\")\n",
        "    for topic, count in results.items():\n",
        "        print(f\"{topic}: {count} articles\")\n",
        "\n",
        "    # Initialize and process articles with ArticleRAG\n",
        "    rag = ArticleRAG(database_folder=\"database\", index_name=\"articles-embeddings\")\n",
        "    rag.process_articles()"
      ],
      "metadata": {
        "id": "Be3hHQD9l5cp",
        "outputId": "197b3e7c-8c3b-4643-8d61-9b2252ac0e1e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching articles...\n",
            "\n",
            "Articles saved:\n",
            "technology: 86 articles\n",
            "artificial intelligence: 96 articles\n",
            "economics: 100 articles\n",
            "politics: 98 articles\n",
            "climate change: 96 articles\n",
            "Initializing HuggingFace Embeddings...\n",
            "Initializing Pinecone...\n",
            "Processing articles from database folder...\n",
            "Adding documents to Pinecone...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_pinecone.vectorstores:Found document with no `text` key. Skipping.\n",
            "WARNING:langchain_pinecone.vectorstores:Found document with no `text` key. Skipping.\n",
            "WARNING:langchain_pinecone.vectorstores:Found document with no `text` key. Skipping.\n",
            "WARNING:langchain_pinecone.vectorstores:Found document with no `text` key. Skipping.\n",
            "WARNING:langchain_pinecone.vectorstores:Found document with no `text` key. Skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Querying Pinecone...\n",
            "\n",
            "Query Results:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example query\n",
        "query = \"What are the latest developments in artificial intelligence?\"\n",
        "results = rag.query(query, k=5)\n",
        "\n",
        "print(\"\\nQuery Results:\")\n",
        "for i, result in enumerate(results, 1):\n",
        "    print(f\"\\nResult {i}:\")\n",
        "    print(f\"Content: {result['content'][:200]}...\")\n",
        "    print(f\"Metadata: {result['metadata']}\")"
      ],
      "metadata": {
        "id": "A7h5CfzLpQl7",
        "outputId": "d7ce86a6-a80d-47b0-e0a6-889f81449933",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Querying Pinecone...\n",
            "\n",
            "Query Results:\n",
            "\n",
            "Result 1:\n",
            "Content: OpenAI CEO Sam Altmanexpects AGI, or artificial general intelligenceAI that outperforms humans at most tasksaround 2027 or 2028. Elon Musks prediction is either 2025 or 2026, and he has claimed that …...\n",
            "Metadata: {'chunk_index': 0.0, 'metadata_text': 'Topic: artificial intelligence\\nTitle: Human Misuse Will Make Artificial Intelligence More Dangerous\\nPublished: 2024-12-13T14:00:00Z\\nSource: Wired\\nURL: https://www.wired.com/story/human-misuse-will-make-artificial-intelligence-more-dangerous/\\nWord Count: 58', 'source_file': 'artificial intelligence_Human Misuse Will Make Artificial Intelligence More Dangerous.txt', 'total_chunks': 1.0}\n",
            "\n",
            "Result 2:\n",
            "Content: Those who are worried that advancements in artificial intelligence could lead to the destruction of humanity have a new reason to be anxious.\n",
            "New research on OpenAI's latest series of AI models, kno… ...\n",
            "Metadata: {'chunk_index': 0.0, 'metadata_text': \"Topic: artificial intelligence\\nTitle: OpenAI's new o1 model sometimes fights back when it thinks it'll be shut down and then lies about it\\nPublished: 2024-12-06T19:50:48Z\\nSource: Business Insider\\nURL: https://www.businessinsider.com/openai-o1-safety-research-scheming-deception-lies-2024-12\\nWord Count: 51\", 'source_file': \"artificial intelligence_OpenAI's new o1 model sometimes fights back when it thinks it'll be shut down and then lies about it.txt\", 'total_chunks': 1.0}\n",
            "\n",
            "Result 3:\n",
            "Content: The old saying goes that, with tech, you should never buy the first generation of anything new. Wait for the devs to work out the kinks, then check back. Were now two years into the AI revolution, an…...\n",
            "Metadata: {'chunk_index': 0.0, 'metadata_text': 'Topic: artificial intelligence\\nTitle: We Did Not Reach the AI Promised Land in 2024\\nPublished: 2024-12-27T11:00:29Z\\nSource: Gizmodo.com\\nURL: https://gizmodo.com/we-did-not-reach-the-ai-promised-land-in-2024-so-whats-next-2000540007\\nWord Count: 64', 'source_file': 'artificial intelligence_We Did Not Reach the AI Promised Land in 2024.txt', 'total_chunks': 1.0}\n",
            "\n",
            "Result 4:\n",
            "Content: Ever since OpenAI released ChatGPT at the tail end of 2022, the tech industry has been chasing artificial intelligence. For the past few years, tech companies have been ramping up AI development — an…...\n",
            "Metadata: {'chunk_index': 0.0, 'metadata_text': 'Topic: artificial intelligence\\nTitle: 2025 will be the year AI finds its purpose on phones, smartwatches, and tablets\\nPublished: 2024-12-23T16:00:00Z\\nSource: Android Central\\nURL: https://www.androidcentral.com/apps-software/2025-the-year-ai-finds-purpose-on-phones-smartwatches-tablets\\nWord Count: 60', 'source_file': 'artificial intelligence_2025 will be the year AI finds its purpose on phones, smartwatches, and tablets.txt', 'total_chunks': 1.0}\n",
            "\n",
            "Result 5:\n",
            "Content: For many of the people served by the humanitarian sector, 2024 has been the worst of times. The most recent UN estimates of those forced to flee violence and disaster is a record of 120 million, a fi…...\n",
            "Metadata: {'chunk_index': 0.0, 'metadata_text': 'Topic: climate change\\nTitle: More Humanitarian Organizations Will Harness AI’s Potential\\nPublished: 2024-12-10T09:00:00Z\\nSource: Wired\\nURL: https://www.wired.com/story/humanitarian-organizations-artificial-intelligence/\\nWord Count: 61', 'source_file': 'climate change_More Humanitarian Organizations Will Harness AI’s Potential.txt', 'total_chunks': 1.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zvo4gqVWoD98"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VNEsbhvusUCt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Backend"
      ],
      "metadata": {
        "id": "qTHwJLOisVC1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install py2neo"
      ],
      "metadata": {
        "id": "SZLNt7Jesdrk",
        "outputId": "26db5361-fd0f-4da9-8db3-dbe21458ab92",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting py2neo\n",
            "  Downloading py2neo-2021.2.4-py2.py3-none-any.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from py2neo) (2024.12.14)\n",
            "Collecting interchange~=2021.0.4 (from py2neo)\n",
            "  Downloading interchange-2021.0.4-py2.py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting monotonic (from py2neo)\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from py2neo) (24.2)\n",
            "Collecting pansi>=2020.7.3 (from py2neo)\n",
            "  Downloading pansi-2024.11.0-py2.py3-none-any.whl.metadata (3.1 kB)\n",
            "Requirement already satisfied: pygments>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from py2neo) (2.18.0)\n",
            "Requirement already satisfied: six>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from py2neo) (1.17.0)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from py2neo) (2.2.3)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from interchange~=2021.0.4->py2neo) (2024.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from pansi>=2020.7.3->py2neo) (11.0.0)\n",
            "Downloading py2neo-2021.2.4-py2.py3-none-any.whl (177 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.2/177.2 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading interchange-2021.0.4-py2.py3-none-any.whl (28 kB)\n",
            "Downloading pansi-2024.11.0-py2.py3-none-any.whl (26 kB)\n",
            "Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Installing collected packages: monotonic, pansi, interchange, py2neo\n",
            "Successfully installed interchange-2021.0.4 monotonic-1.6 pansi-2024.11.0 py2neo-2021.2.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Backend and Neo4j Integration in FastAPI\n",
        "from fastapi import FastAPI, HTTPException\n",
        "from py2neo import Graph, Node, Relationship\n",
        "from pydantic import BaseModel\n",
        "import uvicorn\n",
        "import threading\n",
        "# FastAPI News Endpoint\n",
        "from typing import List, Dict"
      ],
      "metadata": {
        "id": "TlarbCBksUyb"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "app = FastAPI()\n",
        "\n",
        "# Neo4j Connection\n",
        "# graph = Graph(\"bolt://localhost:7687\", auth=(\"neo4j\", \"password\"))\n",
        "graph = Neo4jGraph()\n",
        "\n",
        "class FetchNewsRequest(BaseModel):\n",
        "    user_id: str\n",
        "    category: str\n",
        "    custom_category: str\n",
        "\n",
        "@app.post(\"/fetch_news/\")\n",
        "def fetch_news(data: FetchNewsRequest):\n",
        "\n",
        "    news = [\n",
        "        {\"title\": f\"Latest in {data.category}\", \"content\": \"News content about this category...\"},\n",
        "        {\"title\": f\"More on {data.custom_category}\", \"content\": \"Custom category-related news...\"}\n",
        "    ]\n",
        "    return {\"news\": news}\n",
        "\n",
        "# Run FastAPI server in a thread\n",
        "def run_fastapi():\n",
        "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
        "\n",
        "threading.Thread(target=run_fastapi, daemon=True).start()"
      ],
      "metadata": {
        "id": "RM4F6H5ksUAY",
        "outputId": "d4a6a2d5-3c94-46ef-fd33-9ebac1a39514",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'Neo4jGraph' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-71a210259cfc>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Neo4j Connection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# graph = Graph(\"bolt://localhost:7687\", auth=(\"neo4j\", \"password\"))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNeo4jGraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mFetchNewsRequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseModel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Neo4jGraph' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Frontend"
      ],
      "metadata": {
        "id": "yh6CaAJBq5nE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio"
      ],
      "metadata": {
        "id": "sA5VWLVHr_IZ",
        "outputId": "be8cff6f-a086-4b98-836c-01b814bee56a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gradio\n",
            "  Downloading gradio-5.9.1-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
            "  Downloading fastapi-0.115.6-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting gradio-client==1.5.2 (from gradio)\n",
            "  Downloading gradio_client-1.5.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.25.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.27.0)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.4)\n",
            "Collecting markupsafe~=2.0 (from gradio)\n",
            "  Downloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.26.4)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.10.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (11.0.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.10.3)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.18 (from gradio)\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.2)\n",
            "Collecting ruff>=0.2.2 (from gradio)\n",
            "  Downloading ruff-0.8.6-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n",
            "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.45.2-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.15.1)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.12.2)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.5.2->gradio) (2024.10.0)\n",
            "Requirement already satisfied: websockets<15.0,>=10.0 in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.5.2->gradio) (14.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.2.2)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.41.3-py3-none-any.whl.metadata (6.0 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->gradio) (3.16.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->gradio) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.27.1)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.1->gradio) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.1->gradio) (2.2.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading gradio-5.9.1-py3-none-any.whl (57.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.2/57.2 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.5.2-py3-none-any.whl (320 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.4/320.4 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading fastapi-0.115.6-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
            "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Downloading ruff-0.8.6-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m56.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
            "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading starlette-0.41.3-py3-none-any.whl (73 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
            "Downloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub, uvicorn, tomlkit, semantic-version, ruff, python-multipart, markupsafe, ffmpy, aiofiles, starlette, safehttpx, gradio-client, fastapi, gradio\n",
            "  Attempting uninstall: markupsafe\n",
            "    Found existing installation: MarkupSafe 3.0.2\n",
            "    Uninstalling MarkupSafe-3.0.2:\n",
            "      Successfully uninstalled MarkupSafe-3.0.2\n",
            "Successfully installed aiofiles-23.2.1 fastapi-0.115.6 ffmpy-0.5.0 gradio-5.9.1 gradio-client-1.5.2 markupsafe-2.1.5 pydub-0.25.1 python-multipart-0.0.20 ruff-0.8.6 safehttpx-0.1.6 semantic-version-2.10.0 starlette-0.41.3 tomlkit-0.13.2 uvicorn-0.34.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "markupsafe"
                ]
              },
              "id": "088e48251d54434fa65579214368b8c4"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import requests\n",
        "\n",
        "# Backend URL\n",
        "backend_url = \"http://0.0.0.0:8000/\"\n",
        "\n",
        "# Function to send data to backend and fetch news\n",
        "def fetch_news(user_id, category, custom_category):\n",
        "    payload = {\n",
        "        \"user_id\": user_id,\n",
        "        \"category\": category,\n",
        "        \"custom_category\": custom_category,\n",
        "    }\n",
        "    # Backend endpoint to fetch news (implement this in FastAPI)\n",
        "    response = requests.post(f\"{backend_url}/fetch_news/\", json=payload)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        news = response.json().get(\"news\", [])\n",
        "        cards = \"\\n\\n\".join([f\"**{n['title']}**\\n{n['content']}\" for n in news])\n",
        "        return cards\n",
        "    else:\n",
        "        return \"Error fetching news!\"\n",
        "\n",
        "# Gradio UI\n",
        "users = [\"1\", \"2\", \"3\"]\n",
        "\n",
        "with gr.Blocks() as ui:\n",
        "    # User selection\n",
        "    user_dropdown = gr.Dropdown(users, label=\"Select User\", value=\"1\")\n",
        "    # Category selection\n",
        "    category_dropdown = gr.Dropdown([\"Tech\", \"Sports\", \"Finance\", \"Health\"], label=\"Select Category\")\n",
        "    # Custom category input\n",
        "    custom_category_input = gr.Textbox(label=\"Enter Custom Category (optional)\")\n",
        "    # Button to fetch news\n",
        "    fetch_button = gr.Button(\"Fetch News\")\n",
        "    # News display area\n",
        "    news_display = gr.Textbox(label=\"News\", interactive=False, placeholder=\"News will appear here\")\n",
        "\n",
        "    # Button click event\n",
        "    fetch_button.click(\n",
        "        fn=fetch_news,\n",
        "        inputs=[user_dropdown, category_dropdown, custom_category_input],\n",
        "        outputs=news_display\n",
        "    )\n",
        "\n",
        "ui.launch()"
      ],
      "metadata": {
        "id": "FZVaTTNyq5lP",
        "outputId": "27d38039-7aca-4a1f-cc56-325daf5772b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 650
        }
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://07525ad886d626aabc.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://07525ad886d626aabc.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dj7aiKlVr8zj"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}