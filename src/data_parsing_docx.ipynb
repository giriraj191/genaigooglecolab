{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_NHi2U7Vd7VV"
      },
      "source": [
        "# Data Parsing | Docx | Practice"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installing required libraries"
      ],
      "metadata": {
        "id": "bGKpc0-cf3G1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-community\n",
        "!pip install openai\n",
        "!pip install python-dotenv\n",
        "!pip install \"unstructured[all-docs]\"\n",
        "!pip install jq"
      ],
      "metadata": {
        "id": "pM3y5eo3ftU8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing required libraries"
      ],
      "metadata": {
        "id": "0JgpJ8vDgXVG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "import json"
      ],
      "metadata": {
        "id": "pbOy8ywZgWAN"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import islice\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "from langchain.document_loaders import CSVLoader, TextLoader, JSONLoader\n",
        "from unstructured.partition.docx import partition_docx\n",
        "from IPython.core.display import display, HTML"
      ],
      "metadata": {
        "id": "AltK5J4PgU5h"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unstructured - `partition_docx()`"
      ],
      "metadata": {
        "id": "wUYROHOCeUll"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_doc = partition_docx(filename = '/content/docx/Data Enablement Service.docx')"
      ],
      "metadata": {
        "id": "-4c6DT1gefdi"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "General information function"
      ],
      "metadata": {
        "id": "a7nJeJfxhCzu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def general_info(doc):\n",
        "  print(f'Type of document: {type(doc)}')\n",
        "  print(f'Number of elements in document: {len(doc)}')\n",
        "  print(f'Type of First element in document: {type(doc[0])}')\n",
        "  print(f'First element in document\\n {doc[0]}')\n",
        "  print(f'Type of Last element in document: {type(doc[-1])}')\n",
        "  print(f'Last element in document\\n {doc[-1]}')"
      ],
      "metadata": {
        "id": "7ZtfAqEpgiY8"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "general_info(word_doc)"
      ],
      "metadata": {
        "id": "DTA_6UQZhgdy",
        "outputId": "77c87a3f-6dea-425f-f6ff-9fea171d378a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Type of document: <class 'list'>\n",
            "Number of elements in document: 423\n",
            "Type of First element in document: <class 'unstructured.documents.elements.Title'>\n",
            "First element in document\n",
            "                                              Data Enablement platform\n",
            "Type of Last element in document: <class 'unstructured.documents.elements.NarrativeText'>\n",
            "Last element in document\n",
            " Use ElasticSearch client library elasticsearch-py for Python to interact with ElasticSearch from the product application.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "element_types = set([type(item) for item in word_doc])\n",
        "print(len(element_types))\n",
        "for typ in element_types:\n",
        "  print(typ)"
      ],
      "metadata": {
        "id": "1VtfoN7pho2e",
        "outputId": "74f187db-9819-4068-cd30-81452b74e4bd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6\n",
            "<class 'unstructured.documents.elements.PageBreak'>\n",
            "<class 'unstructured.documents.elements.Text'>\n",
            "<class 'unstructured.documents.elements.Title'>\n",
            "<class 'unstructured.documents.elements.ListItem'>\n",
            "<class 'unstructured.documents.elements.NarrativeText'>\n",
            "<class 'unstructured.documents.elements.Table'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tables = [el for el in word_doc if str(type(el)) == \"<class 'unstructured.documents.elements.Table'>\"]\n",
        "print(len(tables))"
      ],
      "metadata": {
        "id": "Etq3wCC5m1xa",
        "outputId": "1d666821-6029-4876-e459-97c89e91c78c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tables"
      ],
      "metadata": {
        "id": "gAJhfjwSoCiI",
        "outputId": "463db45f-0e87-4a27-aeae-b3ab21cd3196",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<unstructured.documents.elements.Table at 0x7c946e7062f0>,\n",
              " <unstructured.documents.elements.Table at 0x7c946e705540>,\n",
              " <unstructured.documents.elements.Table at 0x7c946f8e4bb0>]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for table in tables:\n",
        "  print('#### Table ###')\n",
        "  print(table.text)"
      ],
      "metadata": {
        "id": "py8q3SIxokor",
        "outputId": "f33d451e-fa98-4680-90f5-d0e8c2bc8a89",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#### Table ###\n",
            "Hit Rate\t​ Proportion of queries for which the system retrieves relevant chunks​ Mean Reciprocal Rank (MRR)​ MRR is the average of the reciprocal ranks of the first relevant item retrieved for each query. It emphasizes the ranking of the first relevant item.​ Precision@K​ Precision@K measures the proportion of retrieved items in the top-K rankings that are relevant to the query. Example: Let's say you retrieve 5 items for a query, out of which only 3 are relevant, then Precision @ 5 would be 3/5 or 0.6.​ Normalized Discounted Cumulative Gain (NDCG)​ NDCG measures the ranking quality by considering both relevance and rank position of retrieved items. It discounts the relevance of items appearing lower in the list​\n",
            "#### Table ###\n",
            "Contextual Relevance ​ This guardrail ensures that the responses generated by the language model are appropriate and related to the given context.​ Answer Relevance​ This guardrail focuses on evaluating generated responses that the provided information is pertinent to the user's query.​ Hallucination / Faithfulness​ This guardrail ensures that the model stays faithful to the input provided and does not generate false or misleading information.​ Bias​ Bias guardrail aims to check if the model is generating biased or discriminatory content based on factors such as race, gender, ethnicity, religion, or other sensitive attributes.​ Toxicity​ This guardrail is concerned with validating the presence of harmful, offensive, or inappropriate content.​\n",
            "#### Table ###\n",
            "Aspect RAG Fine-tuning (FT) Retrieval-Augmented Fine-Tuning (RAFT) Definition Integrates external data retrieval during generation to improve output quality. Adjusts model parameters using specific data to improve performance. Combines fine-tuning with external data retrieval for enhanced context and accuracy. When to Use Complex queries, domain-specific knowledge extraction. Model needs domain-specific adaptation. Tasks requiring high-context awareness and accuracy. Advantages Enhances context awareness, improves relevance of generated content. Tailors model to specific data, improves task-specific performance. Augments model with external data, improves accuracy and specificity. Disadvantages Requires robust retrieval systems, potential for irrelevant retrievals. Limited to available training data quality and diversity. Increased complexity in implementation and retrieval integration. Use Cases - Content generation for complex queries (e.g., medical diagnosis assistance).<br>- Domain-specific knowledge extraction (e.g., legal document summarization).<br>- Customer support for nuanced queries (e.g., technical troubleshooting).<br>- Generating creative content based on diverse inputs (e.g., marketing campaigns).<br>- Enhancing automated responses with real-time data (e.g., financial news updates). - Customer service automation (e.g., retail sector support).<br>- Specific industry applications (e.g., financial predictions).<br>- Personalized recommendations (e.g., e-commerce).<br>- Sentiment analysis (e.g., social media monitoring).<br>- Fraud detection (e.g., banking sector). - Question answering (e.g., internal knowledge base queries).<br>- Summarization tasks (e.g., news article summaries).<br>- Highly specific content generation (e.g., technical manuals).<br>- Contextual recommendations (e.g., healthcare treatment suggestions).<br>- Real-time decision support (e.g., emergency response coordination). Type of Language Models All Models Small & Medium Models Small & Medium Models Response Structure Comparison Provides detailed, context-aware responses using up-to-date information (e.g., troubleshooting steps based on latest device manuals). Delivers consistent, high-quality responses for predefined queries (e.g., billing inquiries). Combines detailed context with fine-tuned knowledge for nuanced inquiries (e.g., customized troubleshooting based on customer’s device history and current network status).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from unstructured.partition.docx import partition_docx\n",
        "from unstructured.documents.elements import Table\n",
        "from typing import List\n",
        "\n",
        "elements = partition_docx(filename='/content/docx/Data Enablement Service.docx')\n",
        "\n",
        "markdown_tables = []\n",
        "\n",
        "for element in elements:\n",
        "\n",
        "    if isinstance(element, Table):\n",
        "        markdown_table = []\n",
        "\n",
        "        # Get the table data\n",
        "        if element.text:\n",
        "            rows = element.text.strip().split('\\n')\n",
        "            if rows:\n",
        "                # Process header\n",
        "                header = [cell.strip() for cell in rows[0].split('\\t')]\n",
        "                markdown_table.append(\"| \" + \" | \".join(header) + \" |\")\n",
        "\n",
        "                # Add separator line\n",
        "                separator = \"|\" + \"|\".join([\" :--- \" for _ in header]) + \"|\"\n",
        "                markdown_table.append(separator)\n",
        "\n",
        "                # Process data rows\n",
        "                for row in rows[1:]:\n",
        "                    cells = [cell.strip() for cell in row.split('\\t')]\n",
        "                    # Replace any | characters with HTML entity\n",
        "                    cells = [cell.replace(\"|\", \"&#124;\") for cell in cells]\n",
        "                    markdown_table.append(\"| \" + \" | \".join(cells) + \" |\")\n",
        "\n",
        "                markdown_tables.append(\"\\n\".join(markdown_table))\n",
        "\n",
        "markdown_tables"
      ],
      "metadata": {
        "id": "KnUASyRzpbtx",
        "outputId": "ebff4263-1439-43f7-c126-e3a9a9cc51d9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"| Hit Rate | \\u200b Proportion of queries for which the system retrieves relevant chunks\\u200b Mean Reciprocal Rank (MRR)\\u200b MRR is the average of the reciprocal ranks of the first relevant item retrieved for each query. It emphasizes the ranking of the first relevant item.\\u200b Precision@K\\u200b Precision@K measures the proportion of retrieved items in the top-K rankings that are relevant to the query. Example: Let's say you retrieve 5 items for a query, out of which only 3 are relevant, then Precision @ 5 would be 3/5 or 0.6.\\u200b Normalized Discounted Cumulative Gain (NDCG)\\u200b NDCG measures the ranking quality by considering both relevance and rank position of retrieved items. It discounts the relevance of items appearing lower in the list\\u200b |\\n| :--- | :--- |\",\n",
              " \"| Contextual Relevance \\u200b This guardrail ensures that the responses generated by the language model are appropriate and related to the given context.\\u200b Answer Relevance\\u200b This guardrail focuses on evaluating generated responses that the provided information is pertinent to the user's query.\\u200b Hallucination / Faithfulness\\u200b This guardrail ensures that the model stays faithful to the input provided and does not generate false or misleading information.\\u200b Bias\\u200b Bias guardrail aims to check if the model is generating biased or discriminatory content based on factors such as race, gender, ethnicity, religion, or other sensitive attributes.\\u200b Toxicity\\u200b This guardrail is concerned with validating the presence of harmful, offensive, or inappropriate content.\\u200b |\\n| :--- |\",\n",
              " '| Aspect RAG Fine-tuning (FT) Retrieval-Augmented Fine-Tuning (RAFT) Definition Integrates external data retrieval during generation to improve output quality. Adjusts model parameters using specific data to improve performance. Combines fine-tuning with external data retrieval for enhanced context and accuracy. When to Use Complex queries, domain-specific knowledge extraction. Model needs domain-specific adaptation. Tasks requiring high-context awareness and accuracy. Advantages Enhances context awareness, improves relevance of generated content. Tailors model to specific data, improves task-specific performance. Augments model with external data, improves accuracy and specificity. Disadvantages Requires robust retrieval systems, potential for irrelevant retrievals. Limited to available training data quality and diversity. Increased complexity in implementation and retrieval integration. Use Cases - Content generation for complex queries (e.g., medical diagnosis assistance).<br>- Domain-specific knowledge extraction (e.g., legal document summarization).<br>- Customer support for nuanced queries (e.g., technical troubleshooting).<br>- Generating creative content based on diverse inputs (e.g., marketing campaigns).<br>- Enhancing automated responses with real-time data (e.g., financial news updates). - Customer service automation (e.g., retail sector support).<br>- Specific industry applications (e.g., financial predictions).<br>- Personalized recommendations (e.g., e-commerce).<br>- Sentiment analysis (e.g., social media monitoring).<br>- Fraud detection (e.g., banking sector). - Question answering (e.g., internal knowledge base queries).<br>- Summarization tasks (e.g., news article summaries).<br>- Highly specific content generation (e.g., technical manuals).<br>- Contextual recommendations (e.g., healthcare treatment suggestions).<br>- Real-time decision support (e.g., emergency response coordination). Type of Language Models All Models Small & Medium Models Small & Medium Models Response Structure Comparison Provides detailed, context-aware responses using up-to-date information (e.g., troubleshooting steps based on latest device manuals). Delivers consistent, high-quality responses for predefined queries (e.g., billing inquiries). Combines detailed context with fine-tuned knowledge for nuanced inquiries (e.g., customized troubleshooting based on customer’s device history and current network status). |\\n| :--- |']"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# *Error in table extraction using Unstructured*"
      ],
      "metadata": {
        "id": "TjarA9fQsNFg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# *Error in extracting images, not showing any of them*"
      ],
      "metadata": {
        "id": "U9E1W2BiuFTZ"
      }
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}