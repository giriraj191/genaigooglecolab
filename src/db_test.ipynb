{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "## data_fetch.py\n",
        "import pandas as pd\n",
        "from pyspark.sql import DataFrame\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "def load_mappings(mapping_file):\n",
        "    \"\"\"\n",
        "    Load the column mappings from the mapping document.\n",
        "    \"\"\"\n",
        "    # Load the first sheet of the mapping document\n",
        "    mapping_df = pd.read_excel(mapping_file)\n",
        "\n",
        "    # Create a dictionary of source-to-target mappings\n",
        "    column_mappings = dict(zip(mapping_df[\"Source\"], mapping_df[\"Target\"]))\n",
        "    return column_mappings\n",
        "\n",
        "\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "def apply_mappings(column_mappings):\n",
        "    \"\"\"\n",
        "    Parse source columns and target columns from mappings, handling multiple source columns.\n",
        "    \"\"\"\n",
        "    mapped_columns = []\n",
        "    for source_col, target_col in column_mappings.items():\n",
        "        source_columns = [col.strip() for col in source_col.split(\",\")]  # Split multi-source columns\n",
        "        mapped_columns.append((source_columns, target_col))\n",
        "    print(\"Mapped Columns for Validation:\", mapped_columns)\n",
        "    return mapped_columns\n",
        "\n",
        "\n",
        "\n",
        "def load_data(spark, source_path: str, target_path: str) -> (DataFrame, DataFrame):\n",
        "    \"\"\"\n",
        "    Load source and target datasets.\n",
        "    \"\"\"\n",
        "    def load_file(path: str) -> DataFrame:\n",
        "        if path.endswith(\".csv\"):\n",
        "            return spark.read.option(\"header\", \"true\").csv(path)\n",
        "        elif path.endswith(\".xlsx\"):\n",
        "            pdf = pd.read_excel(path)\n",
        "            return spark.createDataFrame(pdf)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported file format: {path}\")\n",
        "\n",
        "    # Load source and target data\n",
        "    source_df = load_file(source_path)\n",
        "    target_df = load_file(target_path)\n",
        "\n",
        "    print(f\"Source data loaded with {source_df.count()} records and {len(source_df.columns)} columns.\")\n",
        "    print(f\"Target data loaded with {target_df.count()} records and {len(target_df.columns)} columns.\")\n",
        "\n",
        "    return source_df, target_df"
      ],
      "metadata": {
        "id": "0FyYLEOhnx9x"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_primary_key(mapping_file):\n",
        "    \"\"\"\n",
        "    Extract the primary key column from the mapping file.\n",
        "    Automatically reads the first sheet and handles extra spaces in column names.\n",
        "    \"\"\"\n",
        "    # Load the first sheet of the mapping file\n",
        "    mapping_df = pd.read_excel(mapping_file)\n",
        "\n",
        "    # Strip spaces and clean column names\n",
        "    mapping_df.columns = mapping_df.columns.str.strip()\n",
        "    print(mapping_df.columns)\n",
        "\n",
        "    # Check if 'Primary Key' and 'Column Name' exist in the file\n",
        "    if \"Primary Key\" not in mapping_df.columns or \"Source\" not in mapping_df.columns:\n",
        "        raise ValueError(\"Required columns 'Primary Key' or 'Source' not found in the mapping file.\")\n",
        "\n",
        "    # Find the primary key column\n",
        "    primary_key_row = mapping_df[mapping_df[\"Primary Key\"] == \"Y\"]\n",
        "\n",
        "    if not primary_key_row.empty:\n",
        "        primary_key = primary_key_row.iloc[0][\"Source\"]\n",
        "        print(f\"Detected Primary Key: {primary_key}\")\n",
        "        return primary_key\n",
        "    else:\n",
        "        raise ValueError(\"No primary key detected in the mapping file.\")"
      ],
      "metadata": {
        "id": "9QWx1uB4uwE8"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def perform_dynamic_validation(source_df, target_df, mapped_columns, primary_key):\n",
        "    validation_results = []\n",
        "    metrics=[]\n",
        "\n",
        "    source_columns=source_df.columns\n",
        "    target_columns=target_df.columns\n",
        "\n",
        "    # Record Count Validation\n",
        "    source_count = source_df.count()\n",
        "    target_count = target_df.count()\n",
        "    validation_results.append({\n",
        "        \"ValidationType\": \"Record Count\",\n",
        "        \"Source\": source_count,\n",
        "        \"Target\": target_count\n",
        "    })\n",
        "\n",
        "    # Data Type and Null Validation for Mapped Columns\n",
        "    for source_cols, target_col in mapped_columns:\n",
        "        # Handle multiple source columns mapped to a single target column\n",
        "        source_dtype = [\n",
        "            [f.dataType for f in source_df.schema.fields if f.name == col][0]\n",
        "            for col in source_cols if col in source_df.columns\n",
        "        ]\n",
        "        target_dtype = [f.dataType for f in target_df.schema.fields if f.name == target_col][0]\n",
        "        validation_results.append({\n",
        "            \"ValidationType\": f\"Data Type ({', '.join(source_cols)} -> {target_col})\",\n",
        "            \"Source\": \", \".join(map(str, source_dtype)),\n",
        "            \"Target\": str(target_dtype)\n",
        "        })\n",
        "\n",
        "        # Null Validation for each source column\n",
        "        for source_col in source_cols:\n",
        "            if source_col in source_df.columns:\n",
        "                source_nulls = source_df.filter(F.col(source_col).isNull()).count()\n",
        "                target_nulls = target_df.filter(F.col(target_col).isNull()).count()\n",
        "                validation_results.append({\n",
        "                    \"ValidationType\": f\"Null Values ({source_col} -> {target_col})\",\n",
        "                    \"Source\": source_nulls,\n",
        "                    \"Target\": target_nulls\n",
        "                })\n",
        "\n",
        "    # Primary Key Validation\n",
        "    source_primary_key_count = source_df.select(primary_key).distinct().count()\n",
        "    target_primary_key_count = target_df.select(primary_key).distinct().count()\n",
        "\n",
        "    source_col_count = len(source_columns)\n",
        "    target_col_count = len(target_columns)\n",
        "\n",
        "    validation_results.append({\n",
        "        \"ValidationType\": \"Primary Key Uniqueness\",\n",
        "        \"Source\": source_primary_key_count,\n",
        "        \"Target\": target_primary_key_count\n",
        "    })\n",
        "\n",
        "    validation_results.append({\n",
        "            \"ValidationType\": \"ColumnCount\",\n",
        "            \"Source\": source_col_count,\n",
        "            \"Target\": target_col_count\n",
        "        })\n",
        "\n",
        "    metrics.append({\n",
        "        \"ValidationType\": \"Number of Rows\",\n",
        "        \"Source\": source_count,\n",
        "        \"Target\": target_count\n",
        "    })\n",
        "\n",
        "    # Number of Duplicate Rows (Basic Validation)\n",
        "    source_duplicates = source_df.groupBy(source_df.columns).count().filter(F.col(\"count\") > 1).count()\n",
        "    target_duplicates = target_df.groupBy(target_df.columns).count().filter(F.col(\"count\") > 1).count()\n",
        "    metrics.append({\n",
        "        \"ValidationType\": \"Number of Duplicate Rows\",\n",
        "        \"Source\": source_duplicates,\n",
        "        \"Target\": target_duplicates\n",
        "    })\n",
        "    metrics.append({\n",
        "        \"ValidationType\": \"Transformation Applied\",\n",
        "        \"Direct Move\": skipped_count,\n",
        "        \"Transformed Data Points\" : applied_count\n",
        "    })\n",
        "\n",
        "    return validation_results , metrics\n",
        "\n",
        "\n",
        "def save_dynamic_script(source_columns, target_columns, mapped_columns, primary_key):\n",
        "    \"\"\"\n",
        "    Save the dynamic validation logic as a .py file with user-provided values.\n",
        "    \"\"\"\n",
        "    script_content = f\"\"\"from pyspark.sql import SparkSession, functions as F\n",
        "\n",
        "def perform_dynamic_validation(source_df, target_df):\n",
        "    print(\\\"\\\\n=== Starting Validation ===\\\\n\\\")\n",
        "\n",
        "    # Record Count Validation\n",
        "    source_count = source_df.count()\n",
        "    target_count = target_df.count()\n",
        "    print(f\\\"Record Count Comparison:\\\\n  Source: {{source_count}}\\\\n  Target: {{target_count}}\\\\n\\\")\n",
        "\n",
        "    # Column Count Validation\n",
        "    source_column_count = len(source_df.columns)\n",
        "    target_column_count = len(target_df.columns)\n",
        "    print(f\\\"Column Count Comparison:\\\\n  Source: {{source_column_count}}\\\\n  Target: {{target_column_count}}\\\\n\\\")\n",
        "\n",
        "    # Data Type Validation\n",
        "\"\"\"\n",
        "    for source_col, target_col in mapped_columns:\n",
        "        script_content += f\"\"\"\n",
        "    for source_col, target_col in [(\\\"{source_col}\\\", \\\"{target_col}\\\")]:\n",
        "        source_dtype = [f.dataType for f in source_df.schema.fields if f.name == source_col][0]\n",
        "        target_dtype = [f.dataType for f in target_df.schema.fields if f.name == target_col][0]\n",
        "        print(f\\\"Data Type Comparison for '{{target_col}}':\\\\n  Source: {{source_dtype}}\\\\n  Target: {{target_dtype}}\\\\n\\\")\n",
        "\"\"\"\n",
        "\n",
        "    script_content += \"\"\"\n",
        "    # Null Values Validation\n",
        "\"\"\"\n",
        "    for source_col, target_col in mapped_columns:\n",
        "        script_content += f\"\"\"\n",
        "    for source_col, target_col in [(\\\"{source_col}\\\", \\\"{target_col}\\\")]:\n",
        "        source_nulls = source_df.filter(F.col(source_col).isNull()).count()\n",
        "        target_nulls = target_df.filter(F.col(target_col).isNull()).count()\n",
        "        print(f\\\"Null Values Comparison for '{{target_col}}':\\\\n  Source: {{source_nulls}} nulls\\\\n  Target: {{target_nulls}} nulls\\\\n\\\")\n",
        "\"\"\"\n",
        "\n",
        "    script_content += f\"\"\"\n",
        "    # Primary Key Uniqueness Validation\n",
        "    source_primary_key_count = source_df.select(\\\"{primary_key}\\\").distinct().count()\n",
        "    target_primary_key_count = target_df.select(\\\"{primary_key}\\\").distinct().count()\n",
        "    print(f\\\"Primary Key Uniqueness Validation:\\\\n  Source Unique Keys: {{source_primary_key_count}}\\\\n  Target Unique Keys: {{target_primary_key_count}}\\\\n\\\")\n",
        "\n",
        "if __name__ == \\\"__main__\\\":\n",
        "    spark = SparkSession.builder \\\\\n",
        "        .appName(\\\"Dynamic Validation\\\") \\\\\n",
        "        .master(\\\"local[*]\\\") \\\\\n",
        "        .getOrCreate()\n",
        "\n",
        "    source_data = [(\\\"A1\\\", \\\"Value1\\\", None),\n",
        "                   (\\\"A2\\\", \\\"Value2\\\", \\\"123\\\"),\n",
        "                   (\\\"A3\\\", \\\"Value3\\\", \\\"456\\\")]\n",
        "\n",
        "    target_data = [(\\\"A1\\\", \\\"ValueX\\\", None),\n",
        "                   (\\\"A2\\\", \\\"Value2\\\", None),\n",
        "                   (\\\"A3\\\", None, \\\"456\\\")]\n",
        "\n",
        "    source_df = spark.createDataFrame(source_data, {source_columns})\n",
        "    target_df = spark.createDataFrame(target_data, {target_columns})\n",
        "\n",
        "    perform_dynamic_validation(source_df, target_df)\n",
        "\"\"\"\n",
        "\n",
        "    # Save script to file\n",
        "    script_path = \"generated_dynamic_validation.py\"\n",
        "    with open(script_path, \"w\") as script_file:\n",
        "        script_file.write(script_content)\n",
        "    print(f\"Validation script has been saved to: {script_path}\")"
      ],
      "metadata": {
        "id": "ooZaIDZzu2E3"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "# def perform_full_validation(source_df, target_df, primary_key):\n",
        "#     \"\"\"\n",
        "#     Perform full validation between source and target datasets.\n",
        "#     Outputs mismatched records with details in the desired format, sorted by PrimaryKey.\n",
        "#     \"\"\"\n",
        "#     if not hasattr(source_df, \"columns\") or not hasattr(target_df, \"columns\"):\n",
        "#         raise TypeError(\"Both source_df and target_df must be valid PySpark DataFrames.\")\n",
        "\n",
        "#     # Identify common columns for validation\n",
        "#     common_columns = [col for col in source_df.columns if col in target_df.columns]\n",
        "#     if not common_columns:\n",
        "#         raise ValueError(\"No common columns found between source and target DataFrames.\")\n",
        "\n",
        "#     # Standardize data types for comparison\n",
        "#     for col in common_columns:\n",
        "#         source_df = source_df.withColumn(col, F.col(col).cast(\"string\"))\n",
        "#         target_df = target_df.withColumn(col, F.col(col).cast(\"string\"))\n",
        "\n",
        "#     # Perform full outer join on the primary key\n",
        "#     joined_df = source_df.alias(\"source\").join(\n",
        "#         target_df.alias(\"target\"),\n",
        "#         F.col(f\"source.{primary_key}\") == F.col(f\"target.{primary_key}\"),\n",
        "#         how=\"full_outer\"\n",
        "#     )\n",
        "\n",
        "#     mismatched_records = []\n",
        "#     missing_keys = set()  # To track keys already marked as missing\n",
        "\n",
        "#     # Handle missing keys in source\n",
        "#     missing_in_source = joined_df.filter(F.col(f\"source.{primary_key}\").isNull()) \\\n",
        "#         .select(\n",
        "#             F.col(f\"target.{primary_key}\").alias(\"PrimaryKey\"),\n",
        "#             *[F.col(f\"target.{col}\").alias(col) for col in common_columns]\n",
        "#         )\n",
        "\n",
        "#     for row in missing_in_source.collect():\n",
        "#         row_dict = row.asDict()\n",
        "#         missing_keys.add(row_dict[\"PrimaryKey\"])  # Track the key as missing\n",
        "#         mismatched_records.append({\n",
        "#             \"PrimaryKey\": row_dict[\"PrimaryKey\"],\n",
        "#             \"Column\": \"All Columns\",\n",
        "#             \"SourceValue\": None,\n",
        "#             \"TargetValue\": {col: row_dict[col] for col in common_columns},\n",
        "#             \"Reason\": \"Missing Key in Source\"\n",
        "#         })\n",
        "\n",
        "#     # Handle missing keys in target\n",
        "#     missing_in_target = joined_df.filter(F.col(f\"target.{primary_key}\").isNull()) \\\n",
        "#         .select(\n",
        "#             F.col(f\"source.{primary_key}\").alias(\"PrimaryKey\"),\n",
        "#             *[F.col(f\"source.{col}\").alias(col) for col in common_columns]\n",
        "#         )\n",
        "\n",
        "#     for row in missing_in_target.collect():\n",
        "#         row_dict = row.asDict()\n",
        "#         missing_keys.add(row_dict[\"PrimaryKey\"])  # Track the key as missing\n",
        "#         mismatched_records.append({\n",
        "#             \"PrimaryKey\": row_dict[\"PrimaryKey\"],\n",
        "#             \"Column\": \"All Columns\",\n",
        "#             \"SourceValue\": {col: row_dict[col] for col in common_columns},\n",
        "#             \"TargetValue\": None,\n",
        "#             \"Reason\": \"Missing Key in Target\"\n",
        "#         })\n",
        "\n",
        "#     # Handle column mismatches, skipping keys already marked as missing\n",
        "#     for col in common_columns:\n",
        "#         mismatched_df = joined_df.filter(\n",
        "#             (F.col(f\"source.{col}\") != F.col(f\"target.{col}\")) |\n",
        "#             (F.col(f\"source.{col}\").isNull() & F.col(f\"target.{col}\").isNotNull()) |\n",
        "#             (F.col(f\"source.{col}\").isNotNull() & F.col(f\"target.{col}\").isNull())\n",
        "#         ).select(\n",
        "#             F.coalesce(F.col(f\"source.{primary_key}\"), F.col(f\"target.{primary_key}\")).alias(\"PrimaryKey\"),\n",
        "#             F.lit(col).alias(\"Column\"),\n",
        "#             F.col(f\"source.{col}\").alias(\"SourceValue\"),\n",
        "#             F.col(f\"target.{col}\").alias(\"TargetValue\")\n",
        "#         )\n",
        "\n",
        "#         for row in mismatched_df.collect():\n",
        "#             row_dict = row.asDict()\n",
        "#             primary_key_value = row_dict[\"PrimaryKey\"]\n",
        "\n",
        "#             # Skip if the key is already marked as missing\n",
        "#             if primary_key_value in missing_keys:\n",
        "#                 continue\n",
        "\n",
        "#             reason = \"Null Mismatch\" if row_dict[\"SourceValue\"] is None or row_dict[\"TargetValue\"] is None else \"Value Mismatch\"\n",
        "#             mismatched_records.append({\n",
        "#                 \"PrimaryKey\": primary_key_value,\n",
        "#                 \"Column\": row_dict[\"Column\"],\n",
        "#                 \"SourceValue\": row_dict[\"SourceValue\"],\n",
        "#                 \"TargetValue\": row_dict[\"TargetValue\"],\n",
        "#                 \"Reason\": reason\n",
        "#             })\n",
        "\n",
        "#     # Sort the final records by PrimaryKey\n",
        "#     mismatched_records.sort(key=lambda x: x[\"PrimaryKey\"])\n",
        "\n",
        "#     return mismatched_records\n",
        "\n",
        "\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "def perform_full_validation(source_df, target_df, primary_key):\n",
        "    \"\"\"\n",
        "    Perform full validation between source and target datasets.\n",
        "    Outputs mismatched records with details in the desired format, sorted by PrimaryKey.\n",
        "    \"\"\"\n",
        "\n",
        "    if not hasattr(source_df, \"columns\") or not hasattr(target_df, \"columns\"):\n",
        "        raise TypeError(\"Both source_df and target_df must be valid PySpark DataFrames.\")\n",
        "\n",
        "    # Identify common columns for validation\n",
        "    common_columns = [col for col in source_df.columns if col in target_df.columns]\n",
        "    if not common_columns:\n",
        "        raise ValueError(\"No common columns found between source and target DataFrames.\")\n",
        "\n",
        "    # Standardize data types for comparison\n",
        "    for col in common_columns:\n",
        "        source_df = source_df.withColumn(col, F.col(col).cast(\"string\"))\n",
        "        target_df = target_df.withColumn(col, F.col(col).cast(\"string\"))\n",
        "\n",
        "    # Check for duplicates in source and target\n",
        "    duplicate_in_source = source_df.groupBy(primary_key).count().filter(F.col(\"count\") > 1)\n",
        "    duplicate_in_target = target_df.groupBy(primary_key).count().filter(F.col(\"count\") > 1)\n",
        "\n",
        "    # Perform full outer join on the primary key\n",
        "    joined_df = source_df.alias(\"source\").join(\n",
        "        target_df.alias(\"target\"),\n",
        "        F.col(f\"source.{primary_key}\") == F.col(f\"target.{primary_key}\"),\n",
        "        how=\"full_outer\"\n",
        "    )\n",
        "\n",
        "    mismatched_records = []\n",
        "    missing_keys = set()  # To track keys already marked as missing\n",
        "\n",
        "    # Handle missing keys in source\n",
        "    missing_in_source = joined_df.filter(F.col(f\"source.{primary_key}\").isNull()) \\\n",
        "        .select(\n",
        "            F.col(f\"target.{primary_key}\").alias(\"PrimaryKey\"),\n",
        "            *[F.col(f\"target.{col}\").alias(col) for col in common_columns]\n",
        "        )\n",
        "\n",
        "    for row in missing_in_source.collect():\n",
        "        row_dict = row.asDict()\n",
        "        missing_keys.add(row_dict[\"PrimaryKey\"])  # Track the key as missing\n",
        "        mismatched_records.append({\n",
        "            \"PrimaryKey\": row_dict[\"PrimaryKey\"],\n",
        "            \"Column\": \"All Columns\",\n",
        "            \"SourceValue\": None,\n",
        "            \"TargetValue\": {col: row_dict[col] for col in common_columns},\n",
        "            \"Reason\": \"Missing Key in Source\"\n",
        "        })\n",
        "\n",
        "    # Handle missing keys in target\n",
        "    missing_in_target = joined_df.filter(F.col(f\"target.{primary_key}\").isNull()) \\\n",
        "        .select(\n",
        "            F.col(f\"source.{primary_key}\").alias(\"PrimaryKey\"),\n",
        "            *[F.col(f\"source.{col}\").alias(col) for col in common_columns]\n",
        "        )\n",
        "\n",
        "    for row in missing_in_target.collect():\n",
        "        row_dict = row.asDict()\n",
        "        missing_keys.add(row_dict[\"PrimaryKey\"])  # Track the key as missing\n",
        "        mismatched_records.append({\n",
        "            \"PrimaryKey\": row_dict[\"PrimaryKey\"],\n",
        "            \"Column\": \"All Columns\",\n",
        "            \"SourceValue\": {col: row_dict[col] for col in common_columns},\n",
        "            \"TargetValue\": None,\n",
        "            \"Reason\": \"Missing Key in Target\"\n",
        "        })\n",
        "\n",
        "    # Handle column mismatches, skipping keys already marked as missing\n",
        "    for col in common_columns:\n",
        "        mismatched_df = joined_df.filter(\n",
        "            (F.col(f\"source.{col}\") != F.col(f\"target.{col}\")) |\n",
        "            (F.col(f\"source.{col}\").isNull() & F.col(f\"target.{col}\").isNotNull()) |\n",
        "            (F.col(f\"source.{col}\").isNotNull() & F.col(f\"target.{col}\").isNull())\n",
        "        ).select(\n",
        "            F.coalesce(F.col(f\"source.{primary_key}\"), F.col(f\"target.{primary_key}\")).alias(\"PrimaryKey\"),\n",
        "            F.lit(col).alias(\"Column\"),\n",
        "            F.col(f\"source.{col}\").alias(\"SourceValue\"),\n",
        "            F.col(f\"target.{col}\").alias(\"TargetValue\")\n",
        "        )\n",
        "\n",
        "        for row in mismatched_df.collect():\n",
        "            row_dict = row.asDict()\n",
        "            primary_key_value = row_dict[\"PrimaryKey\"]\n",
        "\n",
        "            # Skip if the key is already marked as missing\n",
        "            if primary_key_value in missing_keys:\n",
        "                continue\n",
        "\n",
        "            reason = \"Null Mismatch\" if row_dict[\"SourceValue\"] is None or row_dict[\"TargetValue\"] is None else \"Value Mismatch\"\n",
        "            mismatched_records.append({\n",
        "                \"PrimaryKey\": primary_key_value,\n",
        "                \"Column\": row_dict[\"Column\"],\n",
        "                \"SourceValue\": row_dict[\"SourceValue\"],\n",
        "                \"TargetValue\": row_dict[\"TargetValue\"],\n",
        "                \"Reason\": reason\n",
        "            })\n",
        "\n",
        "    # Handle duplicate keys\n",
        "    duplicate_source_keys = [row.asDict()[primary_key] for row in duplicate_in_source.select(primary_key).collect()]\n",
        "    duplicate_target_keys = [row.asDict()[primary_key] for row in duplicate_in_target.select(primary_key).collect()]\n",
        "\n",
        "    for key in duplicate_source_keys:\n",
        "        mismatched_records.append({\n",
        "            \"PrimaryKey\": key,\n",
        "            \"Column\": \"All Columns\",\n",
        "            \"SourceValue\": None,\n",
        "            \"TargetValue\": None,\n",
        "            \"Reason\": \"Duplicate Key in Source\"\n",
        "        })\n",
        "\n",
        "    for key in duplicate_target_keys:\n",
        "        mismatched_records.append({\n",
        "            \"PrimaryKey\": key,\n",
        "            \"Column\": \"All Columns\",\n",
        "            \"SourceValue\": None,\n",
        "            \"TargetValue\": None,\n",
        "            \"Reason\": \"Duplicate Key in Target\"\n",
        "        })\n",
        "\n",
        "    # Sort the final records by PrimaryKey\n",
        "    mismatched_records.sort(key=lambda x: x[\"PrimaryKey\"])\n",
        "\n",
        "    return mismatched_records\n",
        "\n",
        "\n",
        "def save_full_validation_script(source_columns, target_columns, mapped_columns, primary_key):\n",
        "    \"\"\"\n",
        "    Save the full validation logic as a .py file with user-provided values.\n",
        "    \"\"\"\n",
        "    script_content = f\"\"\"from pyspark.sql import SparkSession, functions as F\n",
        "\n",
        "def perform_full_validation(source_df, target_df):\n",
        "    print(\\\"\\\\n=== Starting Full Validation ===\\\\n\\\")\n",
        "\n",
        "    mismatched_records = []\n",
        "\n",
        "    # Prepare join condition using the primary key\n",
        "    join_condition = source_df[\\\"{primary_key}\\\"] == target_df[\\\"{primary_key}\\\"]\n",
        "\n",
        "    # Perform a full outer join\n",
        "    joined_df = source_df.alias(\\\"source\\\").join(\n",
        "        target_df.alias(\\\"target\\\"),\n",
        "        on=join_condition,\n",
        "        how=\\\"full_outer\\\"\n",
        "    )\n",
        "\n",
        "    # Check for mismatched columns\n",
        "\"\"\"\n",
        "    for source_col, target_col in mapped_columns:\n",
        "        script_content += f\"\"\"\n",
        "    condition = (\n",
        "        (F.col(f\\\"source.{source_col}\\\") != F.col(f\\\"target.{target_col}\\\")) |\n",
        "        (F.col(f\\\"source.{source_col}\\\").isNull() & F.col(f\\\"target.{target_col}\\\").isNotNull()) |\n",
        "        (F.col(f\\\"source.{source_col}\\\").isNotNull() & F.col(f\\\"target.{target_col}\\\").isNull())\n",
        "    )\n",
        "\n",
        "    mismatched_df = joined_df.filter(condition).select(\n",
        "        F.coalesce(F.col(f\\\"source.{primary_key}\\\"), F.col(f\\\"target.{primary_key}\\\")).alias(\\\"Primary Key\\\"),\n",
        "        F.col(f\\\"source.{source_col}\\\").alias(f\\\"Source {source_col}\\\"),\n",
        "        F.col(f\\\"target.{target_col}\\\").alias(f\\\"Target {target_col}\\\")\n",
        "    )\n",
        "\n",
        "    for row in mismatched_df.collect():\n",
        "        mismatched_records.append({{\n",
        "            \\\"Primary Key\\\": row[\\\"Primary Key\\\"],\n",
        "            f\\\"Source {source_col}\\\": row[f\\\"Source {source_col}\\\"],\n",
        "            f\\\"Target {target_col}\\\": row[f\\\"Target {target_col}\\\"]\n",
        "        }})\n",
        "\"\"\"\n",
        "\n",
        "    script_content += \"\"\"\n",
        "    # Output mismatched records\n",
        "    print(\\\"\\\\n=== Mismatched Records ===\\\\n\\\")\n",
        "    for record in mismatched_records:\n",
        "        print(record)\n",
        "\"\"\"\n",
        "\n",
        "    # Save script to file\n",
        "    script_path = \"generated_full_validation.py\"\n",
        "    with open(script_path, \"w\") as script_file:\n",
        "        script_file.write(script_content)\n",
        "    print(f\"Full validation script has been saved to: {script_path}\")"
      ],
      "metadata": {
        "id": "6xioTOWNSMZs"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yv74raA0QL8b",
        "outputId": "089ee489-d5ea-4ca7-94cb-1ff2b5aced4d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-openai\n",
            "  Downloading langchain_openai-0.3.3-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting langchain-core<0.4.0,>=0.3.33 (from langchain-openai)\n",
            "  Downloading langchain_core-0.3.33-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.58.1 in /usr/local/lib/python3.11/dist-packages (from langchain-openai) (1.59.9)\n",
            "Collecting tiktoken<1,>=0.7 (from langchain-openai)\n",
            "  Downloading tiktoken-0.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.33->langchain-openai) (6.0.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.33->langchain-openai) (1.33)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.33->langchain-openai) (0.3.2)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.33->langchain-openai) (24.2)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.33->langchain-openai) (2.10.6)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.33->langchain-openai) (9.0.0)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.33->langchain-openai) (4.12.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (0.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (4.67.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2.32.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.58.1->langchain-openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.58.1->langchain-openai) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.58.1->langchain-openai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.58.1->langchain-openai) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.33->langchain-openai) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.33->langchain-openai) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.33->langchain-openai) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.33->langchain-openai) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.33->langchain-openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.33->langchain-openai) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (2.3.0)\n",
            "Downloading langchain_openai-0.3.3-py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.3.33-py3-none-any.whl (412 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m412.7/412.7 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tiktoken, langchain-core, langchain-openai\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.32\n",
            "    Uninstalling langchain-core-0.3.32:\n",
            "      Successfully uninstalled langchain-core-0.3.32\n",
            "Successfully installed langchain-core-0.3.33 langchain-openai-0.3.3 tiktoken-0.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-dotenv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p61iSdUNQt-C",
        "outputId": "88242259-e69d-496c-df78-2c0fbd12547e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: python-dotenv\n",
            "Successfully installed python-dotenv-1.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from pyspark.sql import functions as F\n",
        "from langchain_openai import AzureChatOpenAI\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "# Azure OpenAI setup\n",
        "azure_openai_endpoint = os.getenv(\"azure_openai_endpoint\")\n",
        "azure_openai_api_key = os.getenv(\"azure_openai_api_key\")\n",
        "azure_openai_deployment_name = os.getenv(\"azure_openai_deployment_name\")\n",
        "azure_openai_api_version = os.getenv(\"azure_openai_api_version\")\n",
        "\n",
        "# Initialize AzureChatOpenAI\n",
        "llm = AzureChatOpenAI(\n",
        "    azure_endpoint=azure_openai_endpoint,\n",
        "    api_key=azure_openai_api_key,\n",
        "    deployment_name=azure_openai_deployment_name,\n",
        "    api_version=azure_openai_api_version,\n",
        "    temperature=0,\n",
        "    max_tokens=4096\n",
        ")\n",
        "\n",
        "def sanitize_output(response):\n",
        "    \"\"\"\n",
        "    Cleans the output to ensure it only contains valid PySpark transformation logic.\n",
        "    Removes unwanted newlines and ensures single-line expressions.\n",
        "    \"\"\"\n",
        "    sanitized = response.strip().replace(\"```python\", \"\").replace(\"```\", \"\").strip()\n",
        "    return \" \".join(sanitized.splitlines())\n",
        "\n",
        "def azure_translate_logic(logic):\n",
        "    \"\"\"\n",
        "    Translates logic into a single-line PySpark expression using Azure OpenAI.\n",
        "    \"\"\"\n",
        "    prompt = f\"\"\"\n",
        "    Translate the following transformation logic into a concise and valid PySpark expression.\n",
        "    Ensure the following:\n",
        "    - Use appropriate PySpark functions such as F.col, F.when, F.to_date, F.date_format, etc., for date columns.\n",
        "    - Handle dates explicitly in the format 'dd-MM-yyyy' where necessary.\n",
        "    - Return the transformation logic expression as a single line (e.g., F.col(...), F.to_date(...)).\n",
        "    - Do not include DataFrame references, assignments, or import statements.\n",
        "    - Avoid multiline expressions or line continuations.\n",
        "    Transformation Logic: {logic}\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        response = llm.invoke(prompt.strip())\n",
        "        return sanitize_output(response.content)\n",
        "    except Exception as e:\n",
        "        return f\"# Error translating logic: {str(e)}\"\n",
        "\n",
        "def generate_logic_safe(row):\n",
        "    \"\"\"\n",
        "    Generate a concise, single-line PySpark expression for a given row.\n",
        "    \"\"\"\n",
        "    transformation_logic = row.get(\"Transformation Logic\", \"\").strip()\n",
        "\n",
        "    if \"Direct Move\" in transformation_logic:\n",
        "        return \"# Skipped: No valid transformation logic provided\"\n",
        "\n",
        "    # Translate the logic using Azure OpenAI\n",
        "    return azure_translate_logic(transformation_logic)\n",
        "\n",
        "def apply_transformations_with_date_handling(source_df, mapping_df):\n",
        "    \"\"\"\n",
        "    Apply transformations to the source DataFrame with explicit handling for date columns in 'dd-MM-yyyy' format.\n",
        "    \"\"\"\n",
        "    for _, row in mapping_df.iterrows():\n",
        "        target_col = row[\"Target\"]\n",
        "        pyspark_logic = row[\"PySpark Logic\"]\n",
        "\n",
        "        if pd.notna(pyspark_logic) and pyspark_logic.strip() and pyspark_logic != \"# Skipped: No valid transformation logic provided\":\n",
        "            print(f\"\\n=== Applying Transformation for Column: '{target_col}' ===\")\n",
        "            try:\n",
        "                print(f\"Logic for '{target_col}': {pyspark_logic.strip()}\")\n",
        "                # Evaluate and apply the transformation logic\n",
        "                transformed_expr = eval(pyspark_logic.strip())\n",
        "\n",
        "                # Explicitly cast to date if the logic involves date handling\n",
        "                if \"to_date\" in pyspark_logic or \"date_format\" in pyspark_logic:\n",
        "                    source_df = source_df.withColumn(target_col, F.to_date(F.col(target_col), \"dd-MM-yyyy\"))\n",
        "                else:\n",
        "                    source_df = source_df.withColumn(target_col, transformed_expr)\n",
        "\n",
        "                print(f\"Transformation applied successfully for column: '{target_col}'\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error applying logic for column '{target_col}': {e}\")\n",
        "        else:\n",
        "            print(f\"Skipping transformation for column '{target_col}' as no valid PySpark Logic is provided.\")\n",
        "    return source_df\n",
        "\n",
        "def process_mapping_logic(mapping_df):\n",
        "    \"\"\"\n",
        "    Process the mapping DataFrame row by row to generate concise PySpark expressions.\n",
        "    \"\"\"\n",
        "    mapping_df[\"PySpark Logic\"] = mapping_df.apply(generate_logic_safe, axis=1)\n",
        "    mapping_df.to_csv(\"transformed.csv\", index=False)\n",
        "    return mapping_df\n",
        "\n",
        "def process_all_sheets(file_path):\n",
        "    \"\"\"\n",
        "    Process all sheets in the Excel file and save the results in a single Excel file.\n",
        "    \"\"\"\n",
        "    sheets = pd.read_excel(file_path, sheet_name=None)  # Load all sheets as a dictionary\n",
        "    writer = pd.ExcelWriter(\"Processed_ETL_Mapping.xlsx\", engine=\"xlsxwriter\")\n",
        "\n",
        "    for sheet_name, df in sheets.items():\n",
        "        print(f\"\\nProcessing sheet: {sheet_name}\")\n",
        "        processed_df = process_mapping_logic(df)\n",
        "        processed_df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
        "\n",
        "    writer.close()\n",
        "    print(\"\\nAll sheets processed and saved to 'Processed_ETL_Mapping.xlsx'.\")\n"
      ],
      "metadata": {
        "id": "tgcWV4bCPo-Q"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from pyspark.sql import SparkSession, functions as F, types as T\n",
        "from pyspark.sql.window import Window\n",
        "import json\n",
        "import os ,sys\n",
        "\n",
        "\n",
        "def is_json_string(value):\n",
        "    \"\"\"\n",
        "    Check if a given string is a valid JSON.\n",
        "    \"\"\"\n",
        "    if isinstance(value, str):\n",
        "        try:\n",
        "            json.loads(value)\n",
        "            return True\n",
        "        except json.JSONDecodeError:\n",
        "            return False\n",
        "    return False\n",
        "\n",
        "\n",
        "def normalize_json_fields(df):\n",
        "    \"\"\"\n",
        "    Normalize all JSON-like fields in a DataFrame.\n",
        "    Detect JSON-like columns and clean their values.\n",
        "    \"\"\"\n",
        "    json_columns = []\n",
        "\n",
        "    for col in df.columns:\n",
        "        if df[col].apply(lambda x: is_json_string(x)).any():\n",
        "            json_columns.append(col)\n",
        "            # Normalize the column\n",
        "            df[col] = df[col].apply(lambda x: json.loads(x) if is_json_string(x) else x)\n",
        "\n",
        "    #print(f\"Detected JSON columns: {json_columns}\")\n",
        "    return df, json_columns\n",
        "\n",
        "\n",
        "def detect_and_drop_datetime_columns(df):\n",
        "    \"\"\"\n",
        "    Detect columns with values resembling date/datetime strings and drop them.\n",
        "    \"\"\"\n",
        "    datetime_columns = []\n",
        "    for col in df.columns:\n",
        "        # Check if column values match date/datetime patterns\n",
        "        sample_values = df.select(col).filter(F.col(col).isNotNull()).limit(100).rdd.flatMap(lambda x: x).collect()\n",
        "        for value in sample_values:\n",
        "            if isinstance(value, str):\n",
        "                try:\n",
        "                    # Attempt parsing with a common date format\n",
        "                    pd.to_datetime(value, format=\"%d-%m-%Y\", errors='raise')\n",
        "                    datetime_columns.append(col)\n",
        "                    break\n",
        "                except:\n",
        "                    pass\n",
        "    if datetime_columns:\n",
        "        #print(f\"Detected and Dropping DateTime columns: {datetime_columns}\")\n",
        "        df = df.drop(*datetime_columns)\n",
        "    return df\n",
        "\n",
        "\n",
        "def clean_dataframe(df):\n",
        "    \"\"\"\n",
        "    Clean and normalize JSON-like fields in a Spark DataFrame.\n",
        "    \"\"\"\n",
        "    for col in df.columns:\n",
        "        df = df.withColumn(col, F.udf(lambda x: json.dumps(json.loads(x)) if is_json_string(x) else x, T.StringType())(F.col(col)))\n",
        "    return df\n",
        "\n",
        "def validate_metadata(source_df, target_df, mapped_columns):\n",
        "    \"\"\"\n",
        "    Validate if the mapped columns exist in the source and target DataFrames.\n",
        "    \"\"\"\n",
        "    source_columns = set(source_df.columns)\n",
        "    target_columns = set(target_df.columns)\n",
        "    mapped_columns_set = set(mapped_columns)\n",
        "\n",
        "    missing_in_source = mapped_columns_set - source_columns\n",
        "    missing_in_target = mapped_columns_set - target_columns\n",
        "\n",
        "    if missing_in_source or missing_in_target:\n",
        "        return False, missing_in_source, missing_in_target\n",
        "    return True, None, None\n",
        "\n",
        "\n",
        "# def apply_transformations_with_debugging(source_df, mapping_df):\n",
        "#     \"\"\"\n",
        "#     Apply transformations to the source DataFrame based on PySpark Logic with detailed debugging.\n",
        "#     For multi-column transformations, append the same transformed values back to all involved Source columns.\n",
        "#     Also, count the successfully applied transformations and those skipped.\n",
        "#     \"\"\"\n",
        "#     from pyspark.sql import functions as F\n",
        "\n",
        "#     # Step 1: Create a copy of the source DataFrame for transformations\n",
        "#     transformed_df = source_df  # This will hold all transformed values\n",
        "#     original_df = source_df  # Keep the original DataFrame intact\n",
        "\n",
        "#     applied_count = 0\n",
        "#     skipped_count = 0\n",
        "\n",
        "#     for _, row in mapping_df.iterrows():\n",
        "#         source_col = row[\"Source\"]  # Source column name(s)\n",
        "#         target_col = row[\"Target\"]  # Target column name\n",
        "#         pyspark_logic = row[\"PySpark Logic\"]  # PySpark logic to apply\n",
        "\n",
        "#         if pd.notna(pyspark_logic) and pyspark_logic.strip() and pyspark_logic != \"# No valid transformation logic available\":\n",
        "#             print(f\"\\n=== Applying Transformation for Target Column: '{target_col}' ===\")\n",
        "#             try:\n",
        "#                 # Parse the source columns (e.g., split \"Col1, Col2\" into a list)\n",
        "#                 source_columns = [col.strip() for col in source_col.split(\",\")]\n",
        "\n",
        "#                 # Evaluate the transformation logic dynamically\n",
        "#                 print(f\"Logic for '{target_col}': {pyspark_logic.strip()}\")\n",
        "#                 transformed_expr = eval(pyspark_logic.strip(), {\"F\": F, \"source_df\": original_df})\n",
        "\n",
        "#                 # Add the transformed column to the result DataFrame\n",
        "#                 transformed_df = transformed_df.withColumn(target_col, transformed_expr)\n",
        "\n",
        "#                 # Append the same transformed value back to all Source columns involved\n",
        "#                 for col in source_columns:\n",
        "#                     if col in transformed_df.columns:\n",
        "#                         transformed_df = transformed_df.withColumn(col, F.col(target_col))\n",
        "#                         print(f\"Appended transformed value to Source column: '{col}'\")\n",
        "\n",
        "#                 applied_count += 1\n",
        "#                 print(f\"Transformation applied successfully for column: '{target_col}'\")\n",
        "\n",
        "#             except Exception as e:\n",
        "#                 print(f\"Error applying logic for column '{target_col}': {e}\")\n",
        "#                 skipped_count += 1\n",
        "#         else:\n",
        "#             print(f\"Skipping transformation for column '{target_col}' as no valid PySpark Logic is provided.\")\n",
        "#             skipped_count += 1\n",
        "\n",
        "#     return transformed_df, applied_count, skipped_count\n",
        "\n",
        "\n",
        "# def export_transformed_df(transformed_df, output_path: str = \"Transformed_Data.csv\"):\n",
        "#     \"\"\"\n",
        "#     Export the transformed PySpark DataFrame to a CSV file.\n",
        "#     \"\"\"\n",
        "#     transformed_df.toPandas().to_csv(output_path, index=False)\n",
        "#     print(f\"Transformed DataFrame exported to {output_path}.\")\n",
        "\n",
        "def apply_transformations_with_debugging(source_df, mapping_df):\n",
        "    \"\"\"\n",
        "    Apply transformations to a copy of the source DataFrame with detailed debugging.\n",
        "    Use the original source data as a reference during transformations.\n",
        "    Handle multi-column transformations by assigning the transformed value to all involved columns.\n",
        "    \"\"\"\n",
        "    from pyspark.sql import functions as F\n",
        "\n",
        "    # Step 1: Create a copy of the source DataFrame for transformations\n",
        "    transformed_df = source_df.alias(\"transformed_copy\")  # Copy for applying transformations\n",
        "    original_df = source_df.alias(\"original_source\")  # Keep the original source data intact\n",
        "\n",
        "    applied_count = 0\n",
        "    skipped_count = 0\n",
        "\n",
        "    for _, row in mapping_df.iterrows():\n",
        "        source_col = row[\"Source\"]  # Source column name(s)\n",
        "        target_col = row[\"Target\"]  # Target column name\n",
        "        pyspark_logic = row[\"PySpark Logic\"]  # PySpark logic to apply\n",
        "\n",
        "        if pd.notna(pyspark_logic) and pyspark_logic.strip() and pyspark_logic != \"# No valid transformation logic available\":\n",
        "            print(f\"\\n=== Applying Transformation for Target Column: '{target_col}' ===\")\n",
        "            try:\n",
        "                # Parse the source columns (e.g., split \"Col1, Col2\" into a list)\n",
        "                source_columns = [col.strip() for col in source_col.split(\",\")]\n",
        "\n",
        "                # Build the transformation logic using the original DataFrame as reference\n",
        "                print(f\"Logic for '{target_col}': {pyspark_logic.strip()}\")\n",
        "                transformed_expr = eval(pyspark_logic.strip(), {\"F\": F, \"source_df\": original_df})\n",
        "\n",
        "                # Add the transformed column to the transformed DataFrame\n",
        "                transformed_df = transformed_df.withColumn(target_col, transformed_expr)\n",
        "\n",
        "                # If multiple source columns are involved, ensure their transformed values remain intact in the reference\n",
        "                for col in source_columns:\n",
        "                    if col in transformed_df.columns:\n",
        "                        transformed_df = transformed_df.withColumn(col, F.col(target_col))\n",
        "                        print(f\"Updated Source column '{col}' with transformed value.\")\n",
        "\n",
        "                applied_count += 1\n",
        "                print(f\"Transformation applied successfully for column: '{target_col}'\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error applying logic for column '{target_col}': {e}\")\n",
        "                skipped_count += 1\n",
        "        else:\n",
        "            print(f\"Skipping transformation for column '{target_col}' as no valid PySpark Logic is provided.\")\n",
        "            skipped_count += 1\n",
        "\n",
        "    print(f\"\\nTotal Transformations Applied: {applied_count}\")\n",
        "    print(f\"Total Transformations Skipped: {skipped_count}\")\n",
        "\n",
        "    return transformed_df, applied_count, skipped_count\n",
        "\n",
        "def export_transformed_df(transformed_df, output_path: str = \"Transformed_Data.csv\"):\n",
        "    \"\"\"\n",
        "    Export the transformed PySpark DataFrame to a CSV file.\n",
        "    \"\"\"\n",
        "    transformed_df.toPandas().to_csv(output_path, index=False)\n",
        "    print(f\"Transformed DataFrame exported to {output_path}.\")\n",
        "\n",
        "\n",
        "def debug_all_transformations(mapping_df):\n",
        "    \"\"\"\n",
        "    Debug transformation logic for all columns in the mapping DataFrame.\n",
        "    \"\"\"\n",
        "    print(\"\\n=== Debugging All Transformations ===\")\n",
        "    for _, row in mapping_df.iterrows():\n",
        "        target_col = row[\"Target\"]\n",
        "        pyspark_logic = row[\"PySpark Logic\"]\n",
        "\n",
        "        print(f\"\\n=== Debugging Column: '{target_col}' ===\")\n",
        "        if pd.notna(pyspark_logic) and pyspark_logic.strip():\n",
        "            print(f\"Logic: {pyspark_logic.strip()}\")\n",
        "        else:\n",
        "            print(f\"No valid logic for column: '{target_col}'\")\n",
        "\n",
        "import io\n",
        "\n",
        "def write_validation_results_to_excel(validation_results):\n",
        "    \"\"\"\n",
        "    Write validation results to an Excel file using an in-memory buffer.\n",
        "    JSON-like fields in the results are flattened into separate columns.\n",
        "\n",
        "    Args:\n",
        "        validation_results (list): List of validation result dictionaries.\n",
        "\n",
        "    Returns:\n",
        "        BytesIO: In-memory file buffer containing the Excel data.\n",
        "    \"\"\"\n",
        "    # Flatten JSON-like fields if necessary\n",
        "    flattened_records = []\n",
        "\n",
        "    for record in validation_results:\n",
        "        primary_key = record.get(\"primary_key\", \"\")\n",
        "        mismatched_columns = record.get(\"mismatched_columns\", {})\n",
        "\n",
        "        if mismatched_columns:\n",
        "            for col, details in mismatched_columns.items():\n",
        "                flattened_records.append({\n",
        "                    \"PrimaryKey\": primary_key,\n",
        "                    \"Column\": col,\n",
        "                    \"SourceValue\": details.get(\"Source\", \"\"),\n",
        "                    \"TargetValue\": details.get(\"Target\", \"\"),\n",
        "                    \"Reason\": details.get(\"Reason\", \"\")\n",
        "                })\n",
        "        else:\n",
        "            # If there are no mismatched columns, keep the original record structure\n",
        "            flattened_records.append(record)\n",
        "\n",
        "    # Convert to DataFrame\n",
        "    validation_df = pd.DataFrame(flattened_records)\n",
        "\n",
        "    # Write to an in-memory buffer\n",
        "    file_buffer = io.BytesIO()\n",
        "    validation_df.to_excel(file_buffer, index=False, engine='openpyxl')\n",
        "\n",
        "    file_buffer.seek(0)  # Reset the buffer's position to the beginning\n",
        "    print(\"Validation results written to buffer\")\n",
        "    return file_buffer\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    spark = SparkSession.builder \\\n",
        "        .appName(\"DataValidationWithMappings\") \\\n",
        "        .master(\"local[*]\") \\\n",
        "        .config(\"spark.sql.caseSensitive\", \"true\") \\\n",
        "        .getOrCreate()\n",
        "\n",
        "\n",
        "    source_path = \"/content/source_trail_1.csv\"\n",
        "    target_path = \"/content/target_trail_1 - Copy.csv\"\n",
        "    mapping_path = \"/content/mapping_document 1 (1) (5) (2).xlsx\"\n",
        "\n",
        "\n",
        "    # Load data into Pandas for JSON detection and cleaning\n",
        "    source_pdf = pd.read_csv(source_path)\n",
        "    target_pdf = pd.read_csv(target_path)\n",
        "    mapping_df = pd.read_excel(mapping_path)\n",
        "\n",
        "    mapping_df = process_mapping_logic(mapping_df)\n",
        "\n",
        "    # Normalize JSON fields in source and target data\n",
        "    source_pdf, source_json_columns = normalize_json_fields(source_pdf)\n",
        "    target_pdf, target_json_columns = normalize_json_fields(target_pdf)\n",
        "\n",
        "    # Save cleaned JSON back as CSV\n",
        "    source_path_cleaned = \"/content/data/source_cleaned_final.csv\"\n",
        "    target_path_cleaned = \"/content/data/target_cleaned_final.csv\"\n",
        "    os.makedirs(os.path.dirname(source_path_cleaned), exist_ok=True)\n",
        "\n",
        "    source_pdf.to_csv(source_path_cleaned, index=False)\n",
        "    target_pdf.to_csv(target_path_cleaned, index=False)\n",
        "\n",
        "    # Load cleaned data into Spark DataFrames\n",
        "    source_df = spark.read.csv(source_path_cleaned, header=True)\n",
        "    target_df = spark.read.csv(target_path_cleaned, header=True)\n",
        "\n",
        "    # Clean the DataFrames\n",
        "    source_df_cleaned = clean_dataframe(source_df)\n",
        "    target_df_cleaned = clean_dataframe(target_df)\n",
        "\n",
        "\n",
        "    source_df_cleaned , applied_count, skipped_count = apply_transformations_with_debugging(source_df_cleaned, mapping_df)\n",
        "    export_transformed_df(source_df_cleaned, \"Transformed_Data_2.csv\")\n",
        "\n",
        "\n",
        "    # ============================================================\n",
        "    # STEP 1: Check if Primary Key is present in both DataFrames\n",
        "    # ============================================================\n",
        "    primary_key = get_primary_key(mapping_path)\n",
        "    source_cols = set(source_df_cleaned.columns)\n",
        "    print(source_cols)\n",
        "    target_cols = set(target_df_cleaned.columns)\n",
        "\n",
        "    if primary_key not in source_cols or primary_key not in target_cols:\n",
        "        print(\"\\n=== MetaData Validation Error ===\")\n",
        "        print(f\"Primary key '{primary_key}' not found in Source or Target.\")\n",
        "        #print(\"MetaData of Source and Target doesn't match. Skipping validations.\")\n",
        "        sys.exit()\n",
        "\n",
        "    # ============================================================\n",
        "    # STEP 2: Check if mapped columns exist in both DataFrames\n",
        "    # ============================================================\n",
        "\n",
        "    column_mappings = load_mappings(mapping_path)\n",
        "    print(\"Column Mappings:\", column_mappings)\n",
        "\n",
        "    is_valid_metadata, missing_in_source, missing_in_target = validate_metadata(\n",
        "        source_df_cleaned,\n",
        "        target_df_cleaned,\n",
        "        column_mappings\n",
        "    )\n",
        "\n",
        "    # ============================================================\n",
        "    # STEP 3: If all checks pass, proceed with validations\n",
        "    # ============================================================\n",
        "    print(\"\\n=== Proceeding with Validations ===\")\n",
        "\n",
        "    # Get mapped columns for validation\n",
        "    mapped_columns = apply_mappings(column_mappings)\n",
        "\n",
        "    # Perform Basic Validation\n",
        "    print(\"\\n=== Basic Validation ===\")\n",
        "    basic_results , metrics = perform_dynamic_validation(source_df_cleaned, target_df_cleaned, mapped_columns, primary_key)\n",
        "    print(metrics)\n",
        "    save_dynamic_script(source_cols, target_cols, mapped_columns, primary_key)\n",
        "\n",
        "# Convert the JSON results into a pandas DataFrame and print as a table\n",
        "    if isinstance(basic_results, list):\n",
        "      basic_df = pd.DataFrame(basic_results)\n",
        "      basic_df.to_excel(\"basic_valid.xlsx\",index=False)\n",
        "      print(basic_df.to_string(index=False))\n",
        "    else:\n",
        "      print(\"Basic validation results are not in the expected format (list of dictionaries).\")\n",
        "\n",
        "# Perform Full Validation\n",
        "    print(\"\\n=== Full Validation ===\")\n",
        "    mismatched_records = perform_full_validation(source_df_cleaned, target_df, primary_key)\n",
        "    print(mismatched_records)\n",
        "    result_excel = write_validation_results_to_excel(mismatched_records)\n",
        "    print(result_excel)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ks_euP5lfNMb",
        "outputId": "d3221145-8239-4f29-814f-e38236eb2d18"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Applying Transformation for Target Column: 'PatientID' ===\n",
            "Logic for 'PatientID': # Skipped: No valid transformation logic provided\n",
            "Error applying logic for column 'PatientID': invalid syntax (<string>, line 1)\n",
            "\n",
            "=== Applying Transformation for Target Column: 'SampleID' ===\n",
            "Logic for 'SampleID': # Skipped: No valid transformation logic provided\n",
            "Error applying logic for column 'SampleID': invalid syntax (<string>, line 1)\n",
            "\n",
            "=== Applying Transformation for Target Column: 'StudyID' ===\n",
            "Logic for 'StudyID': # Skipped: No valid transformation logic provided\n",
            "Error applying logic for column 'StudyID': invalid syntax (<string>, line 1)\n",
            "\n",
            "=== Applying Transformation for Target Column: 'Age' ===\n",
            "Logic for 'Age': F.lit(2025) - F.year(F.to_date(F.col(\"DateOfBirth\"), \"dd-MM-yyyy\"))\n",
            "Updated Source column 'DateOfBirth' with transformed value.\n",
            "Transformation applied successfully for column: 'Age'\n",
            "\n",
            "=== Applying Transformation for Target Column: 'BiomarkerIndex' ===\n",
            "Logic for 'BiomarkerIndex': F.round(F.col(\"Biomarker_A_Level\") / F.col(\"Biomarker_B_Level\"), 2)\n",
            "Updated Source column 'Biomarker_A_Level' with transformed value.\n",
            "Updated Source column 'Biomarker_B_Level' with transformed value.\n",
            "Transformation applied successfully for column: 'BiomarkerIndex'\n",
            "\n",
            "=== Applying Transformation for Target Column: 'Condition_Therapy' ===\n",
            "Logic for 'Condition_Therapy': F.concat_ws('_', F.col('Condition'), F.col('TherapyType')).alias('Condition_Therapy')\n",
            "Updated Source column 'Condition' with transformed value.\n",
            "Updated Source column 'TherapyType' with transformed value.\n",
            "Transformation applied successfully for column: 'Condition_Therapy'\n",
            "\n",
            "=== Applying Transformation for Target Column: 'TherapyDuration_Days' ===\n",
            "Logic for 'TherapyDuration_Days': F.datediff(F.to_date(F.col(\"TherapyEndDate\"), 'dd-MM-yyyy'), F.to_date(F.col(\"TherapyStartDate\"), 'dd-MM-yyyy')).alias(\"TherapyDuration_Days\")\n",
            "Updated Source column 'TherapyStartDate' with transformed value.\n",
            "Updated Source column 'TherapyEndDate' with transformed value.\n",
            "Transformation applied successfully for column: 'TherapyDuration_Days'\n",
            "\n",
            "=== Applying Transformation for Target Column: 'GeneMut_Flag' ===\n",
            "Logic for 'GeneMut_Flag': F.when(F.col(\"Gene_Mutation_Found\") == \"Mutation_X\", 1).otherwise(0)\n",
            "Updated Source column 'Gene_Mutation_Found' with transformed value.\n",
            "Transformation applied successfully for column: 'GeneMut_Flag'\n",
            "\n",
            "=== Applying Transformation for Target Column: 'ResponseScore' ===\n",
            "Logic for 'ResponseScore': F.when(F.col(\"ResponseToTherapy\") == \"Positive\", 1).when(F.col(\"ResponseToTherapy\") == \"Negative\", -1).when(F.col(\"ResponseToTherapy\") == \"Neutral\", 0)\n",
            "Updated Source column 'ResponseToTherapy' with transformed value.\n",
            "Transformation applied successfully for column: 'ResponseScore'\n",
            "\n",
            "=== Applying Transformation for Target Column: 'PrimaryClinicalSite' ===\n",
            "Logic for 'PrimaryClinicalSite': F.col(\"Clinical_Site\").alias(\"PrimaryClinicalSite\")\n",
            "Updated Source column 'Clinical_Site' with transformed value.\n",
            "Transformation applied successfully for column: 'PrimaryClinicalSite'\n",
            "\n",
            "=== Applying Transformation for Target Column: 'Country_Code' ===\n",
            "Logic for 'Country_Code': F.when(F.col(\"Country\") == \"USA\", \"US\").when(F.col(\"Country\") == \"Canada\", \"CA\").when(F.col(\"Country\") == \"Germany\", \"DE\").otherwise(F.col(\"Country\"))\n",
            "Updated Source column 'Country' with transformed value.\n",
            "Transformation applied successfully for column: 'Country_Code'\n",
            "\n",
            "=== Applying Transformation for Target Column: 'Therapy_Effectiveness' ===\n",
            "Logic for 'Therapy_Effectiveness': F.when((F.col(\"ResponseToTherapy\") == \"Positive\") & (F.col(\"Biomarker_A_Level\") > 3), 0.95).when(F.col(\"ResponseToTherapy\") == \"Positive\", 0.80).when(F.col(\"ResponseToTherapy\") == \"Neutral\", 0.60).otherwise(0.40).alias(\"Therapy_Effectiveness\")\n",
            "Updated Source column 'ResponseToTherapy' with transformed value.\n",
            "Transformation applied successfully for column: 'Therapy_Effectiveness'\n",
            "\n",
            "=== Applying Transformation for Target Column: 'Therapy_Type_Abbr' ===\n",
            "Logic for 'Therapy_Type_Abbr': F.when(F.col('TherapyType') == 'Immunotherapy', 'IMM').when(F.col('TherapyType') == 'Chemotherapy', 'CHE').otherwise('TAR').alias('Therapy_Type_Abbr')\n",
            "Updated Source column 'TherapyType' with transformed value.\n",
            "Transformation applied successfully for column: 'Therapy_Type_Abbr'\n",
            "\n",
            "Total Transformations Applied: 10\n",
            "Total Transformations Skipped: 3\n",
            "Transformed DataFrame exported to Transformed_Data_2.csv.\n",
            "Index(['Source Table', 'Source', 'Target Table', 'Target',\n",
            "       'Transformation Logic', 'Primary Key'],\n",
            "      dtype='object')\n",
            "Detected Primary Key: PatientID\n",
            "{'PrimaryClinicalSite', 'StudyID', 'Country_Code', 'BiomarkerIndex', 'DateOfBirth', 'Condition', 'Clinical_Site', 'Therapy_Effectiveness', 'SampleID', 'Country', 'ResponseScore', 'Gene_Mutation_Found', 'GeneMut_Flag', 'Therapy_Type_Abbr', 'TherapyType', 'TherapyEndDate', 'Biomarker_B_Level', 'TherapyStartDate', 'ResponseToTherapy', 'TherapyDuration_Days', 'Condition_Therapy', 'Biomarker_A_Level', 'Age', 'PatientID'}\n",
            "Column Mappings: {'PatientID': 'PatientID', 'SampleID': 'SampleID', 'StudyID': 'StudyID', 'DateOfBirth': 'Age', 'Biomarker_A_Level, Biomarker_B_Level': 'BiomarkerIndex', 'Condition, TherapyType': 'Condition_Therapy', 'TherapyStartDate, TherapyEndDate': 'TherapyDuration_Days', 'Gene_Mutation_Found': 'GeneMut_Flag', 'ResponseToTherapy': 'Therapy_Effectiveness', 'Clinical_Site': 'PrimaryClinicalSite', 'Country': 'Country_Code', 'TherapyType': 'Therapy_Type_Abbr'}\n",
            "\n",
            "=== Proceeding with Validations ===\n",
            "Mapped Columns for Validation: [(['PatientID'], 'PatientID'), (['SampleID'], 'SampleID'), (['StudyID'], 'StudyID'), (['DateOfBirth'], 'Age'), (['Biomarker_A_Level', 'Biomarker_B_Level'], 'BiomarkerIndex'), (['Condition', 'TherapyType'], 'Condition_Therapy'), (['TherapyStartDate', 'TherapyEndDate'], 'TherapyDuration_Days'), (['Gene_Mutation_Found'], 'GeneMut_Flag'), (['ResponseToTherapy'], 'Therapy_Effectiveness'), (['Clinical_Site'], 'PrimaryClinicalSite'), (['Country'], 'Country_Code'), (['TherapyType'], 'Therapy_Type_Abbr')]\n",
            "\n",
            "=== Basic Validation ===\n",
            "[{'ValidationType': 'Number of Rows', 'Source': 76, 'Target': 75}, {'ValidationType': 'Number of Duplicate Rows', 'Source': 1, 'Target': 1}, {'ValidationType': 'Transformation Applied', 'Direct Move': 3, 'Transformed Data Points': 10}]\n",
            "Validation script has been saved to: generated_dynamic_validation.py\n",
            "                                                      ValidationType                       Source       Target\n",
            "                                                        Record Count                           76           75\n",
            "                                  Data Type (PatientID -> PatientID)                 StringType() StringType()\n",
            "                                Null Values (PatientID -> PatientID)                            0            0\n",
            "                                    Data Type (SampleID -> SampleID)                 StringType() StringType()\n",
            "                                  Null Values (SampleID -> SampleID)                            0            0\n",
            "                                      Data Type (StudyID -> StudyID)                 StringType() StringType()\n",
            "                                    Null Values (StudyID -> StudyID)                            0            0\n",
            "                                      Data Type (DateOfBirth -> Age)                IntegerType() StringType()\n",
            "                                    Null Values (DateOfBirth -> Age)                            0            0\n",
            "  Data Type (Biomarker_A_Level, Biomarker_B_Level -> BiomarkerIndex)   DoubleType(), DoubleType() StringType()\n",
            "                   Null Values (Biomarker_A_Level -> BiomarkerIndex)                            1            0\n",
            "                   Null Values (Biomarker_B_Level -> BiomarkerIndex)                            1            0\n",
            "             Data Type (Condition, TherapyType -> Condition_Therapy)   StringType(), StringType() StringType()\n",
            "                        Null Values (Condition -> Condition_Therapy)                            0            0\n",
            "                      Null Values (TherapyType -> Condition_Therapy)                            0            0\n",
            "Data Type (TherapyStartDate, TherapyEndDate -> TherapyDuration_Days) IntegerType(), IntegerType() StringType()\n",
            "              Null Values (TherapyStartDate -> TherapyDuration_Days)                            0            0\n",
            "                Null Values (TherapyEndDate -> TherapyDuration_Days)                            0            0\n",
            "                     Data Type (Gene_Mutation_Found -> GeneMut_Flag)                IntegerType() StringType()\n",
            "                   Null Values (Gene_Mutation_Found -> GeneMut_Flag)                            0            1\n",
            "              Data Type (ResponseToTherapy -> Therapy_Effectiveness)                 DoubleType() StringType()\n",
            "            Null Values (ResponseToTherapy -> Therapy_Effectiveness)                            0            0\n",
            "                    Data Type (Clinical_Site -> PrimaryClinicalSite)                 StringType() StringType()\n",
            "                  Null Values (Clinical_Site -> PrimaryClinicalSite)                            0            0\n",
            "                                 Data Type (Country -> Country_Code)                 StringType() StringType()\n",
            "                               Null Values (Country -> Country_Code)                            0            0\n",
            "                        Data Type (TherapyType -> Therapy_Type_Abbr)                 StringType() StringType()\n",
            "                      Null Values (TherapyType -> Therapy_Type_Abbr)                            0            0\n",
            "                                              Primary Key Uniqueness                           75           74\n",
            "                                                         ColumnCount                           24           14\n",
            "\n",
            "=== Full Validation ===\n",
            "[{'PrimaryKey': 'PAT_001', 'Column': 'TherapyDuration_Days', 'SourceValue': '334', 'TargetValue': '365', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_001', 'Column': 'GeneMut_Flag', 'SourceValue': '1', 'TargetValue': '1.0', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_001', 'Column': 'Therapy_Effectiveness', 'SourceValue': '0.4', 'TargetValue': '0.95', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_001', 'Column': 'Therapy_Type_Abbr', 'SourceValue': 'TAR', 'TargetValue': 'IMM', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_002', 'Column': 'TherapyDuration_Days', 'SourceValue': '366', 'TargetValue': '365', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_002', 'Column': 'GeneMut_Flag', 'SourceValue': '0', 'TargetValue': '0.0', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_002', 'Column': 'Therapy_Effectiveness', 'SourceValue': '0.4', 'TargetValue': '0.8', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_002', 'Column': 'Therapy_Type_Abbr', 'SourceValue': 'TAR', 'TargetValue': 'CHE', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_003', 'Column': 'TherapyDuration_Days', 'SourceValue': '366', 'TargetValue': '365', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_003', 'Column': 'GeneMut_Flag', 'SourceValue': '0', 'TargetValue': '0.0', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_003', 'Column': 'Therapy_Effectiveness', 'SourceValue': '0.4', 'TargetValue': '0.6', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_004', 'Column': 'TherapyDuration_Days', 'SourceValue': '334', 'TargetValue': '365', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_004', 'Column': 'GeneMut_Flag', 'SourceValue': '1', 'TargetValue': '1.0', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_004', 'Column': 'Therapy_Effectiveness', 'SourceValue': '0.4', 'TargetValue': '0.95', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_004', 'Column': 'Therapy_Type_Abbr', 'SourceValue': 'TAR', 'TargetValue': 'IMM', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_005', 'Column': 'TherapyDuration_Days', 'SourceValue': '366', 'TargetValue': '365', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_005', 'Column': 'GeneMut_Flag', 'SourceValue': '0', 'TargetValue': '0.0', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_005', 'Column': 'Therapy_Effectiveness', 'SourceValue': '0.4', 'TargetValue': '0.8', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_005', 'Column': 'Therapy_Type_Abbr', 'SourceValue': 'TAR', 'TargetValue': 'CHE', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_006', 'Column': 'TherapyDuration_Days', 'SourceValue': '366', 'TargetValue': '365', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_006', 'Column': 'GeneMut_Flag', 'SourceValue': '0', 'TargetValue': '0.0', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_006', 'Column': 'Therapy_Effectiveness', 'SourceValue': '0.4', 'TargetValue': '0.6', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_007', 'Column': 'TherapyDuration_Days', 'SourceValue': '334', 'TargetValue': '365', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_007', 'Column': 'GeneMut_Flag', 'SourceValue': '1', 'TargetValue': '1.0', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_007', 'Column': 'Therapy_Effectiveness', 'SourceValue': '0.4', 'TargetValue': '0.95', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_007', 'Column': 'Therapy_Type_Abbr', 'SourceValue': 'TAR', 'TargetValue': 'IMM', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_008', 'Column': 'TherapyDuration_Days', 'SourceValue': '366', 'TargetValue': '365', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_008', 'Column': 'GeneMut_Flag', 'SourceValue': '0', 'TargetValue': '0.0', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_008', 'Column': 'Therapy_Effectiveness', 'SourceValue': '0.4', 'TargetValue': '0.8', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_008', 'Column': 'Therapy_Type_Abbr', 'SourceValue': 'TAR', 'TargetValue': 'CHE', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_009', 'Column': 'TherapyDuration_Days', 'SourceValue': '366', 'TargetValue': '365', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_009', 'Column': 'GeneMut_Flag', 'SourceValue': '0', 'TargetValue': '0.0', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_009', 'Column': 'Therapy_Effectiveness', 'SourceValue': '0.4', 'TargetValue': '0.6', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_010', 'Column': 'TherapyDuration_Days', 'SourceValue': '334', 'TargetValue': '365', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_010', 'Column': 'GeneMut_Flag', 'SourceValue': '1', 'TargetValue': '1.0', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_010', 'Column': 'Therapy_Effectiveness', 'SourceValue': '0.4', 'TargetValue': '0.95', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_010', 'Column': 'Therapy_Type_Abbr', 'SourceValue': 'TAR', 'TargetValue': 'IMM', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_011', 'Column': 'TherapyDuration_Days', 'SourceValue': '366', 'TargetValue': '365', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_011', 'Column': 'GeneMut_Flag', 'SourceValue': '0', 'TargetValue': '0.0', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_011', 'Column': 'Therapy_Effectiveness', 'SourceValue': '0.4', 'TargetValue': '0.8', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_011', 'Column': 'Therapy_Type_Abbr', 'SourceValue': 'TAR', 'TargetValue': 'CHE', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_012', 'Column': 'TherapyDuration_Days', 'SourceValue': '366', 'TargetValue': '365', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_012', 'Column': 'GeneMut_Flag', 'SourceValue': '0', 'TargetValue': '0.0', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_012', 'Column': 'Therapy_Effectiveness', 'SourceValue': '0.4', 'TargetValue': '0.6', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_013', 'Column': 'TherapyDuration_Days', 'SourceValue': '334', 'TargetValue': '365', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_013', 'Column': 'GeneMut_Flag', 'SourceValue': '1', 'TargetValue': '1.0', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_013', 'Column': 'Therapy_Effectiveness', 'SourceValue': '0.4', 'TargetValue': '0.95', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_013', 'Column': 'Therapy_Type_Abbr', 'SourceValue': 'TAR', 'TargetValue': 'IMM', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_014', 'Column': 'TherapyDuration_Days', 'SourceValue': '366', 'TargetValue': '365', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_014', 'Column': 'GeneMut_Flag', 'SourceValue': '0', 'TargetValue': '0.0', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_014', 'Column': 'Therapy_Effectiveness', 'SourceValue': '0.4', 'TargetValue': '0.8', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_014', 'Column': 'Therapy_Type_Abbr', 'SourceValue': 'TAR', 'TargetValue': 'CHE', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_015', 'Column': 'TherapyDuration_Days', 'SourceValue': '366', 'TargetValue': '365', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_015', 'Column': 'GeneMut_Flag', 'SourceValue': '0', 'TargetValue': '0.0', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_015', 'Column': 'Therapy_Effectiveness', 'SourceValue': '0.4', 'TargetValue': '0.6', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_016', 'Column': 'TherapyDuration_Days', 'SourceValue': '334', 'TargetValue': '365', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_016', 'Column': 'GeneMut_Flag', 'SourceValue': '1', 'TargetValue': '1.0', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_016', 'Column': 'Therapy_Effectiveness', 'SourceValue': '0.4', 'TargetValue': '0.95', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_016', 'Column': 'Therapy_Type_Abbr', 'SourceValue': 'TAR', 'TargetValue': 'IMM', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_017', 'Column': 'TherapyDuration_Days', 'SourceValue': '366', 'TargetValue': '365', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_017', 'Column': 'GeneMut_Flag', 'SourceValue': '0', 'TargetValue': '0.0', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_017', 'Column': 'Therapy_Effectiveness', 'SourceValue': '0.4', 'TargetValue': '0.8', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_017', 'Column': 'Therapy_Type_Abbr', 'SourceValue': 'TAR', 'TargetValue': 'CHE', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_018', 'Column': 'TherapyDuration_Days', 'SourceValue': '366', 'TargetValue': '365', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_018', 'Column': 'GeneMut_Flag', 'SourceValue': '0', 'TargetValue': '0.0', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_018', 'Column': 'Therapy_Effectiveness', 'SourceValue': '0.4', 'TargetValue': '0.6', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_019', 'Column': 'TherapyDuration_Days', 'SourceValue': '334', 'TargetValue': '365', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_019', 'Column': 'GeneMut_Flag', 'SourceValue': '1', 'TargetValue': '1.0', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_019', 'Column': 'Therapy_Effectiveness', 'SourceValue': '0.4', 'TargetValue': '0.95', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_019', 'Column': 'Therapy_Type_Abbr', 'SourceValue': 'TAR', 'TargetValue': 'IMM', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_020', 'Column': 'TherapyDuration_Days', 'SourceValue': '366', 'TargetValue': '365', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_020', 'Column': 'GeneMut_Flag', 'SourceValue': '0', 'TargetValue': '0.0', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_020', 'Column': 'Therapy_Effectiveness', 'SourceValue': '0.4', 'TargetValue': '0.8', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_020', 'Column': 'Therapy_Type_Abbr', 'SourceValue': 'TAR', 'TargetValue': 'CHE', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_021', 'Column': 'TherapyDuration_Days', 'SourceValue': '366', 'TargetValue': '365', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_021', 'Column': 'GeneMut_Flag', 'SourceValue': '0', 'TargetValue': '0.0', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_021', 'Column': 'Therapy_Effectiveness', 'SourceValue': '0.4', 'TargetValue': '0.6', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_022', 'Column': 'TherapyDuration_Days', 'SourceValue': '334', 'TargetValue': '365', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_022', 'Column': 'GeneMut_Flag', 'SourceValue': '1', 'TargetValue': '1.0', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_022', 'Column': 'Therapy_Effectiveness', 'SourceValue': '0.4', 'TargetValue': '0.95', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_022', 'Column': 'Therapy_Type_Abbr', 'SourceValue': 'TAR', 'TargetValue': 'IMM', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_023', 'Column': 'TherapyDuration_Days', 'SourceValue': '366', 'TargetValue': '365', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_023', 'Column': 'GeneMut_Flag', 'SourceValue': '0', 'TargetValue': '0.0', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_023', 'Column': 'Therapy_Effectiveness', 'SourceValue': '0.4', 'TargetValue': '0.8', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_023', 'Column': 'Therapy_Type_Abbr', 'SourceValue': 'TAR', 'TargetValue': 'CHE', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_024', 'Column': 'TherapyDuration_Days', 'SourceValue': '366', 'TargetValue': '365', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_024', 'Column': 'GeneMut_Flag', 'SourceValue': '0', 'TargetValue': '0.0', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_024', 'Column': 'Therapy_Effectiveness', 'SourceValue': '0.4', 'TargetValue': '0.6', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_025', 'Column': 'TherapyDuration_Days', 'SourceValue': '334', 'TargetValue': '365', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_025', 'Column': 'GeneMut_Flag', 'SourceValue': '1', 'TargetValue': '1.0', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_025', 'Column': 'Therapy_Effectiveness', 'SourceValue': '0.4', 'TargetValue': '0.95', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_025', 'Column': 'Therapy_Type_Abbr', 'SourceValue': 'TAR', 'TargetValue': 'IMM', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_026', 'Column': 'TherapyDuration_Days', 'SourceValue': '366', 'TargetValue': '365', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_026', 'Column': 'GeneMut_Flag', 'SourceValue': '0', 'TargetValue': '0.0', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_026', 'Column': 'Therapy_Effectiveness', 'SourceValue': '0.4', 'TargetValue': '0.8', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_026', 'Column': 'Therapy_Type_Abbr', 'SourceValue': 'TAR', 'TargetValue': 'CHE', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_027', 'Column': 'TherapyDuration_Days', 'SourceValue': '366', 'TargetValue': '365', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_027', 'Column': 'GeneMut_Flag', 'SourceValue': '0', 'TargetValue': '0.0', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_027', 'Column': 'Therapy_Effectiveness', 'SourceValue': '0.4', 'TargetValue': '0.6', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_028', 'Column': 'TherapyDuration_Days', 'SourceValue': '334', 'TargetValue': '365', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_028', 'Column': 'GeneMut_Flag', 'SourceValue': '1', 'TargetValue': '1.0', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_028', 'Column': 'Therapy_Effectiveness', 'SourceValue': '0.4', 'TargetValue': '0.95', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_028', 'Column': 'Therapy_Type_Abbr', 'SourceValue': 'TAR', 'TargetValue': 'IMM', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_029', 'Column': 'TherapyDuration_Days', 'SourceValue': '366', 'TargetValue': '365', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_029', 'Column': 'GeneMut_Flag', 'SourceValue': '0', 'TargetValue': '0.0', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_029', 'Column': 'Therapy_Effectiveness', 'SourceValue': '0.4', 'TargetValue': '0.8', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_029', 'Column': 'Therapy_Type_Abbr', 'SourceValue': 'TAR', 'TargetValue': 'CHE', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_030', 'Column': 'TherapyDuration_Days', 'SourceValue': '366', 'TargetValue': '365', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_030', 'Column': 'GeneMut_Flag', 'SourceValue': '0', 'TargetValue': '0.0', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_030', 'Column': 'Therapy_Effectiveness', 'SourceValue': '0.4', 'TargetValue': '0.6', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_031', 'Column': 'TherapyDuration_Days', 'SourceValue': '334', 'TargetValue': '365', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_031', 'Column': 'GeneMut_Flag', 'SourceValue': '1', 'TargetValue': '1.0', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_031', 'Column': 'Therapy_Effectiveness', 'SourceValue': '0.4', 'TargetValue': '0.95', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_031', 'Column': 'Therapy_Type_Abbr', 'SourceValue': 'TAR', 'TargetValue': 'IMM', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_032', 'Column': 'TherapyDuration_Days', 'SourceValue': '366', 'TargetValue': '365', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_032', 'Column': 'GeneMut_Flag', 'SourceValue': '0', 'TargetValue': '0.0', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_032', 'Column': 'Therapy_Effectiveness', 'SourceValue': '0.4', 'TargetValue': '0.8', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_032', 'Column': 'Therapy_Type_Abbr', 'SourceValue': 'TAR', 'TargetValue': 'CHE', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_033', 'Column': 'TherapyDuration_Days', 'SourceValue': '366', 'TargetValue': '365', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_033', 'Column': 'GeneMut_Flag', 'SourceValue': '0', 'TargetValue': '0.0', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_033', 'Column': 'Therapy_Effectiveness', 'SourceValue': '0.4', 'TargetValue': '0.6', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_034', 'Column': 'TherapyDuration_Days', 'SourceValue': '334', 'TargetValue': '365', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_034', 'Column': 'GeneMut_Flag', 'SourceValue': '1', 'TargetValue': '1.0', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_034', 'Column': 'Therapy_Effectiveness', 'SourceValue': '0.4', 'TargetValue': '0.95', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_034', 'Column': 'Therapy_Type_Abbr', 'SourceValue': 'TAR', 'TargetValue': 'IMM', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_035', 'Column': 'TherapyDuration_Days', 'SourceValue': '366', 'TargetValue': '365', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_035', 'Column': 'GeneMut_Flag', 'SourceValue': '0', 'TargetValue': '0.0', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_035', 'Column': 'Therapy_Effectiveness', 'SourceValue': '0.4', 'TargetValue': '0.8', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_035', 'Column': 'Therapy_Type_Abbr', 'SourceValue': 'TAR', 'TargetValue': 'CHE', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_036', 'Column': 'TherapyDuration_Days', 'SourceValue': '366', 'TargetValue': '365', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_036', 'Column': 'GeneMut_Flag', 'SourceValue': '0', 'TargetValue': '0.0', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_036', 'Column': 'Therapy_Effectiveness', 'SourceValue': '0.4', 'TargetValue': '0.6', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_037', 'Column': 'TherapyDuration_Days', 'SourceValue': '334', 'TargetValue': '365', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_037', 'Column': 'GeneMut_Flag', 'SourceValue': '1', 'TargetValue': '1.0', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_037', 'Column': 'Therapy_Effectiveness', 'SourceValue': '0.4', 'TargetValue': '0.95', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_037', 'Column': 'Therapy_Type_Abbr', 'SourceValue': 'TAR', 'TargetValue': 'IMM', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_038', 'Column': 'TherapyDuration_Days', 'SourceValue': '366', 'TargetValue': '365', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_038', 'Column': 'GeneMut_Flag', 'SourceValue': '0', 'TargetValue': '0.0', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_038', 'Column': 'Therapy_Effectiveness', 'SourceValue': '0.4', 'TargetValue': '0.8', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_038', 'Column': 'Therapy_Type_Abbr', 'SourceValue': 'TAR', 'TargetValue': 'CHE', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_039', 'Column': 'TherapyDuration_Days', 'SourceValue': '366', 'TargetValue': '365', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_039', 'Column': 'GeneMut_Flag', 'SourceValue': '0', 'TargetValue': '0.0', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_039', 'Column': 'Therapy_Effectiveness', 'SourceValue': '0.4', 'TargetValue': '0.6', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_040', 'Column': 'TherapyDuration_Days', 'SourceValue': '334', 'TargetValue': '365', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_040', 'Column': 'GeneMut_Flag', 'SourceValue': '1', 'TargetValue': '1.0', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_040', 'Column': 'Therapy_Effectiveness', 'SourceValue': '0.4', 'TargetValue': '0.95', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_040', 'Column': 'Therapy_Type_Abbr', 'SourceValue': 'TAR', 'TargetValue': 'IMM', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_041', 'Column': 'TherapyDuration_Days', 'SourceValue': '366', 'TargetValue': '365', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_041', 'Column': 'GeneMut_Flag', 'SourceValue': '0', 'TargetValue': '0.0', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_041', 'Column': 'Therapy_Effectiveness', 'SourceValue': '0.4', 'TargetValue': '0.8', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_041', 'Column': 'Therapy_Type_Abbr', 'SourceValue': 'TAR', 'TargetValue': 'CHE', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_042', 'Column': 'TherapyDuration_Days', 'SourceValue': '366', 'TargetValue': '365', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_042', 'Column': 'GeneMut_Flag', 'SourceValue': '0', 'TargetValue': '0.0', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_042', 'Column': 'Therapy_Effectiveness', 'SourceValue': '0.4', 'TargetValue': '0.6', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_043', 'Column': 'TherapyDuration_Days', 'SourceValue': '334', 'TargetValue': '365', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_043', 'Column': 'GeneMut_Flag', 'SourceValue': '1', 'TargetValue': '1.0', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_043', 'Column': 'Therapy_Effectiveness', 'SourceValue': '0.4', 'TargetValue': '0.95', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_043', 'Column': 'Therapy_Type_Abbr', 'SourceValue': 'TAR', 'TargetValue': 'IMM', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_044', 'Column': 'TherapyDuration_Days', 'SourceValue': '366', 'TargetValue': '365', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_044', 'Column': 'GeneMut_Flag', 'SourceValue': '0', 'TargetValue': '0.0', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_044', 'Column': 'Therapy_Effectiveness', 'SourceValue': '0.4', 'TargetValue': '0.8', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_044', 'Column': 'Therapy_Type_Abbr', 'SourceValue': 'TAR', 'TargetValue': 'CHE', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_045', 'Column': 'TherapyDuration_Days', 'SourceValue': '366', 'TargetValue': '365', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_045', 'Column': 'GeneMut_Flag', 'SourceValue': '0', 'TargetValue': '0.0', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_045', 'Column': 'Therapy_Effectiveness', 'SourceValue': '0.4', 'TargetValue': '0.6', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_046', 'Column': 'TherapyDuration_Days', 'SourceValue': '334', 'TargetValue': '365', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_046', 'Column': 'GeneMut_Flag', 'SourceValue': '1', 'TargetValue': '1.0', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_046', 'Column': 'Therapy_Effectiveness', 'SourceValue': '0.4', 'TargetValue': '0.95', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_046', 'Column': 'Therapy_Type_Abbr', 'SourceValue': 'TAR', 'TargetValue': 'IMM', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_047', 'Column': 'TherapyDuration_Days', 'SourceValue': '366', 'TargetValue': '365', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_047', 'Column': 'GeneMut_Flag', 'SourceValue': '0', 'TargetValue': '0.0', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_047', 'Column': 'Therapy_Effectiveness', 'SourceValue': '0.4', 'TargetValue': '0.8', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_047', 'Column': 'Therapy_Type_Abbr', 'SourceValue': 'TAR', 'TargetValue': 'CHE', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_048', 'Column': 'TherapyDuration_Days', 'SourceValue': '366', 'TargetValue': '365', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_048', 'Column': 'GeneMut_Flag', 'SourceValue': '0', 'TargetValue': '0.0', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_048', 'Column': 'Therapy_Effectiveness', 'SourceValue': '0.4', 'TargetValue': '0.6', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_049', 'Column': 'TherapyDuration_Days', 'SourceValue': '334', 'TargetValue': '365', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_049', 'Column': 'GeneMut_Flag', 'SourceValue': '1', 'TargetValue': None, 'Reason': 'Null Mismatch'}, {'PrimaryKey': 'PAT_049', 'Column': 'Therapy_Effectiveness', 'SourceValue': '0.4', 'TargetValue': '0.95', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_049', 'Column': 'Therapy_Type_Abbr', 'SourceValue': 'TAR', 'TargetValue': 'IMM', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_050', 'Column': 'TherapyDuration_Days', 'SourceValue': '366', 'TargetValue': '365', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_050', 'Column': 'GeneMut_Flag', 'SourceValue': '0', 'TargetValue': '0.0', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_050', 'Column': 'Therapy_Effectiveness', 'SourceValue': '0.4', 'TargetValue': '0.8', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_050', 'Column': 'Therapy_Type_Abbr', 'SourceValue': 'TAR', 'TargetValue': 'CHE', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_051', 'Column': 'TherapyDuration_Days', 'SourceValue': '366', 'TargetValue': '365', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_051', 'Column': 'GeneMut_Flag', 'SourceValue': '0', 'TargetValue': '0.0', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_051', 'Column': 'Therapy_Effectiveness', 'SourceValue': '0.4', 'TargetValue': '0.6', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_052', 'Column': 'TherapyDuration_Days', 'SourceValue': '334', 'TargetValue': '365', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_052', 'Column': 'GeneMut_Flag', 'SourceValue': '1', 'TargetValue': '1.0', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_052', 'Column': 'Therapy_Effectiveness', 'SourceValue': '0.4', 'TargetValue': '0.95', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_052', 'Column': 'Therapy_Type_Abbr', 'SourceValue': 'TAR', 'TargetValue': 'IMM', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_053', 'Column': 'TherapyDuration_Days', 'SourceValue': '366', 'TargetValue': '365', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_053', 'Column': 'GeneMut_Flag', 'SourceValue': '0', 'TargetValue': '0.0', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_053', 'Column': 'Therapy_Effectiveness', 'SourceValue': '0.4', 'TargetValue': '0.8', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_053', 'Column': 'Therapy_Type_Abbr', 'SourceValue': 'TAR', 'TargetValue': 'CHE', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_054', 'Column': 'TherapyDuration_Days', 'SourceValue': '366', 'TargetValue': '365', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_054', 'Column': 'GeneMut_Flag', 'SourceValue': '0', 'TargetValue': '0.0', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_054', 'Column': 'Therapy_Effectiveness', 'SourceValue': '0.4', 'TargetValue': '0.6', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_055', 'Column': 'TherapyDuration_Days', 'SourceValue': '334', 'TargetValue': '365', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_055', 'Column': 'GeneMut_Flag', 'SourceValue': '1', 'TargetValue': '1.0', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_055', 'Column': 'Therapy_Effectiveness', 'SourceValue': '0.4', 'TargetValue': '0.95', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_055', 'Column': 'Therapy_Type_Abbr', 'SourceValue': 'TAR', 'TargetValue': 'IMM', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_056', 'Column': 'TherapyDuration_Days', 'SourceValue': '366', 'TargetValue': '365', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_056', 'Column': 'GeneMut_Flag', 'SourceValue': '0', 'TargetValue': '0.0', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_056', 'Column': 'Therapy_Effectiveness', 'SourceValue': '0.4', 'TargetValue': '0.8', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_056', 'Column': 'Therapy_Type_Abbr', 'SourceValue': 'TAR', 'TargetValue': 'CHE', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_057', 'Column': 'TherapyDuration_Days', 'SourceValue': '366', 'TargetValue': '365', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_057', 'Column': 'GeneMut_Flag', 'SourceValue': '0', 'TargetValue': '0.0', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_057', 'Column': 'Therapy_Effectiveness', 'SourceValue': '0.4', 'TargetValue': '0.6', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_058', 'Column': 'TherapyDuration_Days', 'SourceValue': '334', 'TargetValue': '365', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_058', 'Column': 'GeneMut_Flag', 'SourceValue': '1', 'TargetValue': '1.0', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_058', 'Column': 'Therapy_Effectiveness', 'SourceValue': '0.4', 'TargetValue': '0.95', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_058', 'Column': 'Therapy_Type_Abbr', 'SourceValue': 'TAR', 'TargetValue': 'IMM', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_059', 'Column': 'TherapyDuration_Days', 'SourceValue': '366', 'TargetValue': '365', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_059', 'Column': 'GeneMut_Flag', 'SourceValue': '0', 'TargetValue': '0.0', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_059', 'Column': 'Therapy_Effectiveness', 'SourceValue': '0.4', 'TargetValue': '0.8', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_059', 'Column': 'Therapy_Type_Abbr', 'SourceValue': 'TAR', 'TargetValue': 'CHE', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_060', 'Column': 'TherapyDuration_Days', 'SourceValue': '366', 'TargetValue': '365', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_060', 'Column': 'GeneMut_Flag', 'SourceValue': '0', 'TargetValue': '0.0', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_060', 'Column': 'Therapy_Effectiveness', 'SourceValue': '0.4', 'TargetValue': '0.6', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_061', 'Column': 'TherapyDuration_Days', 'SourceValue': '334', 'TargetValue': '365', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_061', 'Column': 'GeneMut_Flag', 'SourceValue': '1', 'TargetValue': '1.0', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_061', 'Column': 'Therapy_Effectiveness', 'SourceValue': '0.4', 'TargetValue': '0.95', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_061', 'Column': 'Therapy_Type_Abbr', 'SourceValue': 'TAR', 'TargetValue': 'IMM', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_062', 'Column': 'TherapyDuration_Days', 'SourceValue': '366', 'TargetValue': '365', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_062', 'Column': 'GeneMut_Flag', 'SourceValue': '0', 'TargetValue': '0.0', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_062', 'Column': 'Therapy_Effectiveness', 'SourceValue': '0.4', 'TargetValue': '0.8', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_062', 'Column': 'Therapy_Type_Abbr', 'SourceValue': 'TAR', 'TargetValue': 'CHE', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_063', 'Column': 'TherapyDuration_Days', 'SourceValue': '366', 'TargetValue': '365', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_063', 'Column': 'GeneMut_Flag', 'SourceValue': '0', 'TargetValue': '0.0', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_063', 'Column': 'Therapy_Effectiveness', 'SourceValue': '0.4', 'TargetValue': '0.6', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_064', 'Column': 'TherapyDuration_Days', 'SourceValue': '334', 'TargetValue': '365', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_064', 'Column': 'GeneMut_Flag', 'SourceValue': '1', 'TargetValue': '1.0', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_064', 'Column': 'Therapy_Effectiveness', 'SourceValue': '0.4', 'TargetValue': '0.95', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_064', 'Column': 'Therapy_Type_Abbr', 'SourceValue': 'TAR', 'TargetValue': 'IMM', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_065', 'Column': 'TherapyDuration_Days', 'SourceValue': '366', 'TargetValue': '365', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_065', 'Column': 'GeneMut_Flag', 'SourceValue': '0', 'TargetValue': '0.0', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_065', 'Column': 'Therapy_Effectiveness', 'SourceValue': '0.4', 'TargetValue': '0.8', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_065', 'Column': 'Therapy_Type_Abbr', 'SourceValue': 'TAR', 'TargetValue': 'CHE', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_066', 'Column': 'TherapyDuration_Days', 'SourceValue': '366', 'TargetValue': '365', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_066', 'Column': 'GeneMut_Flag', 'SourceValue': '0', 'TargetValue': '0.0', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_066', 'Column': 'Therapy_Effectiveness', 'SourceValue': '0.4', 'TargetValue': '0.6', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_067', 'Column': 'TherapyDuration_Days', 'SourceValue': '334', 'TargetValue': '365', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_067', 'Column': 'GeneMut_Flag', 'SourceValue': '1', 'TargetValue': '1.0', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_067', 'Column': 'Therapy_Effectiveness', 'SourceValue': '0.4', 'TargetValue': '0.95', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_067', 'Column': 'Therapy_Type_Abbr', 'SourceValue': 'TAR', 'TargetValue': 'IMM', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_068', 'Column': 'TherapyDuration_Days', 'SourceValue': '366', 'TargetValue': '365', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_068', 'Column': 'GeneMut_Flag', 'SourceValue': '0', 'TargetValue': '0.0', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_068', 'Column': 'Therapy_Effectiveness', 'SourceValue': '0.4', 'TargetValue': '0.8', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_068', 'Column': 'Therapy_Type_Abbr', 'SourceValue': 'TAR', 'TargetValue': 'CHE', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_069', 'Column': 'TherapyDuration_Days', 'SourceValue': '366', 'TargetValue': '365', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_069', 'Column': 'GeneMut_Flag', 'SourceValue': '0', 'TargetValue': '0.0', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_069', 'Column': 'Therapy_Effectiveness', 'SourceValue': '0.4', 'TargetValue': '0.6', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_070', 'Column': 'TherapyDuration_Days', 'SourceValue': '334', 'TargetValue': '365', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_070', 'Column': 'GeneMut_Flag', 'SourceValue': '1', 'TargetValue': '1.0', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_070', 'Column': 'Therapy_Effectiveness', 'SourceValue': '0.4', 'TargetValue': '0.95', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_070', 'Column': 'Therapy_Type_Abbr', 'SourceValue': 'TAR', 'TargetValue': 'IMM', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_071', 'Column': 'BiomarkerIndex', 'SourceValue': None, 'TargetValue': '1.77', 'Reason': 'Null Mismatch'}, {'PrimaryKey': 'PAT_071', 'Column': 'BiomarkerIndex', 'SourceValue': None, 'TargetValue': '1.77', 'Reason': 'Null Mismatch'}, {'PrimaryKey': 'PAT_071', 'Column': 'TherapyDuration_Days', 'SourceValue': '366', 'TargetValue': '365', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_071', 'Column': 'TherapyDuration_Days', 'SourceValue': '366', 'TargetValue': '365', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_071', 'Column': 'GeneMut_Flag', 'SourceValue': '0', 'TargetValue': '0.0', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_071', 'Column': 'GeneMut_Flag', 'SourceValue': '0', 'TargetValue': '0.0', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_071', 'Column': 'Therapy_Effectiveness', 'SourceValue': '0.4', 'TargetValue': '0.8', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_071', 'Column': 'Therapy_Effectiveness', 'SourceValue': '0.4', 'TargetValue': '0.8', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_071', 'Column': 'Therapy_Type_Abbr', 'SourceValue': 'TAR', 'TargetValue': 'CHE', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_071', 'Column': 'Therapy_Type_Abbr', 'SourceValue': 'TAR', 'TargetValue': 'CHE', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_071', 'Column': 'All Columns', 'SourceValue': None, 'TargetValue': None, 'Reason': 'Duplicate Key in Target'}, {'PrimaryKey': 'PAT_072', 'Column': 'All Columns', 'SourceValue': {'PatientID': 'PAT_072', 'SampleID': 'SMP_072', 'StudyID': 'STUDY_3', 'Age': '60', 'BiomarkerIndex': '1.77', 'Condition_Therapy': 'Condition_3_Targeted Therapy', 'TherapyDuration_Days': '366', 'GeneMut_Flag': '0', 'ResponseScore': '0', 'PrimaryClinicalSite': 'Site_C', 'Country_Code': 'DE', 'Therapy_Effectiveness': '0.4', 'Therapy_Type_Abbr': 'TAR'}, 'TargetValue': None, 'Reason': 'Missing Key in Target'}, {'PrimaryKey': 'PAT_073', 'Column': 'TherapyDuration_Days', 'SourceValue': '334', 'TargetValue': '365', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_073', 'Column': 'GeneMut_Flag', 'SourceValue': '1', 'TargetValue': '1.0', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_073', 'Column': 'Therapy_Effectiveness', 'SourceValue': '0.4', 'TargetValue': '0.95', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_073', 'Column': 'Therapy_Type_Abbr', 'SourceValue': 'TAR', 'TargetValue': 'IMM', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_074', 'Column': 'TherapyDuration_Days', 'SourceValue': '366', 'TargetValue': '365', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_074', 'Column': 'TherapyDuration_Days', 'SourceValue': '366', 'TargetValue': '365', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_074', 'Column': 'GeneMut_Flag', 'SourceValue': '0', 'TargetValue': '0.0', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_074', 'Column': 'GeneMut_Flag', 'SourceValue': '0', 'TargetValue': '0.0', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_074', 'Column': 'Therapy_Effectiveness', 'SourceValue': '0.4', 'TargetValue': '0.8', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_074', 'Column': 'Therapy_Effectiveness', 'SourceValue': '0.4', 'TargetValue': '0.8', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_074', 'Column': 'Therapy_Type_Abbr', 'SourceValue': 'TAR', 'TargetValue': 'CHE', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_074', 'Column': 'Therapy_Type_Abbr', 'SourceValue': 'TAR', 'TargetValue': 'CHE', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_074', 'Column': 'All Columns', 'SourceValue': None, 'TargetValue': None, 'Reason': 'Duplicate Key in Source'}, {'PrimaryKey': 'PAT_075', 'Column': 'TherapyDuration_Days', 'SourceValue': '366', 'TargetValue': '365', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_075', 'Column': 'GeneMut_Flag', 'SourceValue': '0', 'TargetValue': '0.0', 'Reason': 'Value Mismatch'}, {'PrimaryKey': 'PAT_075', 'Column': 'Therapy_Effectiveness', 'SourceValue': '0.4', 'TargetValue': '0.6', 'Reason': 'Value Mismatch'}]\n",
            "Validation results written to buffer\n",
            "<_io.BytesIO object at 0x7890701fede0>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6Tj_3mJQ-5zg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}