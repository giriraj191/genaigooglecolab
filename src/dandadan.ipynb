{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UyX900e7Xv-q"
      },
      "source": [
        "# NEWS AGGREGATOR"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installations"
      ],
      "metadata": {
        "id": "XzaSVOIkX3i2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install --upgrade --quiet  langchain langchain-community langchain-openai langchain-experimental neo4j wikipedia tiktoken yfiles_jupyter_graphs"
      ],
      "metadata": {
        "id": "YHNfF-grX5I1"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade --quiet newsapi-python requests"
      ],
      "metadata": {
        "id": "Hu36kqX4ZrBO"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imports"
      ],
      "metadata": {
        "id": "UBOXVjk2X-bw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "from typing import Tuple, List, Optional\n",
        "from yfiles_jupyter_graphs import GraphWidget\n",
        "from neo4j import GraphDatabase\n",
        "from langchain_community.vectorstores import Neo4jVector\n",
        "from langchain_community.graphs import Neo4jGraph"
      ],
      "metadata": {
        "id": "lOmHCbUZasBH"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import AIMessage, HumanMessage\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import ConfigurableField\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.prompts.prompt import PromptTemplate"
      ],
      "metadata": {
        "id": "EHv6KYbtaT_G"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import (\n",
        "    RunnableBranch,\n",
        "    RunnableLambda,\n",
        "    RunnableParallel,\n",
        "    RunnablePassthrough,\n",
        ")"
      ],
      "metadata": {
        "id": "mrZIlyKnatnQ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  import google.colab\n",
        "  from google.colab import output\n",
        "  output.enable_custom_widget_manager()\n",
        "except:\n",
        "  pass"
      ],
      "metadata": {
        "id": "HC4nPR7VbP4F"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "API Keys"
      ],
      "metadata": {
        "id": "J0E7sl2hZF1x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "NEWSAPI_KEY = userdata.get('NEWSAPI_KEY')"
      ],
      "metadata": {
        "id": "nk_1feh-bQ09"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NEWSAPI_KEY"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "GyhmVPgEAGdq",
        "outputId": "a570616c-7b5f-4ecc-84f3-4cba7585354d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'7fc3b87a369f46e5922f0df9182dbfb7'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Credentials"
      ],
      "metadata": {
        "id": "AGj6mcUhZHtT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NEO4J_URI=\"neo4j+s://87dc4a97.databases.neo4j.io\"\n",
        "NEO4J_USERNAME=\"neo4j\"\n",
        "NEO4J_PASSWORD=\"2_kgZguSlSe8VXM5fmFa4eRaPMAdykeBoKPdz_D6SGc\""
      ],
      "metadata": {
        "id": "sxTamgnMblcc"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Updating environment"
      ],
      "metadata": {
        "id": "8BKvf_SOZMvu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
        "os.environ[\"NEO4J_URI\"] = NEO4J_URI\n",
        "os.environ[\"NEO4J_USERNAME\"] = NEO4J_USERNAME\n",
        "os.environ[\"NEO4J_PASSWORD\"] = NEO4J_PASSWORD\n",
        "os.environ[\"NEWSAPI_KEY\"] = NEWSAPI_KEY"
      ],
      "metadata": {
        "id": "KL9075jwc6ss"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Graphs   \n",
        "\n",
        "\n",
        "Future Error:\n",
        "<ipython-input-13-6e7385464d00>:1: LangChainDeprecationWarning: The class `Neo4jGraph` was deprecated in LangChain 0.3.8 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-neo4j package and should be used instead. To use it run `pip install -U :class:`~langchain-neo4j` and import as `from :class:`~langchain_neo4j import Neo4jGraph``.\n",
        "  graph = Neo4jGraph()"
      ],
      "metadata": {
        "id": "vpzGf8QNZXxd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "graph = Neo4jGraph()"
      ],
      "metadata": {
        "id": "y0vIyNkpdBEH"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4juw8oe3enDE"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import requests\n",
        "# import json\n",
        "\n",
        "# def fetch_articles(api_key, query, filename):\n",
        "#     url = f\"https://newsapi.org/v2/everything?q={query}&from=2025-01-02&sortBy=popularity&apiKey={api_key}\"\n",
        "#     response = requests.get(url)\n",
        "#     articles = response.json().get('articles', [])\n",
        "\n",
        "#     # Store articles in a text file\n",
        "#     with open(filename, 'w') as file:\n",
        "#         for article in articles:\n",
        "#             title = article.get('title')\n",
        "#             description = article.get('description')\n",
        "#             url = article.get('url')\n",
        "#             file.write(f\"Title: {title}\\nDescription: {description}\\nURL: {url}\\n\\n\")\n",
        "\n",
        "# # Example usage\n",
        "# api_key = 'YOUR_API_KEY'\n",
        "# fetch_articles(api_key, 'Apple', 'apple_articles.txt')"
      ],
      "metadata": {
        "id": "qNCb_qjEeza2"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import time\n",
        "import os\n",
        "from typing import List, Dict\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "def count_words(text: str) -> int:\n",
        "    \"\"\"Count words in a text string.\"\"\"\n",
        "    if not text:\n",
        "        return 0\n",
        "    return len(text.split())\n",
        "\n",
        "def create_safe_filename(topic: str, title: str) -> str:\n",
        "    \"\"\"Create a safe filename from topic and title.\"\"\"\n",
        "    # Remove or replace invalid filename characters\n",
        "    invalid_chars = '<>:\"/\\\\|?*'\n",
        "    safe_title = ''.join(c if c not in invalid_chars else '_' for c in title)\n",
        "    safe_title = safe_title[:100]  # Limit length\n",
        "    return f\"{topic}_{safe_title}.txt\"\n",
        "\n",
        "def fetch_multiple_topics(api_key: str, topics: List[str], database_folder: str = \"database\") -> Dict[str, int]:\n",
        "    \"\"\"\n",
        "    Fetch articles for multiple topics and save each article to a separate file.\n",
        "\n",
        "    Args:\n",
        "        api_key: NewsAPI key\n",
        "        topics: List of topics to fetch articles for\n",
        "        database_folder: Folder to store article files\n",
        "        days_from: Number of days from today to fetch articles\n",
        "\n",
        "    Returns:\n",
        "        Dictionary with topics and their saved article counts\n",
        "    \"\"\"\n",
        "    # Ensure database folder exists\n",
        "    os.makedirs(database_folder, exist_ok=True)\n",
        "    article_counts = {topic: 0 for topic in topics}\n",
        "\n",
        "    for topic in topics:\n",
        "        try:\n",
        "            url = (\n",
        "                f\"https://newsapi.org/v2/everything\"\n",
        "                f\"?q={topic}\"\n",
        "                f\"&sortBy=popularity\"\n",
        "                f\"&pageSize=100\"\n",
        "                f\"&apiKey={api_key}\"\n",
        "            )\n",
        "\n",
        "            response = requests.get(url)\n",
        "            response.raise_for_status()\n",
        "\n",
        "            articles = response.json().get('articles', [])\n",
        "\n",
        "            for article in articles:\n",
        "                title = article.get('title', 'No title')\n",
        "                content = article.get('content', '')\n",
        "                description = article.get('description', '')\n",
        "\n",
        "                # Combine content and description for word count\n",
        "                full_text = f\"{content}\\n{description}\".strip()\n",
        "                word_count = count_words(full_text)\n",
        "\n",
        "                # Skip if content is too short\n",
        "                if word_count < 10:\n",
        "                    continue\n",
        "\n",
        "                # Create filename using topic and title\n",
        "                filename = create_safe_filename(topic, title)\n",
        "                filepath = os.path.join(database_folder, filename)\n",
        "\n",
        "                # Write article to file\n",
        "                with open(filepath, 'w', encoding='utf-8') as file:\n",
        "                    # Write metadata header\n",
        "                    file.write(\"=\" * 50 + \"\\n\")\n",
        "                    file.write(f\"Topic: {topic}\\n\")\n",
        "                    file.write(f\"Title: {title}\\n\")\n",
        "                    file.write(f\"Published: {article.get('publishedAt', 'No date')}\\n\")\n",
        "                    file.write(f\"Source: {article.get('source', {}).get('name', 'Unknown')}\\n\")\n",
        "                    file.write(f\"URL: {article.get('url', 'No URL')}\\n\")\n",
        "                    file.write(f\"Word Count: {word_count}\\n\")\n",
        "                    file.write(\"=\" * 50 + \"\\n\\n\")\n",
        "\n",
        "                    # Write content\n",
        "                    file.write(full_text)\n",
        "\n",
        "                article_counts[topic] += 1\n",
        "\n",
        "            # Sleep to respect API rate limits\n",
        "            time.sleep(1)\n",
        "\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Error fetching articles for {topic}: {str(e)}\")\n",
        "            continue\n",
        "\n",
        "    return article_counts\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    API_KEY = NEWSAPI_KEY  # Get API key from environment variable\n",
        "\n",
        "    topics = [\n",
        "        \"technology\",\n",
        "        \"artificial intelligence\",\n",
        "        \"economics\",\n",
        "        \"politics\",\n",
        "        \"climate change\"\n",
        "    ]\n",
        "\n",
        "    results = fetch_multiple_topics(\n",
        "        api_key=API_KEY,\n",
        "        topics=topics,\n",
        "        database_folder=\"database\"\n",
        "    )\n",
        "\n",
        "    # Print summary\n",
        "    print(\"\\nArticles saved (400+ words only):\")\n",
        "    for topic, count in results.items():\n",
        "        print(f\"{topic}: {count} articles\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GzjkToR27Jv-",
        "outputId": "a5291733-0ae3-4cfd-bcb1-d8323ba25198"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Articles saved (400+ words only):\n",
            "technology: 85 articles\n",
            "artificial intelligence: 96 articles\n",
            "economics: 100 articles\n",
            "politics: 97 articles\n",
            "climate change: 96 articles\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import requests\n",
        "\n",
        "# def fetch_article_metadata(api_key, query):\n",
        "#     url = f\"https://newsapi.org/v2/everything?q={query}&sortBy=popularity&apiKey={api_key}\"\n",
        "#     response = requests.get(url)\n",
        "#     articles = response.json().get('articles', [])\n",
        "#     return articles\n",
        "\n",
        "# # Example usage\n",
        "# api_key = NEWSAPI_KEY\n",
        "# articles = fetch_article_metadata(api_key, 'Apple')\n",
        "# for article in articles:\n",
        "#     print(article['title'], article['url'])"
      ],
      "metadata": {
        "id": "qY9bjUcrfvMS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import requests\n",
        "# from bs4 import BeautifulSoup\n",
        "\n",
        "# def scrape_article_content(article_url):\n",
        "#     response = requests.get(article_url)\n",
        "#     soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "#     # This is a simple example; you may need to adjust selectors based on the website structure.\n",
        "#     paragraphs = soup.find_all('p')\n",
        "#     article_content = '\\n'.join([para.get_text() for para in paragraphs])\n",
        "\n",
        "#     return article_content\n",
        "\n",
        "# # Example usage\n",
        "# for article in articles:\n",
        "#     url = article['url']\n",
        "#     content = scrape_article_content(url)\n",
        "#     print(content)  # You can also save this content to a file\n"
      ],
      "metadata": {
        "id": "lhC5_WVPf1eT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def save_article_to_file(title, content):\n",
        "#     filename = f\"{title}.txt\".replace('/', '-')  # Replace slashes for valid filename\n",
        "#     with open(filename, 'w', encoding='utf-8') as file:\n",
        "#         file.write(content)\n",
        "\n",
        "# # Example usage\n",
        "# for article in articles:\n",
        "#     title = article['title']\n",
        "#     url = article['url']\n",
        "#     content = scrape_article_content(url)\n",
        "#     save_article_to_file(title, content)\n"
      ],
      "metadata": {
        "id": "VUPQEaQugDTu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Database Headaches"
      ],
      "metadata": {
        "id": "ZSC6F3gsDMJu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pinecone-client openai langchain-core tqdm"
      ],
      "metadata": {
        "id": "i9OsB8EjDRGM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pinecone\n",
        "from openai import OpenAI\n",
        "import os\n",
        "from typing import List, Dict\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "import time"
      ],
      "metadata": {
        "id": "xbit02HlEY_6"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PINECONE_API_KEY = userdata.get('PINECONE_API_KEY')"
      ],
      "metadata": {
        "id": "JJsHRDgXE4MK"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pinecone import Pinecone, ServerlessSpec\n",
        "\n"
      ],
      "metadata": {
        "id": "jvRvIs2nEx4i"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class ArticleRAG:\n",
        "#     def __init__(self, database_folder: str = \"database\", index_name: str = \"articles-embeddings\"):\n",
        "#         \"\"\"\n",
        "#         Initialize RAG system with Pinecone\n",
        "\n",
        "#         Args:\n",
        "#             database_folder: Folder containing article files\n",
        "#             index_name: Name for the Pinecone index\n",
        "#         \"\"\"\n",
        "#         # Initialize OpenAI client\n",
        "#         self.openai_client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "\n",
        "#         # Initialize Pinecone\n",
        "#         self.pc = Pinecone(PINECONE_API_KEY)\n",
        "#         self.index_name = index_name\n",
        "\n",
        "#         # Create index if it doesn't exist\n",
        "#         existing_indexes = [index.name for index in self.pc.list_indexes()]\n",
        "#         if index_name not in existing_indexes:\n",
        "#             self.pc.create_index(\n",
        "#                 name=index_name,\n",
        "#                 dimension=1536,  # dimension for text-embedding-3-small\n",
        "#                 metric=\"cosine\",\n",
        "#                 spec=ServerlessSpec(\n",
        "#                         cloud=\"aws\",\n",
        "#                         region=\"us-east-1\"\n",
        "#                      )\n",
        "#             )\n",
        "#             print(f\"Created new Pinecone index: {index_name}\")\n",
        "#             # Wait for index to be ready\n",
        "#             time.sleep(20)\n",
        "\n",
        "#         self.database_folder = database_folder\n",
        "\n",
        "#         # Initialize text splitter\n",
        "#         self.text_splitter = RecursiveCharacterTextSplitter(\n",
        "#             chunk_size=1000,\n",
        "#             chunk_overlap=200,\n",
        "#             length_function=len\n",
        "#         )\n",
        "\n",
        "#     def get_embedding(self, text: str) -> List[float]:\n",
        "#         \"\"\"Get embeddings using OpenAI API.\"\"\"\n",
        "#         response = self.openai_client.embeddings.create(\n",
        "#             input=text,\n",
        "#             model=\"text-embedding-3-small\"\n",
        "#         )\n",
        "#         return response.data[0].embedding\n",
        "\n",
        "#     def read_article_file(self, filepath: str) -> Dict:\n",
        "#         \"\"\"Read and parse article file with metadata.\"\"\"\n",
        "#         with open(filepath, 'r', encoding='utf-8') as file:\n",
        "#             content = file.read()\n",
        "\n",
        "#             # Split metadata and content\n",
        "#             parts = content.split('='*50)\n",
        "#             metadata_text = parts[1].strip()\n",
        "#             article_content = parts[2].strip()\n",
        "\n",
        "#             # Parse metadata\n",
        "#             metadata = {}\n",
        "#             for line in metadata_text.split('\\n'):\n",
        "#                 if ':' in line:\n",
        "#                     key, value = line.split(':', 1)\n",
        "#                     metadata[key.strip()] = value.strip()\n",
        "\n",
        "#             return {\n",
        "#                 'metadata': metadata,\n",
        "#                 'content': article_content\n",
        "#             }\n",
        "\n",
        "#     def process_and_embed_articles(self, batch_size: int = 100):\n",
        "#         \"\"\"Process all articles and add them to Pinecone.\"\"\"\n",
        "#         print(\"Processing and embedding articles...\")\n",
        "\n",
        "#         vectors_to_upsert = []\n",
        "\n",
        "#         for filename in tqdm(os.listdir(self.database_folder)):\n",
        "#             if not filename.endswith('.txt'):\n",
        "#                 continue\n",
        "\n",
        "#             filepath = os.path.join(self.database_folder, filename)\n",
        "#             article_data = self.read_article_file(filepath)\n",
        "\n",
        "#             # Split content into chunks\n",
        "#             chunks = self.text_splitter.split_text(article_data['content'])\n",
        "\n",
        "#             # Prepare metadata for each chunk\n",
        "#             metadata = article_data['metadata']\n",
        "\n",
        "#             # Process chunks\n",
        "#             for i, chunk in enumerate(chunks):\n",
        "#                 chunk_metadata = {\n",
        "#                     **metadata,\n",
        "#                     'chunk_index': i,\n",
        "#                     'total_chunks': len(chunks),\n",
        "#                     'content': chunk  # Store content in metadata for retrieval\n",
        "#                 }\n",
        "\n",
        "#                 # Create unique ID for each chunk\n",
        "#                 vector_id = f\"{filename}_{i}\"\n",
        "\n",
        "#                 try:\n",
        "#                     embedding = self.get_embedding(chunk)\n",
        "#                     vectors_to_upsert.append((vector_id, embedding, chunk_metadata))\n",
        "\n",
        "#                     # Batch upsert when we reach batch_size\n",
        "#                     if len(vectors_to_upsert) >= batch_size:\n",
        "#                         self._upsert_batch(vectors_to_upsert)\n",
        "#                         vectors_to_upsert = []\n",
        "\n",
        "#                 except Exception as e:\n",
        "#                     print(f\"Error processing chunk {vector_id}: {str(e)}\")\n",
        "\n",
        "#         # Upsert any remaining vectors\n",
        "#         if vectors_to_upsert:\n",
        "#             self._upsert_batch(vectors_to_upsert)\n",
        "\n",
        "#     def _upsert_batch(self, vectors):\n",
        "#         \"\"\"Upsert a batch of vectors to Pinecone.\"\"\"\n",
        "#         try:\n",
        "#             self.pc.Index(self.index_name).upsert(\n",
        "#                 vectors=[(id, embedding, metadata) for id, embedding, metadata in vectors]\n",
        "#             )\n",
        "#         except Exception as e:\n",
        "#             print(f\"Error upserting batch: {str(e)}\")\n",
        "\n",
        "#     def query(self, query_text: str, n_results: int = 5) -> List[Dict]:\n",
        "#         \"\"\"\n",
        "#         Query the vector database\n",
        "\n",
        "#         Args:\n",
        "#             query_text: Text to search for\n",
        "#             n_results: Number of results to return\n",
        "\n",
        "#         Returns:\n",
        "#             List of results with content and metadata\n",
        "#         \"\"\"\n",
        "#         query_embedding = self.get_embedding(query_text)\n",
        "\n",
        "#         results = self.pc.Index(self.index_name).query(\n",
        "#             vector=query_embedding,\n",
        "#             top_k=n_results,\n",
        "#             include_metadata=True\n",
        "#         )\n",
        "\n",
        "#         # Format results\n",
        "#         formatted_results = []\n",
        "#         for match in results.matches:\n",
        "#             formatted_results.append({\n",
        "#                 'content': match.metadata['content'],\n",
        "#                 'metadata': {k:v for k,v in match.metadata.items() if k != 'content'},\n",
        "#                 'score': match.score\n",
        "#             })\n",
        "\n",
        "#         return formatted_results\n",
        "\n",
        "#     def get_collection_stats(self) -> Dict:\n",
        "#         \"\"\"Get statistics about the collection.\"\"\"\n",
        "#         stats = self.pc.Index(self.index_name).describe_index_stats()\n",
        "#         return {\n",
        "#             'total_vectors': stats.total_vector_count,\n",
        "#             'dimension': stats.dimension\n",
        "#         }\n",
        "\n",
        "# # Example usage\n",
        "# if __name__ == \"__main__\":\n",
        "#     # Initialize RAG system\n",
        "#     rag = ArticleRAG()\n",
        "\n",
        "#     # Process and embed all articles\n",
        "#     rag.process_and_embed_articles()\n",
        "\n",
        "#     # Print collection statistics\n",
        "#     stats = rag.get_collection_stats()\n",
        "#     print(\"\\nCollection Statistics:\")\n",
        "#     print(f\"Total Vectors: {stats['total_vectors']}\")\n",
        "#     print(f\"Embedding Dimension: {stats['dimension']}\")\n",
        "\n",
        "#     # Example query\n",
        "#     query = \"What are the latest developments in artificial intelligence?\"\n",
        "#     results = rag.query(query)\n",
        "\n",
        "#     print(f\"\\nQuery: {query}\")\n",
        "#     print(\"\\nTop Results:\")\n",
        "#     for i, result in enumerate(results, 1):\n",
        "#         print(f\"\\n{i}. {result['metadata']['Title']}\")\n",
        "#         print(f\"Topic: {result['metadata']['Topic']}\")\n",
        "#         print(f\"Source: {result['metadata']['Source']}\")\n",
        "#         print(f\"Relevance Score: {result['score']:.2f}\")\n",
        "#         print(f\"Content Preview: {result['content'][:200]}...\")"
      ],
      "metadata": {
        "id": "L25eDWc-gPiL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import hashlib"
      ],
      "metadata": {
        "id": "lxKhXY6NS-8K"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pinecone import Pinecone\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import os\n",
        "from typing import List, Dict\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "import time\n",
        "import torch"
      ],
      "metadata": {
        "id": "Oi9vZYLNJSQg"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ArticleRAG:\n",
        "    def __init__(self, database_folder: str = \"database\", index_name: str = \"articles-embeddings\", force_recreate: bool = False):\n",
        "        \"\"\"\n",
        "        Initialize RAG system with Pinecone and HuggingFace model\n",
        "\n",
        "        Args:\n",
        "            database_folder: Folder containing article files\n",
        "            index_name: Name for the Pinecone index\n",
        "            force_recreate: If True, deletes existing index and creates a new one\n",
        "        \"\"\"\n",
        "        # Initialize embedding model\n",
        "        print(\"Loading embedding model...\")\n",
        "        self.model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
        "        self.embedding_dimension = self.model.get_sentence_embedding_dimension()\n",
        "\n",
        "        # Set device (GPU if available)\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        self.model = self.model.to(self.device)\n",
        "\n",
        "        # Initialize Pinecone\n",
        "        self.pc = Pinecone(PINECONE_API_KEY)\n",
        "        self.index_name = index_name\n",
        "\n",
        "        # Handle existing index\n",
        "        existing_indexes = [index.name for index in self.pc.list_indexes()]\n",
        "\n",
        "        if index_name in existing_indexes:\n",
        "            if force_recreate:\n",
        "                print(f\"Deleting existing index {index_name}...\")\n",
        "                self.pc.delete_index(index_name)\n",
        "                time.sleep(20)  # Wait for deletion\n",
        "            else:\n",
        "                # Check dimension of existing index\n",
        "                index_stats = self.pc.Index(index_name).describe_index_stats()\n",
        "                if index_stats.dimension != self.embedding_dimension:\n",
        "                    raise ValueError(\n",
        "                        f\"Existing index dimension ({index_stats.dimension}) doesn't match model dimension \"\n",
        "                        f\"({self.embedding_dimension}). Use force_recreate=True to recreate the index.\"\n",
        "                    )\n",
        "\n",
        "        # Create index if it doesn't exist\n",
        "        if index_name not in existing_indexes or force_recreate:\n",
        "            print(f\"Creating new Pinecone index: {index_name}\")\n",
        "            self.pc.create_index(\n",
        "                name=index_name,\n",
        "                dimension=self.embedding_dimension,\n",
        "                metric=\"cosine\",\n",
        "                spec=ServerlessSpec(\n",
        "                        cloud=\"aws\",\n",
        "                        region=\"us-east-1\"\n",
        "                     )\n",
        "            )\n",
        "            print(\"Waiting for index to be ready...\")\n",
        "            time.sleep(20)\n",
        "\n",
        "        self.database_folder = database_folder\n",
        "\n",
        "        # Initialize text splitter\n",
        "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=500,\n",
        "            chunk_overlap=100,\n",
        "            length_function=len\n",
        "        )\n",
        "\n",
        "    def get_embedding(self, text: str) -> List[float]:\n",
        "        \"\"\"Get embeddings using SentenceTransformer model.\"\"\"\n",
        "        with torch.no_grad():\n",
        "            embedding = self.model.encode(text, convert_to_numpy=True)\n",
        "            return embedding.tolist()\n",
        "\n",
        "    def read_article_file(self, filepath: str) -> Dict:\n",
        "        \"\"\"Read and parse article file with metadata.\"\"\"\n",
        "        with open(filepath, 'r', encoding='utf-8') as file:\n",
        "            content = file.read()\n",
        "\n",
        "            # Split metadata and content\n",
        "            parts = content.split('='*50)\n",
        "            metadata_text = parts[1].strip()\n",
        "            article_content = parts[2].strip()\n",
        "\n",
        "            # Parse metadata\n",
        "            metadata = {}\n",
        "            for line in metadata_text.split('\\n'):\n",
        "                if ':' in line:\n",
        "                    key, value = line.split(':', 1)\n",
        "                    metadata[key.strip()] = value.strip()\n",
        "\n",
        "            return {\n",
        "                'metadata': metadata,\n",
        "                'content': article_content\n",
        "            }\n",
        "\n",
        "    def generate_vector_id(self, filename: str, chunk_index: int) -> str:\n",
        "        \"\"\"\n",
        "        Generate a unique, ASCII-compliant vector ID.\n",
        "\n",
        "        Args:\n",
        "            filename: Original filename (may contain non-English characters).\n",
        "            chunk_index: Index of the chunk.\n",
        "\n",
        "        Returns:\n",
        "            A unique vector ID containing only English alphabets and numbers.\n",
        "        \"\"\"\n",
        "        # Use only alphanumeric characters in the filename\n",
        "        sanitized_filename = ''.join(c for c in filename if c.isalnum())\n",
        "\n",
        "        # Hash the sanitized filename to ensure uniqueness\n",
        "        unique_hash = hashlib.md5(sanitized_filename[:7].encode('utf-8')).hexdigest()\n",
        "\n",
        "        # Combine with chunk index to create final vector ID\n",
        "        return f\"{unique_hash}_{chunk_index}\"\n",
        "\n",
        "\n",
        "    def process_and_embed_articles(self, batch_size: int = 50):\n",
        "        \"\"\"Process all articles and add them to Pinecone.\"\"\"\n",
        "        print(\"Processing and embedding articles...\")\n",
        "\n",
        "        vectors_to_upsert = []\n",
        "\n",
        "        for filename in tqdm(os.listdir(self.database_folder)):\n",
        "            if not filename.endswith('.txt'):\n",
        "                continue\n",
        "\n",
        "            filepath = os.path.join(self.database_folder, filename)\n",
        "            article_data = self.read_article_file(filepath)\n",
        "\n",
        "            # Split content into chunks\n",
        "            chunks = self.text_splitter.split_text(article_data['content'])\n",
        "\n",
        "            # Prepare metadata for each chunk\n",
        "            metadata = article_data['metadata']\n",
        "\n",
        "            # Process chunks\n",
        "            for i, chunk in enumerate(chunks):\n",
        "                chunk_metadata = {\n",
        "                    **metadata,\n",
        "                    'chunk_index': i,\n",
        "                    'total_chunks': len(chunks),\n",
        "                    'content': chunk  # Store content in metadata for retrieval\n",
        "                }\n",
        "\n",
        "                # Generate valid ASCII vector ID\n",
        "                sanitized_filename = ''.join(c if c.isalnum() or c in ['_', '-'] else '_' for c in filename)\n",
        "                vector_id = self.generate_vector_id(filename, i)\n",
        "\n",
        "                try:\n",
        "                    embedding = self.get_embedding(chunk)\n",
        "                    vectors_to_upsert.append((vector_id, embedding, chunk_metadata))\n",
        "\n",
        "                    # Batch upsert when we reach batch_size\n",
        "                    if len(vectors_to_upsert) >= batch_size:\n",
        "                        self._upsert_batch(vectors_to_upsert)\n",
        "                        vectors_to_upsert = []\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing chunk {vector_id}: {str(e)}\")\n",
        "\n",
        "        # Upsert any remaining vectors\n",
        "        if vectors_to_upsert:\n",
        "            self._upsert_batch(vectors_to_upsert)\n",
        "\n",
        "\n",
        "    def _upsert_batch(self, vectors):\n",
        "        \"\"\"Upsert a batch of vectors to Pinecone.\"\"\"\n",
        "        try:\n",
        "            self.pc.Index(self.index_name).upsert(\n",
        "                vectors=[(id, embedding, metadata) for id, embedding, metadata in vectors]\n",
        "            )\n",
        "        except Exception as e:\n",
        "            print(f\"Error upserting batch: {str(e)}\")\n",
        "\n",
        "    def query(self, query_text: str, n_results: int = 5) -> List[Dict]:\n",
        "        \"\"\"\n",
        "        Query the vector database\n",
        "\n",
        "        Args:\n",
        "            query_text: Text to search for\n",
        "            n_results: Number of results to return\n",
        "\n",
        "        Returns:\n",
        "            List of results with content and metadata\n",
        "        \"\"\"\n",
        "        query_embedding = self.get_embedding(query_text)\n",
        "\n",
        "        results = self.pc.Index(self.index_name).query(\n",
        "            vector=query_embedding,\n",
        "            top_k=n_results,\n",
        "            include_metadata=True\n",
        "        )\n",
        "\n",
        "        # Format results\n",
        "        formatted_results = []\n",
        "        for match in results.matches:\n",
        "            formatted_results.append({\n",
        "                'content': match.metadata['content'],\n",
        "                'metadata': {k:v for k,v in match.metadata.items() if k != 'content'},\n",
        "                'score': match.score\n",
        "            })\n",
        "\n",
        "        return formatted_results\n",
        "\n",
        "    def get_collection_stats(self) -> Dict:\n",
        "        \"\"\"Get statistics about the collection.\"\"\"\n",
        "        stats = self.pc.Index(self.index_name).describe_index_stats()\n",
        "        return {\n",
        "            'total_vectors': stats.total_vector_count,\n",
        "            'dimension': stats.dimension\n",
        "        }\n"
      ],
      "metadata": {
        "id": "wOD3UZIEGOkr"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Initialize RAG system with force_recreate=True to ensure correct dimensions\n",
        "    rag = ArticleRAG(\n",
        "        index_name=\"articles-embeddings\",\n",
        "        force_recreate=True  # This will delete existing index and create new one\n",
        "    )\n",
        "\n",
        "    # Process and embed all articles\n",
        "    rag.process_and_embed_articles()\n",
        "\n",
        "    # Print collection statistics\n",
        "    stats = rag.get_collection_stats()\n",
        "    print(\"\\nCollection Statistics:\")\n",
        "    print(f\"Total Vectors: {stats['total_vectors']}\")\n",
        "    print(f\"Embedding Dimension: {stats['dimension']}\")\n",
        "\n",
        "    # Example query\n",
        "    query = \"What are the latest developments in artificial intelligence?\"\n",
        "    results = rag.query(query)\n",
        "\n",
        "    print(f\"\\nQuery: {query}\")\n",
        "    print(\"\\nTop Results:\")\n",
        "    for i, result in enumerate(results, 1):\n",
        "        print(f\"\\n{i}. {result['metadata']['Title']}\")\n",
        "        print(f\"Topic: {result['metadata']['Topic']}\")\n",
        "        print(f\"Source: {result['metadata']['Source']}\")\n",
        "        print(f\"Relevance Score: {result['score']:.2f}\")\n",
        "        print(f\"Content Preview: {result['content'][:200]}...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kZ5uJgJBK8KS",
        "outputId": "7c45f4d6-ed65-4c9b-f8d4-348a322cc88e"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading embedding model...\n",
            "Deleting existing index articles-embeddings...\n",
            "Creating new Pinecone index: articles-embeddings\n",
            "Waiting for index to be ready...\n",
            "Processing and embedding articles...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 477/477 [03:17<00:00,  2.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Collection Statistics:\n",
            "Total Vectors: 5\n",
            "Embedding Dimension: 768\n",
            "\n",
            "Query: What are the latest developments in artificial intelligence?\n",
            "\n",
            "Top Results:\n",
            "\n",
            "1. I Tried to Build a Website Using AI in 3 Hours -- and It Only Took 30 Minutes\n",
            "Topic: artificial intelligence\n",
            "Source: CNET\n",
            "Relevance Score: 0.25\n",
            "Content Preview: There are many reasons you might need to build a website in 3 hours, and some people are particularly driven to do so -- a creative person who loves the idea of experimenting with artificial intellig…...\n",
            "\n",
            "2. Exclusive: Read the 9-page memo hyperscaler startup Nscale used to raise a $155 million Series A\n",
            "Topic: economics\n",
            "Source: Business Insider\n",
            "Relevance Score: 0.16\n",
            "Content Preview: Josh Payne, founder and CEO of Nscale.Nscale\n",
            "<ul><li>Nscale has raised $155 million in Series A funding for its hyperscaler platform.</li><li>The startup offers everything from access to data center… ...\n",
            "\n",
            "3. A palm oil company, a group of US financiers, and the destruction of Peru's rainforest\n",
            "Topic: climate change\n",
            "Source: Business Insider\n",
            "Relevance Score: 0.04\n",
            "Content Preview: Two of the largest palm oil plantations in Peru are located on the west side of the Ucayali River, which flows from the Andes to the Amazon. From above, the surrounding landscape looks like stirred p…...\n",
            "\n",
            "4. How an affordability crisis has led to Republican gains in a progressive bastion\n",
            "Topic: politics\n",
            "Source: Business Insider\n",
            "Relevance Score: 0.01\n",
            "Content Preview: Vermont Republican Gov. Phil Scott, right, easily won reelection even as voters in his state overwhelmingly backed Vice President Kamala Harris for president.Suzanne Kreiter/The Boston Globe via Gett…...\n",
            "\n",
            "5. The EcoFlow Portable Power Station Is Going for Less Than Black Friday Prices for the Holiday Deal\n",
            "Topic: technology\n",
            "Source: Gizmodo.com\n",
            "Relevance Score: 0.00\n",
            "Content Preview: As extreme weather becomes more frequent and power grids show their age, having reliable backup power isn’t just about convenience anymore. You need to take matters into your own hands, and one great…...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qQVOeHMmPhS5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RoMEU__SNayp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JuBpL7XRXNZY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SEhsk1BuXNWJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fastapi nest-asyncio pyngrok uvicorn"
      ],
      "metadata": {
        "id": "a-KhPuiHXNTC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install py2neo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S-I8pNSaZKmQ",
        "outputId": "77136a67-a171-48b5-f703-23b231b83000"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting py2neo\n",
            "  Downloading py2neo-2021.2.4-py2.py3-none-any.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from py2neo) (2024.12.14)\n",
            "Collecting interchange~=2021.0.4 (from py2neo)\n",
            "  Downloading interchange-2021.0.4-py2.py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting monotonic (from py2neo)\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from py2neo) (24.2)\n",
            "Collecting pansi>=2020.7.3 (from py2neo)\n",
            "  Downloading pansi-2024.11.0-py2.py3-none-any.whl.metadata (3.1 kB)\n",
            "Requirement already satisfied: pygments>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from py2neo) (2.18.0)\n",
            "Requirement already satisfied: six>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from py2neo) (1.17.0)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from py2neo) (2.2.3)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from interchange~=2021.0.4->py2neo) (2024.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from pansi>=2020.7.3->py2neo) (11.0.0)\n",
            "Downloading py2neo-2021.2.4-py2.py3-none-any.whl (177 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/177.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.2/177.2 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading interchange-2021.0.4-py2.py3-none-any.whl (28 kB)\n",
            "Downloading pansi-2024.11.0-py2.py3-none-any.whl (26 kB)\n",
            "Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Installing collected packages: monotonic, pansi, interchange, py2neo\n",
            "Successfully installed interchange-2021.0.4 monotonic-1.6 pansi-2024.11.0 py2neo-2021.2.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Backend and Neo4j Integration in FastAPI\n",
        "from fastapi import FastAPI, HTTPException\n",
        "from py2neo import Graph, Node, Relationship\n",
        "from pydantic import BaseModel\n",
        "import uvicorn\n",
        "import threading\n",
        "# FastAPI News Endpoint\n",
        "from typing import List, Dict"
      ],
      "metadata": {
        "id": "qxAtx6VbY-69"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "app = FastAPI()\n",
        "\n",
        "# Neo4j Connection\n",
        "# graph = Graph(\"bolt://localhost:7687\", auth=(\"neo4j\", \"password\"))\n",
        "graph = Neo4jGraph()\n",
        "\n",
        "class FetchNewsRequest(BaseModel):\n",
        "    user_id: str\n",
        "    category: str\n",
        "    custom_category: str\n",
        "\n",
        "@app.post(\"/fetch_news/\")\n",
        "def fetch_news(data: FetchNewsRequest):\n",
        "\n",
        "    news = [\n",
        "        {\"title\": f\"Latest in {data.category}\", \"content\": \"News content about this category...\"},\n",
        "        {\"title\": f\"More on {data.custom_category}\", \"content\": \"Custom category-related news...\"}\n",
        "    ]\n",
        "    return {\"news\": news}\n",
        "\n",
        "# Run FastAPI server in a thread\n",
        "def run_fastapi():\n",
        "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
        "\n",
        "threading.Thread(target=run_fastapi, daemon=True).start()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LnNBMZO3XNQE",
        "outputId": "37e13daa-fd51-4272-fcef-dd7b6b8e20ed"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-293af20dc3de>:5: LangChainDeprecationWarning: The class `Neo4jGraph` was deprecated in LangChain 0.3.8 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-neo4j package and should be used instead. To use it run `pip install -U :class:`~langchain-neo4j` and import as `from :class:`~langchain_neo4j import Neo4jGraph``.\n",
            "  graph = Neo4jGraph()\n",
            "INFO:     Started server process [80372]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # FastAPI News Endpoint\n",
        "# from typing import List, Dict\n",
        "\n",
        "# class FetchNewsRequest(BaseModel):\n",
        "#     user_id: str\n",
        "#     category: str\n",
        "#     custom_category: str\n",
        "\n",
        "# @app.post(\"/fetch_news/\")\n",
        "# def fetch_news(data: FetchNewsRequest):\n",
        "#     # Example: Fetch news based on user preferences\n",
        "#     # This could be dynamically generated from a news API or database\n",
        "#     news = [\n",
        "#         {\"title\": f\"Latest in {data.category}\", \"content\": \"News content about this category...\"},\n",
        "#         {\"title\": f\"More on {data.custom_category}\", \"content\": \"Custom category-related news...\"}\n",
        "#     ]\n",
        "#     return {\"news\": news}"
      ],
      "metadata": {
        "id": "qRQrMX8ycU7u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio"
      ],
      "metadata": {
        "id": "8PXm9zrWXNNE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import requests\n",
        "\n",
        "# Backend URL\n",
        "backend_url = \"http://0.0.0.0:8000/\"\n",
        "\n",
        "# Function to send data to backend and fetch news\n",
        "def fetch_news(user_id, category, custom_category):\n",
        "    payload = {\n",
        "        \"user_id\": user_id,\n",
        "        \"category\": category,\n",
        "        \"custom_category\": custom_category,\n",
        "    }\n",
        "    # Backend endpoint to fetch news (implement this in FastAPI)\n",
        "    response = requests.post(f\"{backend_url}/fetch_news/\", json=payload)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        news = response.json().get(\"news\", [])\n",
        "        cards = \"\\n\\n\".join([f\"**{n['title']}**\\n{n['content']}\" for n in news])\n",
        "        return cards\n",
        "    else:\n",
        "        return \"Error fetching news!\"\n",
        "\n",
        "# Gradio UI\n",
        "users = [\"1\", \"2\", \"3\"]\n",
        "\n",
        "with gr.Blocks() as ui:\n",
        "    # User selection\n",
        "    user_dropdown = gr.Dropdown(users, label=\"Select User\", value=\"1\")\n",
        "    # Category selection\n",
        "    category_dropdown = gr.Dropdown([\"Tech\", \"Sports\", \"Finance\", \"Health\"], label=\"Select Category\")\n",
        "    # Custom category input\n",
        "    custom_category_input = gr.Textbox(label=\"Enter Custom Category (optional)\")\n",
        "    # Button to fetch news\n",
        "    fetch_button = gr.Button(\"Fetch News\")\n",
        "    # News display area\n",
        "    news_display = gr.Textbox(label=\"News\", interactive=False, placeholder=\"News will appear here\")\n",
        "\n",
        "    # Button click event\n",
        "    fetch_button.click(\n",
        "        fn=fetch_news,\n",
        "        inputs=[user_dropdown, category_dropdown, custom_category_input],\n",
        "        outputs=news_display\n",
        "    )\n",
        "\n",
        "ui.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 650
        },
        "id": "LB7opCTkXNKJ",
        "outputId": "34a65cbe-35b5-4b95-892a-a82afc69b94e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://50549cd626136aae2b.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://50549cd626136aae2b.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iuFEG_83XNHQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "j-rq420nXNDk"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}