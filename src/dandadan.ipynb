{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UyX900e7Xv-q"
      },
      "source": [
        "# NEWS AGGREGATOR"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installations"
      ],
      "metadata": {
        "id": "XzaSVOIkX3i2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install --upgrade --quiet  langchain langchain-community langchain-openai langchain-experimental neo4j wikipedia tiktoken yfiles_jupyter_graphs"
      ],
      "metadata": {
        "id": "YHNfF-grX5I1"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade --quiet newsapi-python requests"
      ],
      "metadata": {
        "id": "Hu36kqX4ZrBO"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imports"
      ],
      "metadata": {
        "id": "UBOXVjk2X-bw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "from typing import Tuple, List, Optional\n",
        "from yfiles_jupyter_graphs import GraphWidget\n",
        "from neo4j import GraphDatabase\n",
        "from langchain_community.vectorstores import Neo4jVector\n",
        "from langchain_community.graphs import Neo4jGraph"
      ],
      "metadata": {
        "id": "lOmHCbUZasBH"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import AIMessage, HumanMessage\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import ConfigurableField\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.prompts.prompt import PromptTemplate"
      ],
      "metadata": {
        "id": "EHv6KYbtaT_G"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import (\n",
        "    RunnableBranch,\n",
        "    RunnableLambda,\n",
        "    RunnableParallel,\n",
        "    RunnablePassthrough,\n",
        ")"
      ],
      "metadata": {
        "id": "mrZIlyKnatnQ"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  import google.colab\n",
        "  from google.colab import output\n",
        "  output.enable_custom_widget_manager()\n",
        "except:\n",
        "  pass"
      ],
      "metadata": {
        "id": "HC4nPR7VbP4F"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "API Keys"
      ],
      "metadata": {
        "id": "J0E7sl2hZF1x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "NEWSAPI_KEY = userdata.get('NEWSAPI_KEY')"
      ],
      "metadata": {
        "id": "nk_1feh-bQ09"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NEWSAPI_KEY"
      ],
      "metadata": {
        "id": "GyhmVPgEAGdq",
        "outputId": "5e0be5ff-5113-45d1-d654-425c5bd97ca2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'7fc3b87a369f46e5922f0df9182dbfb7'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Credentials"
      ],
      "metadata": {
        "id": "AGj6mcUhZHtT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NEO4J_URI=\"neo4j+s://87dc4a97.databases.neo4j.io\"\n",
        "NEO4J_USERNAME=\"neo4j\"\n",
        "NEO4J_PASSWORD=\"2_kgZguSlSe8VXM5fmFa4eRaPMAdykeBoKPdz_D6SGc\""
      ],
      "metadata": {
        "id": "sxTamgnMblcc"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Updating environment"
      ],
      "metadata": {
        "id": "8BKvf_SOZMvu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
        "os.environ[\"NEO4J_URI\"] = NEO4J_URI\n",
        "os.environ[\"NEO4J_USERNAME\"] = NEO4J_USERNAME\n",
        "os.environ[\"NEO4J_PASSWORD\"] = NEO4J_PASSWORD\n",
        "os.environ[\"NEWSAPI_KEY\"] = NEWSAPI_KEY"
      ],
      "metadata": {
        "id": "KL9075jwc6ss"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Graphs   \n",
        "\n",
        "\n",
        "Future Error:\n",
        "<ipython-input-13-6e7385464d00>:1: LangChainDeprecationWarning: The class `Neo4jGraph` was deprecated in LangChain 0.3.8 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-neo4j package and should be used instead. To use it run `pip install -U :class:`~langchain-neo4j` and import as `from :class:`~langchain_neo4j import Neo4jGraph``.\n",
        "  graph = Neo4jGraph()"
      ],
      "metadata": {
        "id": "vpzGf8QNZXxd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "graph = Neo4jGraph()"
      ],
      "metadata": {
        "id": "y0vIyNkpdBEH"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4juw8oe3enDE"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import requests\n",
        "# import json\n",
        "\n",
        "# def fetch_articles(api_key, query, filename):\n",
        "#     url = f\"https://newsapi.org/v2/everything?q={query}&from=2025-01-02&sortBy=popularity&apiKey={api_key}\"\n",
        "#     response = requests.get(url)\n",
        "#     articles = response.json().get('articles', [])\n",
        "\n",
        "#     # Store articles in a text file\n",
        "#     with open(filename, 'w') as file:\n",
        "#         for article in articles:\n",
        "#             title = article.get('title')\n",
        "#             description = article.get('description')\n",
        "#             url = article.get('url')\n",
        "#             file.write(f\"Title: {title}\\nDescription: {description}\\nURL: {url}\\n\\n\")\n",
        "\n",
        "# # Example usage\n",
        "# api_key = 'YOUR_API_KEY'\n",
        "# fetch_articles(api_key, 'Apple', 'apple_articles.txt')"
      ],
      "metadata": {
        "id": "qNCb_qjEeza2"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import time\n",
        "import os\n",
        "from typing import List, Dict\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "def count_words(text: str) -> int:\n",
        "    \"\"\"Count words in a text string.\"\"\"\n",
        "    if not text:\n",
        "        return 0\n",
        "    return len(text.split())\n",
        "\n",
        "def create_safe_filename(topic: str, title: str) -> str:\n",
        "    \"\"\"Create a safe filename from topic and title.\"\"\"\n",
        "    # Remove or replace invalid filename characters\n",
        "    invalid_chars = '<>:\"/\\\\|?*'\n",
        "    safe_title = ''.join(c if c not in invalid_chars else '_' for c in title)\n",
        "    safe_title = safe_title[:100]  # Limit length\n",
        "    return f\"{topic}_{safe_title}.txt\"\n",
        "\n",
        "def fetch_multiple_topics(api_key: str, topics: List[str], database_folder: str = \"database\") -> Dict[str, int]:\n",
        "    \"\"\"\n",
        "    Fetch articles for multiple topics and save each article to a separate file.\n",
        "\n",
        "    Args:\n",
        "        api_key: NewsAPI key\n",
        "        topics: List of topics to fetch articles for\n",
        "        database_folder: Folder to store article files\n",
        "        days_from: Number of days from today to fetch articles\n",
        "\n",
        "    Returns:\n",
        "        Dictionary with topics and their saved article counts\n",
        "    \"\"\"\n",
        "    # Ensure database folder exists\n",
        "    os.makedirs(database_folder, exist_ok=True)\n",
        "    article_counts = {topic: 0 for topic in topics}\n",
        "\n",
        "    for topic in topics:\n",
        "        try:\n",
        "            url = (\n",
        "                f\"https://newsapi.org/v2/everything\"\n",
        "                f\"?q={topic}\"\n",
        "                f\"&sortBy=popularity\"\n",
        "                f\"&pageSize=100\"\n",
        "                f\"&apiKey={api_key}\"\n",
        "            )\n",
        "\n",
        "            response = requests.get(url)\n",
        "            response.raise_for_status()\n",
        "\n",
        "            articles = response.json().get('articles', [])\n",
        "\n",
        "            for article in articles:\n",
        "                title = article.get('title', 'No title')\n",
        "                content = article.get('content', '')\n",
        "                description = article.get('description', '')\n",
        "\n",
        "                # Combine content and description for word count\n",
        "                full_text = f\"{content}\\n{description}\".strip()\n",
        "                word_count = count_words(full_text)\n",
        "\n",
        "                # Skip if content is too short\n",
        "                if word_count < 10:\n",
        "                    continue\n",
        "\n",
        "                # Create filename using topic and title\n",
        "                filename = create_safe_filename(topic, title)\n",
        "                filepath = os.path.join(database_folder, filename)\n",
        "\n",
        "                # Write article to file\n",
        "                with open(filepath, 'w', encoding='utf-8') as file:\n",
        "                    # Write metadata header\n",
        "                    file.write(\"=\" * 50 + \"\\n\")\n",
        "                    file.write(f\"Topic: {topic}\\n\")\n",
        "                    file.write(f\"Title: {title}\\n\")\n",
        "                    file.write(f\"Published: {article.get('publishedAt', 'No date')}\\n\")\n",
        "                    file.write(f\"Source: {article.get('source', {}).get('name', 'Unknown')}\\n\")\n",
        "                    file.write(f\"URL: {article.get('url', 'No URL')}\\n\")\n",
        "                    file.write(f\"Word Count: {word_count}\\n\")\n",
        "                    file.write(\"=\" * 50 + \"\\n\\n\")\n",
        "\n",
        "                    # Write content\n",
        "                    file.write(full_text)\n",
        "\n",
        "                article_counts[topic] += 1\n",
        "\n",
        "            # Sleep to respect API rate limits\n",
        "            time.sleep(1)\n",
        "\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Error fetching articles for {topic}: {str(e)}\")\n",
        "            continue\n",
        "\n",
        "    return article_counts\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    API_KEY = NEWSAPI_KEY  # Get API key from environment variable\n",
        "\n",
        "    topics = [\n",
        "        \"technology\",\n",
        "        \"artificial intelligence\",\n",
        "        \"economics\",\n",
        "        \"politics\",\n",
        "        \"climate change\"\n",
        "    ]\n",
        "\n",
        "    results = fetch_multiple_topics(\n",
        "        api_key=API_KEY,\n",
        "        topics=topics,\n",
        "        database_folder=\"database\"\n",
        "    )\n",
        "\n",
        "    # Print summary\n",
        "    print(\"\\nArticles saved (400+ words only):\")\n",
        "    for topic, count in results.items():\n",
        "        print(f\"{topic}: {count} articles\")"
      ],
      "metadata": {
        "id": "GzjkToR27Jv-",
        "outputId": "a5291733-0ae3-4cfd-bcb1-d8323ba25198",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Articles saved (400+ words only):\n",
            "technology: 85 articles\n",
            "artificial intelligence: 96 articles\n",
            "economics: 100 articles\n",
            "politics: 97 articles\n",
            "climate change: 96 articles\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import requests\n",
        "\n",
        "# def fetch_article_metadata(api_key, query):\n",
        "#     url = f\"https://newsapi.org/v2/everything?q={query}&sortBy=popularity&apiKey={api_key}\"\n",
        "#     response = requests.get(url)\n",
        "#     articles = response.json().get('articles', [])\n",
        "#     return articles\n",
        "\n",
        "# # Example usage\n",
        "# api_key = NEWSAPI_KEY\n",
        "# articles = fetch_article_metadata(api_key, 'Apple')\n",
        "# for article in articles:\n",
        "#     print(article['title'], article['url'])"
      ],
      "metadata": {
        "id": "qY9bjUcrfvMS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import requests\n",
        "# from bs4 import BeautifulSoup\n",
        "\n",
        "# def scrape_article_content(article_url):\n",
        "#     response = requests.get(article_url)\n",
        "#     soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "#     # This is a simple example; you may need to adjust selectors based on the website structure.\n",
        "#     paragraphs = soup.find_all('p')\n",
        "#     article_content = '\\n'.join([para.get_text() for para in paragraphs])\n",
        "\n",
        "#     return article_content\n",
        "\n",
        "# # Example usage\n",
        "# for article in articles:\n",
        "#     url = article['url']\n",
        "#     content = scrape_article_content(url)\n",
        "#     print(content)  # You can also save this content to a file\n"
      ],
      "metadata": {
        "id": "lhC5_WVPf1eT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def save_article_to_file(title, content):\n",
        "#     filename = f\"{title}.txt\".replace('/', '-')  # Replace slashes for valid filename\n",
        "#     with open(filename, 'w', encoding='utf-8') as file:\n",
        "#         file.write(content)\n",
        "\n",
        "# # Example usage\n",
        "# for article in articles:\n",
        "#     title = article['title']\n",
        "#     url = article['url']\n",
        "#     content = scrape_article_content(url)\n",
        "#     save_article_to_file(title, content)\n"
      ],
      "metadata": {
        "id": "VUPQEaQugDTu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Database Headaches"
      ],
      "metadata": {
        "id": "ZSC6F3gsDMJu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pinecone-client openai langchain-core tqdm"
      ],
      "metadata": {
        "id": "i9OsB8EjDRGM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pinecone\n",
        "from openai import OpenAI\n",
        "import os\n",
        "from typing import List, Dict\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "import time"
      ],
      "metadata": {
        "id": "xbit02HlEY_6"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PINECONE_API_KEY = userdata.get('PINECONE_API_KEY')"
      ],
      "metadata": {
        "id": "JJsHRDgXE4MK"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pinecone import Pinecone, ServerlessSpec\n",
        "\n"
      ],
      "metadata": {
        "id": "jvRvIs2nEx4i"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class ArticleRAG:\n",
        "#     def __init__(self, database_folder: str = \"database\", index_name: str = \"articles-embeddings\"):\n",
        "#         \"\"\"\n",
        "#         Initialize RAG system with Pinecone\n",
        "\n",
        "#         Args:\n",
        "#             database_folder: Folder containing article files\n",
        "#             index_name: Name for the Pinecone index\n",
        "#         \"\"\"\n",
        "#         # Initialize OpenAI client\n",
        "#         self.openai_client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "\n",
        "#         # Initialize Pinecone\n",
        "#         self.pc = Pinecone(PINECONE_API_KEY)\n",
        "#         self.index_name = index_name\n",
        "\n",
        "#         # Create index if it doesn't exist\n",
        "#         existing_indexes = [index.name for index in self.pc.list_indexes()]\n",
        "#         if index_name not in existing_indexes:\n",
        "#             self.pc.create_index(\n",
        "#                 name=index_name,\n",
        "#                 dimension=1536,  # dimension for text-embedding-3-small\n",
        "#                 metric=\"cosine\",\n",
        "#                 spec=ServerlessSpec(\n",
        "#                         cloud=\"aws\",\n",
        "#                         region=\"us-east-1\"\n",
        "#                      )\n",
        "#             )\n",
        "#             print(f\"Created new Pinecone index: {index_name}\")\n",
        "#             # Wait for index to be ready\n",
        "#             time.sleep(20)\n",
        "\n",
        "#         self.database_folder = database_folder\n",
        "\n",
        "#         # Initialize text splitter\n",
        "#         self.text_splitter = RecursiveCharacterTextSplitter(\n",
        "#             chunk_size=1000,\n",
        "#             chunk_overlap=200,\n",
        "#             length_function=len\n",
        "#         )\n",
        "\n",
        "#     def get_embedding(self, text: str) -> List[float]:\n",
        "#         \"\"\"Get embeddings using OpenAI API.\"\"\"\n",
        "#         response = self.openai_client.embeddings.create(\n",
        "#             input=text,\n",
        "#             model=\"text-embedding-3-small\"\n",
        "#         )\n",
        "#         return response.data[0].embedding\n",
        "\n",
        "#     def read_article_file(self, filepath: str) -> Dict:\n",
        "#         \"\"\"Read and parse article file with metadata.\"\"\"\n",
        "#         with open(filepath, 'r', encoding='utf-8') as file:\n",
        "#             content = file.read()\n",
        "\n",
        "#             # Split metadata and content\n",
        "#             parts = content.split('='*50)\n",
        "#             metadata_text = parts[1].strip()\n",
        "#             article_content = parts[2].strip()\n",
        "\n",
        "#             # Parse metadata\n",
        "#             metadata = {}\n",
        "#             for line in metadata_text.split('\\n'):\n",
        "#                 if ':' in line:\n",
        "#                     key, value = line.split(':', 1)\n",
        "#                     metadata[key.strip()] = value.strip()\n",
        "\n",
        "#             return {\n",
        "#                 'metadata': metadata,\n",
        "#                 'content': article_content\n",
        "#             }\n",
        "\n",
        "#     def process_and_embed_articles(self, batch_size: int = 100):\n",
        "#         \"\"\"Process all articles and add them to Pinecone.\"\"\"\n",
        "#         print(\"Processing and embedding articles...\")\n",
        "\n",
        "#         vectors_to_upsert = []\n",
        "\n",
        "#         for filename in tqdm(os.listdir(self.database_folder)):\n",
        "#             if not filename.endswith('.txt'):\n",
        "#                 continue\n",
        "\n",
        "#             filepath = os.path.join(self.database_folder, filename)\n",
        "#             article_data = self.read_article_file(filepath)\n",
        "\n",
        "#             # Split content into chunks\n",
        "#             chunks = self.text_splitter.split_text(article_data['content'])\n",
        "\n",
        "#             # Prepare metadata for each chunk\n",
        "#             metadata = article_data['metadata']\n",
        "\n",
        "#             # Process chunks\n",
        "#             for i, chunk in enumerate(chunks):\n",
        "#                 chunk_metadata = {\n",
        "#                     **metadata,\n",
        "#                     'chunk_index': i,\n",
        "#                     'total_chunks': len(chunks),\n",
        "#                     'content': chunk  # Store content in metadata for retrieval\n",
        "#                 }\n",
        "\n",
        "#                 # Create unique ID for each chunk\n",
        "#                 vector_id = f\"{filename}_{i}\"\n",
        "\n",
        "#                 try:\n",
        "#                     embedding = self.get_embedding(chunk)\n",
        "#                     vectors_to_upsert.append((vector_id, embedding, chunk_metadata))\n",
        "\n",
        "#                     # Batch upsert when we reach batch_size\n",
        "#                     if len(vectors_to_upsert) >= batch_size:\n",
        "#                         self._upsert_batch(vectors_to_upsert)\n",
        "#                         vectors_to_upsert = []\n",
        "\n",
        "#                 except Exception as e:\n",
        "#                     print(f\"Error processing chunk {vector_id}: {str(e)}\")\n",
        "\n",
        "#         # Upsert any remaining vectors\n",
        "#         if vectors_to_upsert:\n",
        "#             self._upsert_batch(vectors_to_upsert)\n",
        "\n",
        "#     def _upsert_batch(self, vectors):\n",
        "#         \"\"\"Upsert a batch of vectors to Pinecone.\"\"\"\n",
        "#         try:\n",
        "#             self.pc.Index(self.index_name).upsert(\n",
        "#                 vectors=[(id, embedding, metadata) for id, embedding, metadata in vectors]\n",
        "#             )\n",
        "#         except Exception as e:\n",
        "#             print(f\"Error upserting batch: {str(e)}\")\n",
        "\n",
        "#     def query(self, query_text: str, n_results: int = 5) -> List[Dict]:\n",
        "#         \"\"\"\n",
        "#         Query the vector database\n",
        "\n",
        "#         Args:\n",
        "#             query_text: Text to search for\n",
        "#             n_results: Number of results to return\n",
        "\n",
        "#         Returns:\n",
        "#             List of results with content and metadata\n",
        "#         \"\"\"\n",
        "#         query_embedding = self.get_embedding(query_text)\n",
        "\n",
        "#         results = self.pc.Index(self.index_name).query(\n",
        "#             vector=query_embedding,\n",
        "#             top_k=n_results,\n",
        "#             include_metadata=True\n",
        "#         )\n",
        "\n",
        "#         # Format results\n",
        "#         formatted_results = []\n",
        "#         for match in results.matches:\n",
        "#             formatted_results.append({\n",
        "#                 'content': match.metadata['content'],\n",
        "#                 'metadata': {k:v for k,v in match.metadata.items() if k != 'content'},\n",
        "#                 'score': match.score\n",
        "#             })\n",
        "\n",
        "#         return formatted_results\n",
        "\n",
        "#     def get_collection_stats(self) -> Dict:\n",
        "#         \"\"\"Get statistics about the collection.\"\"\"\n",
        "#         stats = self.pc.Index(self.index_name).describe_index_stats()\n",
        "#         return {\n",
        "#             'total_vectors': stats.total_vector_count,\n",
        "#             'dimension': stats.dimension\n",
        "#         }\n",
        "\n",
        "# # Example usage\n",
        "# if __name__ == \"__main__\":\n",
        "#     # Initialize RAG system\n",
        "#     rag = ArticleRAG()\n",
        "\n",
        "#     # Process and embed all articles\n",
        "#     rag.process_and_embed_articles()\n",
        "\n",
        "#     # Print collection statistics\n",
        "#     stats = rag.get_collection_stats()\n",
        "#     print(\"\\nCollection Statistics:\")\n",
        "#     print(f\"Total Vectors: {stats['total_vectors']}\")\n",
        "#     print(f\"Embedding Dimension: {stats['dimension']}\")\n",
        "\n",
        "#     # Example query\n",
        "#     query = \"What are the latest developments in artificial intelligence?\"\n",
        "#     results = rag.query(query)\n",
        "\n",
        "#     print(f\"\\nQuery: {query}\")\n",
        "#     print(\"\\nTop Results:\")\n",
        "#     for i, result in enumerate(results, 1):\n",
        "#         print(f\"\\n{i}. {result['metadata']['Title']}\")\n",
        "#         print(f\"Topic: {result['metadata']['Topic']}\")\n",
        "#         print(f\"Source: {result['metadata']['Source']}\")\n",
        "#         print(f\"Relevance Score: {result['score']:.2f}\")\n",
        "#         print(f\"Content Preview: {result['content'][:200]}...\")"
      ],
      "metadata": {
        "id": "L25eDWc-gPiL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pinecone import Pinecone\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import os\n",
        "from typing import List, Dict\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "import time\n",
        "import torch"
      ],
      "metadata": {
        "id": "Oi9vZYLNJSQg"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ArticleRAG:\n",
        "    def __init__(self, database_folder: str = \"database\", index_name: str = \"articles-embeddings\"):\n",
        "        \"\"\"\n",
        "        Initialize RAG system with Pinecone and HuggingFace model\n",
        "\n",
        "        Args:\n",
        "            database_folder: Folder containing article files\n",
        "            index_name: Name for the Pinecone index\n",
        "        \"\"\"\n",
        "        # Initialize embedding model\n",
        "        print(\"Loading embedding model...\")\n",
        "        self.model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
        "\n",
        "        # Set device (GPU if available)\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        self.model = self.model.to(self.device)\n",
        "\n",
        "        # Initialize Pinecone\n",
        "        self.pc = Pinecone(api_key=os.getenv(\"PINECONE_API_KEY\"))\n",
        "        self.index_name = index_name\n",
        "\n",
        "        # Create index if it doesn't exist\n",
        "        existing_indexes = [index.name for index in self.pc.list_indexes()]\n",
        "        if index_name not in existing_indexes:\n",
        "            self.pc.create_index(\n",
        "                name=index_name,\n",
        "                dimension=768,  # dimension for all-mpnet-base-v2\n",
        "                metric=\"cosine\"\n",
        "            )\n",
        "            print(f\"Created new Pinecone index: {index_name}\")\n",
        "            # Wait for index to be ready\n",
        "            time.sleep(20)\n",
        "\n",
        "        self.database_folder = database_folder\n",
        "\n",
        "        # Initialize text splitter\n",
        "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=500,  # Smaller chunks for better context\n",
        "            chunk_overlap=100,\n",
        "            length_function=len\n",
        "        )\n",
        "\n",
        "    def get_embedding(self, text: str) -> List[float]:\n",
        "        \"\"\"Get embeddings using SentenceTransformer model.\"\"\"\n",
        "        with torch.no_grad():\n",
        "            embedding = self.model.encode(text, convert_to_numpy=True)\n",
        "            return embedding.tolist()\n",
        "\n",
        "    def read_article_file(self, filepath: str) -> Dict:\n",
        "        \"\"\"Read and parse article file with metadata.\"\"\"\n",
        "        with open(filepath, 'r', encoding='utf-8') as file:\n",
        "            content = file.read()\n",
        "\n",
        "            # Split metadata and content\n",
        "            parts = content.split('='*50)\n",
        "            metadata_text = parts[1].strip()\n",
        "            article_content = parts[2].strip()\n",
        "\n",
        "            # Parse metadata\n",
        "            metadata = {}\n",
        "            for line in metadata_text.split('\\n'):\n",
        "                if ':' in line:\n",
        "                    key, value = line.split(':', 1)\n",
        "                    metadata[key.strip()] = value.strip()\n",
        "\n",
        "            return {\n",
        "                'metadata': metadata,\n",
        "                'content': article_content\n",
        "            }\n",
        "\n",
        "    def process_and_embed_articles(self, batch_size: int = 50):\n",
        "        \"\"\"Process all articles and add them to Pinecone.\"\"\"\n",
        "        print(\"Processing and embedding articles...\")\n",
        "\n",
        "        vectors_to_upsert = []\n",
        "\n",
        "        for filename in tqdm(os.listdir(self.database_folder)):\n",
        "            if not filename.endswith('.txt'):\n",
        "                continue\n",
        "\n",
        "            filepath = os.path.join(self.database_folder, filename)\n",
        "            article_data = self.read_article_file(filepath)\n",
        "\n",
        "            # Split content into chunks\n",
        "            chunks = self.text_splitter.split_text(article_data['content'])\n",
        "\n",
        "            # Prepare metadata for each chunk\n",
        "            metadata = article_data['metadata']\n",
        "\n",
        "            # Process chunks\n",
        "            for i, chunk in enumerate(chunks):\n",
        "                chunk_metadata = {\n",
        "                    **metadata,\n",
        "                    'chunk_index': i,\n",
        "                    'total_chunks': len(chunks),\n",
        "                    'content': chunk  # Store content in metadata for retrieval\n",
        "                }\n",
        "\n",
        "                # Create unique ID for each chunk\n",
        "                vector_id = f\"{filename}_{i}\"\n",
        "\n",
        "                try:\n",
        "                    embedding = self.get_embedding(chunk)\n",
        "                    vectors_to_upsert.append((vector_id, embedding, chunk_metadata))\n",
        "\n",
        "                    # Batch upsert when we reach batch_size\n",
        "                    if len(vectors_to_upsert) >= batch_size:\n",
        "                        self._upsert_batch(vectors_to_upsert)\n",
        "                        vectors_to_upsert = []\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing chunk {vector_id}: {str(e)}\")\n",
        "\n",
        "        # Upsert any remaining vectors\n",
        "        if vectors_to_upsert:\n",
        "            self._upsert_batch(vectors_to_upsert)\n",
        "\n",
        "    def _upsert_batch(self, vectors):\n",
        "        \"\"\"Upsert a batch of vectors to Pinecone.\"\"\"\n",
        "        try:\n",
        "            self.pc.Index(self.index_name).upsert(\n",
        "                vectors=[(id, embedding, metadata) for id, embedding, metadata in vectors]\n",
        "            )\n",
        "        except Exception as e:\n",
        "            print(f\"Error upserting batch: {str(e)}\")\n",
        "\n",
        "    def query(self, query_text: str, n_results: int = 5) -> List[Dict]:\n",
        "        \"\"\"\n",
        "        Query the vector database\n",
        "\n",
        "        Args:\n",
        "            query_text: Text to search for\n",
        "            n_results: Number of results to return\n",
        "\n",
        "        Returns:\n",
        "            List of results with content and metadata\n",
        "        \"\"\"\n",
        "        query_embedding = self.get_embedding(query_text)\n",
        "\n",
        "        results = self.pc.Index(self.index_name).query(\n",
        "            vector=query_embedding,\n",
        "            top_k=n_results,\n",
        "            include_metadata=True\n",
        "        )\n",
        "\n",
        "        # Format results\n",
        "        formatted_results = []\n",
        "        for match in results.matches:\n",
        "            formatted_results.append({\n",
        "                'content': match.metadata['content'],\n",
        "                'metadata': {k:v for k,v in match.metadata.items() if k != 'content'},\n",
        "                'score': match.score\n",
        "            })\n",
        "\n",
        "        return formatted_results\n",
        "\n",
        "    def get_collection_stats(self) -> Dict:\n",
        "        \"\"\"Get statistics about the collection.\"\"\"\n",
        "        stats = self.pc.Index(self.index_name).describe_index_stats()\n",
        "        return {\n",
        "            'total_vectors': stats.total_vector_count,\n",
        "            'dimension': stats.dimension\n",
        "        }\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Initialize RAG system\n",
        "    rag = ArticleRAG(index_name=\"news-articles\")\n",
        "\n",
        "    # Process and embed all articles\n",
        "    rag.process_and_embed_articles()\n",
        "\n",
        "    # Print collection statistics\n",
        "    stats = rag.get_collection_stats()\n",
        "    print(\"\\nCollection Statistics:\")\n",
        "    print(f\"Total Vectors: {stats['total_vectors']}\")\n",
        "    print(f\"Embedding Dimension: {stats['dimension']}\")\n",
        "\n",
        "    # Example query\n",
        "    query = \"What are the latest developments in artificial intelligence?\"\n",
        "    results = rag.query(query)\n",
        "\n",
        "    print(f\"\\nQuery: {query}\")\n",
        "    print(\"\\nTop Results:\")\n",
        "    for i, result in enumerate(results, 1):\n",
        "        print(f\"\\n{i}. {result['metadata']['Title']}\")\n",
        "        print(f\"Topic: {result['metadata']['Topic']}\")\n",
        "        print(f\"Source: {result['metadata']['Source']}\")\n",
        "        print(f\"Relevance Score: {result['score']:.2f}\")\n",
        "        print(f\"Content Preview: {result['content'][:200]}...\")"
      ],
      "metadata": {
        "id": "wOD3UZIEGOkr"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}